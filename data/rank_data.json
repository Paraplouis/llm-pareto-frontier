{
  "last_updated": "2025-06-24",
  "models": [
    {
      "Score": 1477.4863778383874,
      "variance": 7.727551079068792,
      "rating_q975": 1482.7419545025468,
      "rating_q025": 1472.8287529244394,
      "Votes": 12327,
      "Rank (UB)": 1,
      "Model": "gemini-2.5-pro",
      "95% CI": {
        "rating_q025": 1472.8287529244394,
        "rating_q975": 1482.7419545025468
      }
    },
    {
      "Score": 1446.0,
      "variance": 7.206877765518961,
      "rating_q975": 1450.291788564756,
      "rating_q025": 1440.0510842391216,
      "Votes": 14040,
      "Rank (UB)": 2,
      "Model": "gemini-2.5-pro-preview-05-06",
      "95% CI": {
        "rating_q025": 1440.0510842391216,
        "rating_q975": 1450.291788564756
      }
    },
    {
      "Score": 1428.4217600936042,
      "variance": 3.934085511974684,
      "rating_q975": 1433.0885340368259,
      "rating_q025": 1425.0660600339559,
      "Votes": 22488,
      "Rank (UB)": 3,
      "Model": "chatgpt-4o-latest-20250326",
      "95% CI": {
        "rating_q025": 1425.0660600339559,
        "rating_q975": 1433.0885340368259
      }
    },
    {
      "Score": 1427.8851245419642,
      "variance": 5.25275273005909,
      "rating_q975": 1432.8532252081357,
      "rating_q025": 1424.0395238385026,
      "Votes": 18205,
      "Rank (UB)": 3,
      "Model": "o3-2025-04-16",
      "95% CI": {
        "rating_q025": 1424.0395238385026,
        "rating_q975": 1432.8532252081357
      }
    },
    {
      "Score": 1423.5072475317088,
      "variance": 8.943986796268334,
      "rating_q975": 1428.6027733878577,
      "rating_q025": 1417.2809883405325,
      "Votes": 11871,
      "Rank (UB)": 3,
      "Model": "deepseek-r1-0528",
      "95% CI": {
        "rating_q025": 1417.2809883405325,
        "rating_q975": 1428.6027733878577
      }
    },
    {
      "Score": 1422.3078147807478,
      "variance": 4.825751678997994,
      "rating_q975": 1426.3947457648485,
      "rating_q025": 1417.1391662421124,
      "Votes": 24316,
      "Rank (UB)": 3,
      "Model": "grok-3-preview-02-24",
      "95% CI": {
        "rating_q025": 1417.1391662421124,
        "rating_q975": 1426.3947457648485
      }
    },
    {
      "Score": 1420.2658215508768,
      "variance": 6.4334211775736465,
      "rating_q975": 1424.8336596213312,
      "rating_q025": 1414.4319001390495,
      "Votes": 17535,
      "Rank (UB)": 4,
      "Model": "gemini-2.5-flash",
      "95% CI": {
        "rating_q025": 1414.4319001390495,
        "rating_q975": 1424.8336596213312
      }
    },
    {
      "Score": 1415.2329721471244,
      "variance": 6.7028585373694805,
      "rating_q975": 1420.287985677584,
      "rating_q025": 1410.5464320017886,
      "Votes": 15271,
      "Rank (UB)": 5,
      "Model": "gpt-4.5-preview-2025-02-27",
      "95% CI": {
        "rating_q025": 1410.5464320017886,
        "rating_q975": 1420.287985677584
      }
    },
    {
      "Score": 1399.542999803087,
      "variance": 5.086788742680118,
      "rating_q975": 1403.4482844660463,
      "rating_q025": 1394.5907428233734,
      "Votes": 16123,
      "Rank (UB)": 11,
      "Model": "gemini-2.5-flash-preview-04-17",
      "95% CI": {
        "rating_q025": 1394.5907428233734,
        "rating_q975": 1403.4482844660463
      }
    },
    {
      "Score": 1388.37208224932,
      "variance": 9.618979540507338,
      "rating_q975": 1395.2528448220055,
      "rating_q025": 1382.9671483964278,
      "Votes": 12320,
      "Rank (UB)": 11,
      "Model": "qwen3-235b-a22b-no-thinking",
      "95% CI": {
        "rating_q025": 1382.9671483964278,
        "rating_q975": 1395.2528448220055
      }
    },
    {
      "Score": 1385.8559898617154,
      "variance": 6.479052611938058,
      "rating_q975": 1390.3849273970761,
      "rating_q025": 1381.3401427813017,
      "Votes": 16362,
      "Rank (UB)": 14,
      "Model": "gpt-4.1-2025-04-14",
      "95% CI": {
        "rating_q025": 1381.3401427813017,
        "rating_q975": 1390.3849273970761
      }
    },
    {
      "Score": 1385.466689292816,
      "variance": 5.149985842985978,
      "rating_q975": 1388.7738099932885,
      "rating_q025": 1380.5208371319638,
      "Votes": 19091,
      "Rank (UB)": 14,
      "Model": "deepseek-v3-0324",
      "95% CI": {
        "rating_q025": 1380.5208371319638,
        "rating_q975": 1388.7738099932885
      }
    },
    {
      "Score": 1375.941872632033,
      "variance": 10.972191379011518,
      "rating_q975": 1382.2025676562193,
      "rating_q025": 1370.1171127730759,
      "Votes": 7816,
      "Rank (UB)": 17,
      "Model": "hunyuan-turbos-20250416",
      "95% CI": {
        "rating_q025": 1370.1171127730759,
        "rating_q975": 1382.2025676562193
      }
    },
    {
      "Score": 1375.3301385294096,
      "variance": 3.544201400442676,
      "rating_q975": 1379.5086920523775,
      "rating_q025": 1371.9916739223384,
      "Votes": 19430,
      "Rank (UB)": 19,
      "Model": "deepseek-r1",
      "95% CI": {
        "rating_q025": 1371.9916739223384,
        "rating_q975": 1379.5086920523775
      }
    },
    {
      "Score": 1373.383076581067,
      "variance": 24.14671085968298,
      "rating_q975": 1382.7925682188554,
      "rating_q025": 1365.135273611975,
      "Votes": 3895,
      "Rank (UB)": 17,
      "Model": "minimax-m1",
      "95% CI": {
        "rating_q025": 1365.135273611975,
        "rating_q975": 1382.7925682188554
      }
    },
    {
      "Score": 1372.7924402342653,
      "variance": 5.932526775192035,
      "rating_q975": 1377.10486977809,
      "rating_q025": 1367.9900896945237,
      "Votes": 18287,
      "Rank (UB)": 21,
      "Model": "claude-opus-4-20250514",
      "95% CI": {
        "rating_q025": 1367.9900896945237,
        "rating_q975": 1377.10486977809
      }
    },
    {
      "Score": 1369.3445027414843,
      "variance": 5.73256692690115,
      "rating_q975": 1373.1145416545733,
      "rating_q025": 1363.6419145624438,
      "Votes": 16637,
      "Rank (UB)": 22,
      "Model": "mistral-medium-2505",
      "95% CI": {
        "rating_q025": 1363.6419145624438,
        "rating_q975": 1373.1145416545733
      }
    },
    {
      "Score": 1367.2068718073597,
      "variance": 3.270104119798149,
      "rating_q975": 1371.118350456822,
      "rating_q025": 1364.1756256397146,
      "Votes": 29038,
      "Rank (UB)": 23,
      "Model": "o1-2024-12-17",
      "95% CI": {
        "rating_q025": 1364.1756256397146,
        "rating_q975": 1371.118350456822
      }
    },
    {
      "Score": 1364.9349376504804,
      "variance": 8.332145321974581,
      "rating_q975": 1370.1226914915735,
      "rating_q025": 1359.5119088185195,
      "Votes": 13002,
      "Rank (UB)": 23,
      "Model": "qwen3-235b-a22b",
      "95% CI": {
        "rating_q025": 1359.5119088185195,
        "rating_q975": 1370.1226914915735
      }
    },
    {
      "Score": 1364.4290385915244,
      "variance": 2.3772895069290243,
      "rating_q975": 1367.449451351929,
      "rating_q025": 1361.7049079671528,
      "Votes": 35894,
      "Rank (UB)": 26,
      "Model": "gemini-2.0-flash-001",
      "95% CI": {
        "rating_q025": 1361.7049079671528,
        "rating_q975": 1367.449451351929
      }
    },
    {
      "Score": 1363.2832776462312,
      "variance": 6.636513305517163,
      "rating_q975": 1368.4297916323048,
      "rating_q025": 1359.0028654113187,
      "Votes": 16112,
      "Rank (UB)": 25,
      "Model": "o4-mini-2025-04-16",
      "95% CI": {
        "rating_q025": 1359.0028654113187,
        "rating_q975": 1368.4297916323048
      }
    },
    {
      "Score": 1362.725529569859,
      "variance": 2.267161635135078,
      "rating_q975": 1365.382433864291,
      "rating_q025": 1359.5664809383602,
      "Votes": 31170,
      "Rank (UB)": 26,
      "Model": "qwen2.5-max",
      "95% CI": {
        "rating_q025": 1359.5664809383602,
        "rating_q975": 1365.382433864291
      }
    },
    {
      "Score": 1362.6525073782109,
      "variance": 8.935895095725556,
      "rating_q975": 1368.3780289314814,
      "rating_q025": 1357.2822526250761,
      "Votes": 8715,
      "Rank (UB)": 25,
      "Model": "grok-3-mini-beta",
      "95% CI": {
        "rating_q025": 1357.2822526250761,
        "rating_q975": 1368.3780289314814
      }
    },
    {
      "Score": 1357.1724923298434,
      "variance": 3.258110170339007,
      "rating_q975": 1360.7805178962767,
      "rating_q025": 1354.0325452132115,
      "Votes": 25323,
      "Rank (UB)": 30,
      "Model": "gemma-3-27b-it",
      "95% CI": {
        "rating_q025": 1354.0325452132115,
        "rating_q975": 1360.7805178962767
      }
    },
    {
      "Score": 1352.4169120188146,
      "variance": 3.272674643126604,
      "rating_q975": 1355.6872108607026,
      "rating_q025": 1349.4542796261971,
      "Votes": 33177,
      "Rank (UB)": 35,
      "Model": "o1-preview",
      "95% CI": {
        "rating_q025": 1349.4542796261971,
        "rating_q975": 1355.6872108607026
      }
    },
    {
      "Score": 1344.568792323884,
      "variance": 5.743847957743427,
      "rating_q975": 1349.2554338153839,
      "rating_q025": 1340.3340946568294,
      "Votes": 14984,
      "Rank (UB)": 38,
      "Model": "claude-sonnet-4-20250514",
      "95% CI": {
        "rating_q025": 1340.3340946568294,
        "rating_q975": 1349.2554338153839
      }
    },
    {
      "Score": 1341.8608386420367,
      "variance": 4.390423173349201,
      "rating_q975": 1345.6458847221127,
      "rating_q025": 1338.117187642226,
      "Votes": 19404,
      "Rank (UB)": 38,
      "Model": "o3-mini-high",
      "95% CI": {
        "rating_q025": 1338.117187642226,
        "rating_q975": 1345.6458847221127
      }
    },
    {
      "Score": 1339.0359285826817,
      "variance": 5.298336966293602,
      "rating_q975": 1343.9157424966813,
      "rating_q025": 1335.090099174933,
      "Votes": 15337,
      "Rank (UB)": 38,
      "Model": "gpt-4.1-mini-2025-04-14",
      "95% CI": {
        "rating_q025": 1335.090099174933,
        "rating_q975": 1343.9157424966813
      }
    },
    {
      "Score": 1337.7837354941928,
      "variance": 23.146655505284407,
      "rating_q975": 1347.288293914262,
      "rating_q025": 1328.669364811459,
      "Votes": 3976,
      "Rank (UB)": 38,
      "Model": "gemma-3-12b-it",
      "95% CI": {
        "rating_q025": 1328.669364811459,
        "rating_q975": 1347.288293914262
      }
    },
    {
      "Score": 1335.9109267999338,
      "variance": 4.057243027433306,
      "rating_q975": 1339.3215629816602,
      "rating_q025": 1331.860642029511,
      "Votes": 22841,
      "Rank (UB)": 39,
      "Model": "deepseek-v3",
      "95% CI": {
        "rating_q025": 1331.860642029511,
        "rating_q975": 1339.3215629816602
      }
    },
    {
      "Score": 1334.403036788626,
      "variance": 4.7666193186048345,
      "rating_q975": 1339.2696820849294,
      "rating_q025": 1330.5607511971639,
      "Votes": 17462,
      "Rank (UB)": 39,
      "Model": "qwq-32b",
      "95% CI": {
        "rating_q025": 1330.5607511971639,
        "rating_q975": 1339.2696820849294
      }
    },
    {
      "Score": 1330.1588299754058,
      "variance": 2.476058341970297,
      "rating_q975": 1333.0232777987517,
      "rating_q025": 1327.0447272225974,
      "Votes": 26104,
      "Rank (UB)": 41,
      "Model": "gemini-2.0-flash-lite-preview-02-05",
      "95% CI": {
        "rating_q025": 1327.0447272225974,
        "rating_q975": 1333.0232777987517
      }
    },
    {
      "Score": 1328.429077293644,
      "variance": 16.671863230868507,
      "rating_q975": 1336.0191132652312,
      "rating_q025": 1320.246595862198,
      "Votes": 6028,
      "Rank (UB)": 40,
      "Model": "glm-4-plus-0111",
      "95% CI": {
        "rating_q025": 1320.246595862198,
        "rating_q975": 1336.0191132652312
      }
    },
    {
      "Score": 1328.3157429000757,
      "variance": 19.189288658551657,
      "rating_q975": 1336.4827985897834,
      "rating_q025": 1319.8152983626464,
      "Votes": 5213,
      "Rank (UB)": 40,
      "Model": "amazon-nova-experimental-chat-05-14",
      "95% CI": {
        "rating_q025": 1319.8152983626464,
        "rating_q975": 1336.4827985897834
      }
    },
    {
      "Score": 1327.8236594177242,
      "variance": 13.977919091607413,
      "rating_q975": 1335.310330472761,
      "rating_q025": 1321.7326911976224,
      "Votes": 6055,
      "Rank (UB)": 40,
      "Model": "qwen-plus-0125",
      "95% CI": {
        "rating_q025": 1321.7326911976224,
        "rating_q975": 1335.310330472761
      }
    },
    {
      "Score": 1326.8166759460828,
      "variance": 3.208749879371869,
      "rating_q975": 1329.9017047288935,
      "rating_q025": 1323.4025011432852,
      "Votes": 22851,
      "Rank (UB)": 44,
      "Model": "command-a-03-2025",
      "95% CI": {
        "rating_q025": 1323.4025011432852,
        "rating_q975": 1329.9017047288935
      }
    },
    {
      "Score": 1322.5942822395566,
      "variance": 2.207771310445694,
      "rating_q975": 1325.5309936976982,
      "rating_q025": 1320.129767168756,
      "Votes": 35063,
      "Rank (UB)": 46,
      "Model": "o3-mini",
      "95% CI": {
        "rating_q025": 1320.129767168756,
        "rating_q975": 1325.5309936976982
      }
    },
    {
      "Score": 1322.221734056332,
      "variance": 12.85140244883161,
      "rating_q975": 1328.9622182073897,
      "rating_q025": 1315.6557634280941,
      "Votes": 5126,
      "Rank (UB)": 44,
      "Model": "step-2-16k-exp-202412",
      "95% CI": {
        "rating_q025": 1315.6557634280941,
        "rating_q975": 1328.9622182073897
      }
    },
    {
      "Score": 1321.3553998325651,
      "variance": 1.5403935378830553,
      "rating_q975": 1323.6031831129349,
      "rating_q025": 1318.5969267083015,
      "Votes": 54951,
      "Rank (UB)": 46,
      "Model": "o1-mini",
      "95% CI": {
        "rating_q025": 1318.5969267083015,
        "rating_q975": 1323.6031831129349
      }
    },
    {
      "Score": 1319.7828159795104,
      "variance": 28.892507984964233,
      "rating_q975": 1330.0794479092397,
      "rating_q025": 1310.4254665918768,
      "Votes": 2452,
      "Rank (UB)": 44,
      "Model": "hunyuan-turbos-20250226",
      "95% CI": {
        "rating_q025": 1310.4254665918768,
        "rating_q975": 1330.0794479092397
      }
    },
    {
      "Score": 1319.5993235357867,
      "variance": 1.4096660635300462,
      "rating_q975": 1321.8592224571569,
      "rating_q025": 1317.4150111020215,
      "Votes": 58645,
      "Rank (UB)": 47,
      "Model": "gemini-1.5-pro-002",
      "95% CI": {
        "rating_q025": 1317.4150111020215,
        "rating_q975": 1321.8592224571569
      }
    },
    {
      "Score": 1316.747235741741,
      "variance": 4.001679435800976,
      "rating_q975": 1320.1682877779472,
      "rating_q025": 1312.527912118128,
      "Votes": 24159,
      "Rank (UB)": 49,
      "Model": "claude-3-7-sonnet-20250219-thinking-32k",
      "95% CI": {
        "rating_q025": 1312.527912118128,
        "rating_q975": 1320.1682877779472
      }
    },
    {
      "Score": 1313.7428393662926,
      "variance": 33.492325717351214,
      "rating_q975": 1323.855662828396,
      "rating_q025": 1304.8747445792335,
      "Votes": 2371,
      "Rank (UB)": 46,
      "Model": "llama-3.3-nemotron-49b-super-v1",
      "95% CI": {
        "rating_q025": 1304.8747445792335,
        "rating_q975": 1323.855662828396
      }
    },
    {
      "Score": 1313.5249946826318,
      "variance": 30.487241796131546,
      "rating_q975": 1323.9861244713936,
      "rating_q025": 1305.7254920662854,
      "Votes": 2510,
      "Rank (UB)": 46,
      "Model": "hunyuan-turbo-0110",
      "95% CI": {
        "rating_q025": 1305.7254920662854,
        "rating_q975": 1323.9861244713936
      }
    },
    {
      "Score": 1309.4262658719035,
      "variance": 3.0988422658420496,
      "rating_q975": 1312.8266138689617,
      "rating_q025": 1305.7402144030843,
      "Votes": 28664,
      "Rank (UB)": 55,
      "Model": "claude-3-7-sonnet-20250219",
      "95% CI": {
        "rating_q025": 1305.7402144030843,
        "rating_q975": 1312.8266138689617
      }
    },
    {
      "Score": 1305.2913772393852,
      "variance": 1.105551701716215,
      "rating_q975": 1306.8972093725322,
      "rating_q025": 1303.0325042587642,
      "Votes": 67084,
      "Rank (UB)": 58,
      "Model": "grok-2-2024-08-13",
      "95% CI": {
        "rating_q025": 1303.0325042587642,
        "rating_q975": 1306.8972093725322
      }
    },
    {
      "Score": 1304.49913469711,
      "variance": 2.665652550939783,
      "rating_q975": 1307.589435700859,
      "rating_q025": 1300.83653383368,
      "Votes": 28968,
      "Rank (UB)": 58,
      "Model": "yi-lightning",
      "95% CI": {
        "rating_q025": 1300.83653383368,
        "rating_q975": 1307.589435700859
      }
    },
    {
      "Score": 1302.985542757927,
      "variance": 13.468214075189069,
      "rating_q975": 1308.8213784230916,
      "rating_q025": 1295.3020711963477,
      "Votes": 5282,
      "Rank (UB)": 58,
      "Model": "gemma-3n-e4b-it",
      "95% CI": {
        "rating_q025": 1295.3020711963477,
        "rating_q975": 1308.8213784230916
      }
    },
    {
      "Score": 1302.4306803927798,
      "variance": 0.9328678297726606,
      "rating_q975": 1304.3548853841605,
      "rating_q025": 1300.5687203350653,
      "Votes": 117747,
      "Rank (UB)": 61,
      "Model": "gpt-4o-2024-05-13",
      "95% CI": {
        "rating_q025": 1300.5687203350653,
        "rating_q975": 1304.3548853841605
      }
    },
    {
      "Score": 1301.0365896231713,
      "variance": 1.1443943498312183,
      "rating_q975": 1303.120721629121,
      "rating_q025": 1299.355844006131,
      "Votes": 75986,
      "Rank (UB)": 61,
      "Model": "claude-3-5-sonnet-20241022",
      "95% CI": {
        "rating_q025": 1299.355844006131,
        "rating_q975": 1303.120721629121
      }
    },
    {
      "Score": 1299.6328411806185,
      "variance": 6.938768759648069,
      "rating_q975": 1303.5658030137329,
      "rating_q025": 1294.177382103578,
      "Votes": 10715,
      "Rank (UB)": 61,
      "Model": "qwen2.5-plus-1127",
      "95% CI": {
        "rating_q025": 1294.177382103578,
        "rating_q975": 1303.5658030137329
      }
    },
    {
      "Score": 1296.565838661678,
      "variance": 8.337179489426115,
      "rating_q975": 1302.4653722610137,
      "rating_q025": 1290.7502058454202,
      "Votes": 7243,
      "Rank (UB)": 62,
      "Model": "deepseek-v2.5-1210",
      "95% CI": {
        "rating_q025": 1290.7502058454202,
        "rating_q975": 1302.4653722610137
      }
    },
    {
      "Score": 1292.8566550447479,
      "variance": 3.354072430703398,
      "rating_q975": 1295.7117148108127,
      "rating_q025": 1289.3699232891402,
      "Votes": 26074,
      "Rank (UB)": 65,
      "Model": "athene-v2-chat",
      "95% CI": {
        "rating_q025": 1289.3699232891402,
        "rating_q975": 1295.7117148108127
      }
    },
    {
      "Score": 1292.2361405875602,
      "variance": 18.1061115281034,
      "rating_q975": 1300.3254490960271,
      "rating_q025": 1284.4754005695474,
      "Votes": 4321,
      "Rank (UB)": 64,
      "Model": "gemma-3-4b-it",
      "95% CI": {
        "rating_q025": 1284.4754005695474,
        "rating_q975": 1300.3254490960271
      }
    },
    {
      "Score": 1291.9473369301807,
      "variance": 5.383168482448149,
      "rating_q975": 1295.750352555558,
      "rating_q025": 1287.4523027735183,
      "Votes": 15906,
      "Rank (UB)": 65,
      "Model": "llama-4-maverick-17b-128e-instruct",
      "95% CI": {
        "rating_q025": 1287.4523027735183,
        "rating_q975": 1295.750352555558
      }
    },
    {
      "Score": 1291.4032887847509,
      "variance": 3.1078035009941685,
      "rating_q975": 1295.5741088515117,
      "rating_q025": 1288.9852449628363,
      "Votes": 27788,
      "Rank (UB)": 65,
      "Model": "glm-4-plus",
      "95% CI": {
        "rating_q025": 1288.9852449628363,
        "rating_q975": 1295.5741088515117
      }
    },
    {
      "Score": 1289.2758471180618,
      "variance": 22.257552621146626,
      "rating_q975": 1298.6371100140727,
      "rating_q025": 1279.4332979059639,
      "Votes": 3856,
      "Rank (UB)": 65,
      "Model": "hunyuan-large-2025-02-10",
      "95% CI": {
        "rating_q025": 1279.4332979059639,
        "rating_q975": 1298.6371100140727
      }
    },
    {
      "Score": 1289.1298526517558,
      "variance": 1.3233114986649759,
      "rating_q975": 1291.6545528898118,
      "rating_q025": 1287.1936476558676,
      "Votes": 72536,
      "Rank (UB)": 67,
      "Model": "gpt-4o-mini-2024-07-18",
      "95% CI": {
        "rating_q025": 1287.1936476558676,
        "rating_q975": 1291.6545528898118
      }
    },
    {
      "Score": 1288.9344724985006,
      "variance": 2.1715412021751526,
      "rating_q975": 1292.5330604126636,
      "rating_q025": 1286.415340436721,
      "Votes": 37021,
      "Rank (UB)": 67,
      "Model": "gemini-1.5-flash-002",
      "95% CI": {
        "rating_q025": 1286.415340436721,
        "rating_q975": 1292.5330604126636
      }
    },
    {
      "Score": 1288.060552672309,
      "variance": 13.440029565737277,
      "rating_q975": 1295.8357817375652,
      "rating_q025": 1281.307585221928,
      "Votes": 6302,
      "Rank (UB)": 65,
      "Model": "gpt-4.1-nano-2025-04-14",
      "95% CI": {
        "rating_q025": 1281.307585221928,
        "rating_q975": 1295.8357817375652
      }
    },
    {
      "Score": 1286.4088290898276,
      "variance": 1.6520392835695403,
      "rating_q975": 1288.6817470220014,
      "rating_q025": 1283.9857518684857,
      "Votes": 43788,
      "Rank (UB)": 70,
      "Model": "llama-3.1-405b-instruct-bf16",
      "95% CI": {
        "rating_q025": 1283.9857518684857,
        "rating_q975": 1288.6817470220014
      }
    },
    {
      "Score": 1285.998687701971,
      "variance": 11.58835611584996,
      "rating_q975": 1292.6104992097416,
      "rating_q025": 1280.2102333689704,
      "Votes": 7577,
      "Rank (UB)": 67,
      "Model": "llama-3.1-nemotron-70b-instruct",
      "95% CI": {
        "rating_q025": 1280.2102333689704,
        "rating_q975": 1292.6104992097416
      }
    },
    {
      "Score": 1285.6717525559675,
      "variance": 0.8645243001236339,
      "rating_q975": 1287.443055175437,
      "rating_q025": 1283.5965726589625,
      "Votes": 86159,
      "Rank (UB)": 71,
      "Model": "claude-3-5-sonnet-20240620",
      "95% CI": {
        "rating_q025": 1283.5965726589625,
        "rating_q975": 1287.443055175437
      }
    },
    {
      "Score": 1284.9566351834028,
      "variance": 1.306215973358925,
      "rating_q975": 1287.1563083425876,
      "rating_q025": 1282.769236454092,
      "Votes": 63038,
      "Rank (UB)": 72,
      "Model": "llama-3.1-405b-instruct-fp8",
      "95% CI": {
        "rating_q025": 1282.769236454092,
        "rating_q975": 1287.1563083425876
      }
    },
    {
      "Score": 1283.9960386652342,
      "variance": 1.6987726939977146,
      "rating_q975": 1286.924834605456,
      "rating_q025": 1281.2664692987325,
      "Votes": 52144,
      "Rank (UB)": 72,
      "Model": "gemini-advanced-0514",
      "95% CI": {
        "rating_q025": 1281.2664692987325,
        "rating_q975": 1286.924834605456
      }
    },
    {
      "Score": 1283.6408510429264,
      "variance": 1.647556691473906,
      "rating_q975": 1285.806708325311,
      "rating_q025": 1281.3595605326536,
      "Votes": 55442,
      "Rank (UB)": 73,
      "Model": "grok-2-mini-2024-08-13",
      "95% CI": {
        "rating_q025": 1281.3595605326536,
        "rating_q975": 1285.806708325311
      }
    },
    {
      "Score": 1282.7002906792518,
      "variance": 2.252433097442659,
      "rating_q975": 1285.9404314569085,
      "rating_q025": 1280.0632322091483,
      "Votes": 47973,
      "Rank (UB)": 73,
      "Model": "gpt-4o-2024-08-06",
      "95% CI": {
        "rating_q025": 1280.0632322091483,
        "rating_q975": 1285.9404314569085
      }
    },
    {
      "Score": 1280.7933199619583,
      "variance": 4.092365050794014,
      "rating_q975": 1284.5677059460445,
      "rating_q025": 1277.200742853798,
      "Votes": 17432,
      "Rank (UB)": 73,
      "Model": "qwen-max-0919",
      "95% CI": {
        "rating_q025": 1277.200742853798,
        "rating_q975": 1284.5677059460445
      }
    },
    {
      "Score": 1278.1445425566649,
      "variance": 21.82639554250842,
      "rating_q975": 1289.509711389524,
      "rating_q025": 1270.4209044931156,
      "Votes": 4014,
      "Rank (UB)": 68,
      "Model": "hunyuan-standard-2025-02-10",
      "95% CI": {
        "rating_q025": 1270.4209044931156,
        "rating_q975": 1289.509711389524
      }
    },
    {
      "Score": 1277.3268171983973,
      "variance": 1.2956230028955475,
      "rating_q975": 1279.3409469949765,
      "rating_q025": 1275.1102090756706,
      "Votes": 82435,
      "Rank (UB)": 84,
      "Model": "gemini-1.5-pro-001",
      "95% CI": {
        "rating_q025": 1275.1102090756706,
        "rating_q975": 1279.3409469949765
      }
    },
    {
      "Score": 1275.5367902821731,
      "variance": 2.0279379546445253,
      "rating_q975": 1277.720973728262,
      "rating_q025": 1272.8542170404546,
      "Votes": 26344,
      "Rank (UB)": 84,
      "Model": "deepseek-v2.5",
      "95% CI": {
        "rating_q025": 1272.8542170404546,
        "rating_q975": 1277.720973728262
      }
    },
    {
      "Score": 1274.8448152742912,
      "variance": 1.5887255004114396,
      "rating_q975": 1277.2202780265136,
      "rating_q025": 1272.585673338547,
      "Votes": 46558,
      "Rank (UB)": 85,
      "Model": "llama-3.3-70b-instruct",
      "95% CI": {
        "rating_q025": 1272.585673338547,
        "rating_q975": 1277.2202780265136
      }
    },
    {
      "Score": 1274.5451911408222,
      "variance": 1.8312350160195148,
      "rating_q975": 1277.0474418395415,
      "rating_q025": 1271.8356800516665,
      "Votes": 41519,
      "Rank (UB)": 86,
      "Model": "qwen2.5-72b-instruct",
      "95% CI": {
        "rating_q025": 1271.8356800516665,
        "rating_q975": 1277.0474418395415
      }
    },
    {
      "Score": 1273.89010378071,
      "variance": 1.2157057005739986,
      "rating_q975": 1275.866491611304,
      "rating_q025": 1271.8712067859076,
      "Votes": 102133,
      "Rank (UB)": 86,
      "Model": "gpt-4-turbo-2024-04-09",
      "95% CI": {
        "rating_q025": 1271.8712067859076,
        "rating_q975": 1275.866491611304
      }
    },
    {
      "Score": 1271.063069990893,
      "variance": 17.891193410461447,
      "rating_q975": 1281.6093543694067,
      "rating_q025": 1264.9918222348745,
      "Votes": 4998,
      "Rank (UB)": 78,
      "Model": "llama-4-scout-17b-16e-instruct",
      "95% CI": {
        "rating_q025": 1264.9918222348745,
        "rating_q975": 1281.6093543694067
      }
    },
    {
      "Score": 1270.6214703287806,
      "variance": 17.095524786540658,
      "rating_q975": 1279.1049120429618,
      "rating_q025": 1263.5781818140526,
      "Votes": 4963,
      "Rank (UB)": 84,
      "Model": "mistral-small-3.1-24b-instruct-2503",
      "95% CI": {
        "rating_q025": 1263.5781818140526,
        "rating_q975": 1279.1049120429618
      }
    },
    {
      "Score": 1269.0176122919747,
      "variance": 1.8959695368027252,
      "rating_q975": 1271.3600177653936,
      "rating_q025": 1266.599504578191,
      "Votes": 48217,
      "Rank (UB)": 92,
      "Model": "mistral-large-2407",
      "95% CI": {
        "rating_q025": 1266.599504578191,
        "rating_q975": 1271.3600177653936
      }
    },
    {
      "Score": 1267.7962680512014,
      "variance": 3.891141314811822,
      "rating_q975": 1271.5480123823584,
      "rating_q025": 1264.1753761432099,
      "Votes": 20580,
      "Rank (UB)": 92,
      "Model": "athene-70b-0725",
      "95% CI": {
        "rating_q025": 1264.1753761432099,
        "rating_q975": 1271.5480123823584
      }
    },
    {
      "Score": 1267.4033863847903,
      "variance": 1.0116324971657882,
      "rating_q975": 1269.2138465101505,
      "rating_q025": 1265.3851739556028,
      "Votes": 103748,
      "Rank (UB)": 93,
      "Model": "gpt-4-1106-preview",
      "95% CI": {
        "rating_q025": 1265.3851739556028,
        "rating_q975": 1269.2138465101505
      }
    },
    {
      "Score": 1266.354184203025,
      "variance": 2.750692361481322,
      "rating_q975": 1269.6349782301809,
      "rating_q025": 1263.6855659704213,
      "Votes": 29633,
      "Rank (UB)": 93,
      "Model": "mistral-large-2411",
      "95% CI": {
        "rating_q025": 1263.6855659704213,
        "rating_q975": 1269.6349782301809
      }
    },
    {
      "Score": 1265.18116355656,
      "variance": 1.46298875531472,
      "rating_q975": 1267.367879501326,
      "rating_q025": 1262.943495768737,
      "Votes": 58637,
      "Rank (UB)": 93,
      "Model": "llama-3.1-70b-instruct",
      "95% CI": {
        "rating_q025": 1262.943495768737,
        "rating_q975": 1267.367879501326
      }
    },
    {
      "Score": 1264.7380611182416,
      "variance": 0.6235695723266682,
      "rating_q975": 1266.1677768701466,
      "rating_q025": 1263.1723710156173,
      "Votes": 202641,
      "Rank (UB)": 94,
      "Model": "claude-3-opus-20240229",
      "95% CI": {
        "rating_q025": 1263.1723710156173,
        "rating_q975": 1266.1677768701466
      }
    },
    {
      "Score": 1262.7398471614697,
      "variance": 39.4087910110368,
      "rating_q975": 1273.4057470805226,
      "rating_q025": 1252.1345465845877,
      "Votes": 3089,
      "Rank (UB)": 87,
      "Model": "magistral-medium-2506",
      "95% CI": {
        "rating_q025": 1252.1345465845877,
        "rating_q975": 1273.4057470805226
      }
    },
    {
      "Score": 1262.4096426465312,
      "variance": 3.244364836142391,
      "rating_q975": 1265.3877473332595,
      "rating_q025": 1258.9073483510829,
      "Votes": 26371,
      "Rank (UB)": 94,
      "Model": "amazon-nova-pro-v1.0",
      "95% CI": {
        "rating_q025": 1258.9073483510829,
        "rating_q975": 1265.3877473332595
      }
    },
    {
      "Score": 1262.3336124855346,
      "variance": 1.4157182892112203,
      "rating_q975": 1264.4241110772286,
      "rating_q025": 1259.9093174040331,
      "Votes": 97079,
      "Rank (UB)": 96,
      "Model": "gpt-4-0125-preview",
      "95% CI": {
        "rating_q025": 1259.9093174040331,
        "rating_q975": 1264.4241110772286
      }
    },
    {
      "Score": 1261.7059138838472,
      "variance": 21.14238652976314,
      "rating_q975": 1270.7049349982215,
      "rating_q025": 1252.9973796070465,
      "Votes": 3010,
      "Rank (UB)": 92,
      "Model": "llama-3.1-tulu-3-70b",
      "95% CI": {
        "rating_q025": 1252.9973796070465,
        "rating_q975": 1270.7049349982215
      }
    },
    {
      "Score": 1256.0940283283417,
      "variance": 1.788534793768413,
      "rating_q975": 1258.2067296852977,
      "rating_q025": 1253.3990343948706,
      "Votes": 47551,
      "Rank (UB)": 103,
      "Model": "claude-3-5-haiku-20241022",
      "95% CI": {
        "rating_q025": 1253.3990343948706,
        "rating_q975": 1258.2067296852977
      }
    },
    {
      "Score": 1252.5662189669106,
      "variance": 9.422043149362503,
      "rating_q975": 1257.8948925873815,
      "rating_q025": 1246.4295783140296,
      "Votes": 7948,
      "Rank (UB)": 103,
      "Model": "reka-core-20240904",
      "95% CI": {
        "rating_q025": 1246.4295783140296,
        "rating_q975": 1257.8948925873815
      }
    },
    {
      "Score": 1244.3250029078367,
      "variance": 1.297877887001598,
      "rating_q975": 1246.5067467391857,
      "rating_q025": 1242.3631789231904,
      "Votes": 65661,
      "Rank (UB)": 107,
      "Model": "gemini-1.5-flash-001",
      "95% CI": {
        "rating_q025": 1242.3631789231904,
        "rating_q975": 1246.5067467391857
      }
    },
    {
      "Score": 1239.0889678809544,
      "variance": 10.246026042152554,
      "rating_q975": 1245.426217279567,
      "rating_q025": 1233.6116726573548,
      "Votes": 9125,
      "Rank (UB)": 108,
      "Model": "jamba-1.5-large",
      "95% CI": {
        "rating_q025": 1233.6116726573548,
        "rating_q975": 1245.426217279567
      }
    },
    {
      "Score": 1237.2850675698799,
      "variance": 0.8928719520960644,
      "rating_q975": 1239.082633308759,
      "rating_q025": 1235.530738424816,
      "Votes": 79538,
      "Rank (UB)": 111,
      "Model": "gemma-2-27b-it",
      "95% CI": {
        "rating_q025": 1235.530738424816,
        "rating_q975": 1239.082633308759
      }
    },
    {
      "Score": 1234.7299351490115,
      "variance": 13.854207500595567,
      "rating_q975": 1241.6527201356917,
      "rating_q025": 1226.8682162557495,
      "Votes": 5730,
      "Rank (UB)": 110,
      "Model": "qwen2.5-coder-32b-instruct",
      "95% CI": {
        "rating_q025": 1226.8682162557495,
        "rating_q975": 1241.6527201356917
      }
    },
    {
      "Score": 1234.7034621338687,
      "variance": 4.92851755280997,
      "rating_q975": 1238.3971763129532,
      "rating_q025": 1231.2299142791428,
      "Votes": 15321,
      "Rank (UB)": 111,
      "Model": "mistral-small-24b-instruct-2501",
      "95% CI": {
        "rating_q025": 1231.2299142791428,
        "rating_q975": 1238.3971763129532
      }
    },
    {
      "Score": 1234.33748789009,
      "variance": 3.3286861179242626,
      "rating_q975": 1237.1783458790733,
      "rating_q025": 1230.3834952301986,
      "Votes": 20646,
      "Rank (UB)": 111,
      "Model": "amazon-nova-lite-v1.0",
      "95% CI": {
        "rating_q025": 1230.3834952301986,
        "rating_q975": 1237.1783458790733
      }
    },
    {
      "Score": 1233.5467564159035,
      "variance": 8.231427402170965,
      "rating_q975": 1239.0264928070476,
      "rating_q025": 1228.5388440785753,
      "Votes": 10548,
      "Rank (UB)": 111,
      "Model": "gemma-2-9b-it-simpo",
      "95% CI": {
        "rating_q025": 1228.5388440785753,
        "rating_q975": 1239.0264928070476
      }
    },
    {
      "Score": 1232.755422552512,
      "variance": 9.822251402300212,
      "rating_q975": 1240.1267534927163,
      "rating_q025": 1227.6634373545462,
      "Votes": 10535,
      "Rank (UB)": 110,
      "Model": "command-r-plus-08-2024",
      "95% CI": {
        "rating_q025": 1227.6634373545462,
        "rating_q975": 1240.1267534927163
      }
    },
    {
      "Score": 1230.057947418146,
      "variance": 2.6428525320016236,
      "rating_q975": 1233.1323168305705,
      "rating_q025": 1226.8677955920318,
      "Votes": 37697,
      "Rank (UB)": 114,
      "Model": "gemini-1.5-flash-8b-001",
      "95% CI": {
        "rating_q025": 1226.8677955920318,
        "rating_q975": 1233.1323168305705
      }
    },
    {
      "Score": 1229.2341127747804,
      "variance": 20.831477464969844,
      "rating_q975": 1240.2646029847597,
      "rating_q025": 1222.438964508323,
      "Votes": 3889,
      "Rank (UB)": 110,
      "Model": "llama-3.1-nemotron-51b-instruct",
      "95% CI": {
        "rating_q025": 1222.438964508323,
        "rating_q975": 1240.2646029847597
      }
    },
    {
      "Score": 1226.6870393146046,
      "variance": 2.263273811042775,
      "rating_q975": 1229.7352050606348,
      "rating_q025": 1223.8924583852988,
      "Votes": 28768,
      "Rank (UB)": 116,
      "Model": "c4ai-aya-expanse-32b",
      "95% CI": {
        "rating_q025": 1223.8924583852988,
        "rating_q975": 1229.7352050606348
      }
    },
    {
      "Score": 1226.5485544421404,
      "variance": 4.35065221568839,
      "rating_q975": 1230.2768482423357,
      "rating_q025": 1222.2231675144888,
      "Votes": 20608,
      "Rank (UB)": 116,
      "Model": "nemotron-4-340b-instruct",
      "95% CI": {
        "rating_q025": 1222.2231675144888,
        "rating_q975": 1230.2768482423357
      }
    },
    {
      "Score": 1224.0877410497362,
      "variance": 7.351171345544473,
      "rating_q975": 1229.1674568874384,
      "rating_q025": 1219.0739785855285,
      "Votes": 10221,
      "Rank (UB)": 116,
      "Model": "glm-4-0520",
      "95% CI": {
        "rating_q025": 1219.0739785855285,
        "rating_q975": 1229.1674568874384
      }
    },
    {
      "Score": 1223.9827857900602,
      "variance": 0.745785485511127,
      "rating_q975": 1225.7574283685842,
      "rating_q025": 1222.191673210538,
      "Votes": 163629,
      "Rank (UB)": 122,
      "Model": "llama-3-70b-instruct",
      "95% CI": {
        "rating_q025": 1222.191673210538,
        "rating_q975": 1225.7574283685842
      }
    },
    {
      "Score": 1223.0932125889406,
      "variance": 26.762594406986093,
      "rating_q975": 1232.1914189370573,
      "rating_q025": 1214.368099259305,
      "Votes": 3460,
      "Rank (UB)": 114,
      "Model": "olmo-2-0325-32b-instruct",
      "95% CI": {
        "rating_q025": 1214.368099259305,
        "rating_q975": 1232.1914189370573
      }
    },
    {
      "Score": 1222.978795765498,
      "variance": 7.50883425694297,
      "rating_q975": 1227.5160641803839,
      "rating_q025": 1217.784680107049,
      "Votes": 8132,
      "Rank (UB)": 118,
      "Model": "reka-flash-20240904",
      "95% CI": {
        "rating_q025": 1217.784680107049,
        "rating_q975": 1227.5160641803839
      }
    },
    {
      "Score": 1222.773454610985,
      "variance": 2.8419782804404523,
      "rating_q975": 1226.2264445927294,
      "rating_q025": 1220.3610825282706,
      "Votes": 25213,
      "Rank (UB)": 121,
      "Model": "phi-4",
      "95% CI": {
        "rating_q025": 1220.3610825282706,
        "rating_q975": 1226.2264445927294
      }
    },
    {
      "Score": 1218.8171375416655,
      "variance": 33.4568852958531,
      "rating_q975": 1230.3564570385013,
      "rating_q025": 1207.263535767706,
      "Votes": 3478,
      "Rank (UB)": 116,
      "Model": "hunyuan-large-vision",
      "95% CI": {
        "rating_q025": 1207.263535767706,
        "rating_q975": 1230.3564570385013
      }
    },
    {
      "Score": 1218.4396803225582,
      "variance": 0.8585311298596849,
      "rating_q975": 1219.9567304634493,
      "rating_q025": 1216.6239066443811,
      "Votes": 113067,
      "Rank (UB)": 128,
      "Model": "claude-3-sonnet-20240229",
      "95% CI": {
        "rating_q025": 1216.6239066443811,
        "rating_q975": 1219.9567304634493
      }
    },
    {
      "Score": 1215.4393638472034,
      "variance": 4.159666566602228,
      "rating_q975": 1218.7319355164127,
      "rating_q025": 1211.5534293920996,
      "Votes": 20654,
      "Rank (UB)": 129,
      "Model": "amazon-nova-micro-v1.0",
      "95% CI": {
        "rating_q025": 1211.5534293920996,
        "rating_q975": 1218.7319355164127
      }
    },
    {
      "Score": 1209.4772219628985,
      "variance": 1.7267539653384183,
      "rating_q975": 1212.0160154009504,
      "rating_q025": 1207.018631574108,
      "Votes": 57197,
      "Rank (UB)": 135,
      "Model": "gemma-2-9b-it",
      "95% CI": {
        "rating_q025": 1207.018631574108,
        "rating_q975": 1212.0160154009504
      }
    },
    {
      "Score": 1207.3762501244746,
      "variance": 1.030981936417149,
      "rating_q975": 1209.2685048085264,
      "rating_q025": 1205.2744907282395,
      "Votes": 80846,
      "Rank (UB)": 136,
      "Model": "command-r-plus",
      "95% CI": {
        "rating_q025": 1205.2744907282395,
        "rating_q975": 1209.2685048085264
      }
    },
    {
      "Score": 1206.3437246876651,
      "variance": 27.025943621051795,
      "rating_q975": 1215.8876885352777,
      "rating_q025": 1197.477845187844,
      "Votes": 2901,
      "Rank (UB)": 132,
      "Model": "hunyuan-standard-256k",
      "95% CI": {
        "rating_q025": 1197.477845187844,
        "rating_q975": 1215.8876885352777
      }
    },
    {
      "Score": 1204.523812695526,
      "variance": 2.6056205276052617,
      "rating_q975": 1207.2962458662628,
      "rating_q025": 1201.6060253623418,
      "Votes": 38872,
      "Rank (UB)": 136,
      "Model": "qwen2-72b-instruct",
      "95% CI": {
        "rating_q025": 1201.6060253623418,
        "rating_q975": 1207.2962458662628
      }
    },
    {
      "Score": 1203.6014841221818,
      "variance": 1.4390075347736533,
      "rating_q975": 1205.8773629603684,
      "rating_q025": 1201.4149715876188,
      "Votes": 55962,
      "Rank (UB)": 138,
      "Model": "gpt-4-0314",
      "95% CI": {
        "rating_q025": 1201.4149715876188,
        "rating_q975": 1205.8773629603684
      }
    },
    {
      "Score": 1202.7932813800098,
      "variance": 22.97549369254058,
      "rating_q975": 1212.94994849847,
      "rating_q025": 1194.1454341270562,
      "Votes": 3074,
      "Rank (UB)": 135,
      "Model": "llama-3.1-tulu-3-8b",
      "95% CI": {
        "rating_q025": 1194.1454341270562,
        "rating_q975": 1212.94994849847
      }
    },
    {
      "Score": 1199.7044965998414,
      "variance": 16.649771378835204,
      "rating_q975": 1207.0795144580873,
      "rating_q025": 1191.6948753957913,
      "Votes": 5111,
      "Rank (UB)": 137,
      "Model": "ministral-8b-2410",
      "95% CI": {
        "rating_q025": 1191.6948753957913,
        "rating_q975": 1207.0795144580873
      }
    },
    {
      "Score": 1197.435427880339,
      "variance": 5.813894763638633,
      "rating_q975": 1201.902978339189,
      "rating_q025": 1192.4556312593081,
      "Votes": 10391,
      "Rank (UB)": 139,
      "Model": "c4ai-aya-expanse-8b",
      "95% CI": {
        "rating_q025": 1192.4556312593081,
        "rating_q975": 1201.902978339189
      }
    },
    {
      "Score": 1197.1062020060476,
      "variance": 7.802271917425133,
      "rating_q975": 1201.9903394091864,
      "rating_q025": 1191.507802153963,
      "Votes": 10851,
      "Rank (UB)": 139,
      "Model": "command-r-08-2024",
      "95% CI": {
        "rating_q025": 1191.507802153963,
        "rating_q975": 1201.9903394091864
      }
    },
    {
      "Score": 1196.7239212152679,
      "variance": 1.101413214811063,
      "rating_q975": 1198.4312820100615,
      "rating_q025": 1194.6331895190651,
      "Votes": 122309,
      "Rank (UB)": 141,
      "Model": "claude-3-haiku-20240307",
      "95% CI": {
        "rating_q025": 1194.6331895190651,
        "rating_q975": 1198.4312820100615
      }
    },
    {
      "Score": 1195.834229611315,
      "variance": 5.815149221673418,
      "rating_q975": 1199.8868072368243,
      "rating_q025": 1191.7840309383464,
      "Votes": 15753,
      "Rank (UB)": 141,
      "Model": "deepseek-coder-v2",
      "95% CI": {
        "rating_q025": 1191.7840309383464,
        "rating_q975": 1199.8868072368243
      }
    },
    {
      "Score": 1193.3547817922972,
      "variance": 9.428036199617896,
      "rating_q975": 1198.602312675084,
      "rating_q025": 1187.3462893613137,
      "Votes": 9274,
      "Rank (UB)": 141,
      "Model": "jamba-1.5-mini",
      "95% CI": {
        "rating_q025": 1187.3462893613137,
        "rating_q975": 1198.602312675084
      }
    },
    {
      "Score": 1193.120271957488,
      "variance": 1.5682692463393435,
      "rating_q975": 1195.6783967947636,
      "rating_q025": 1191.0913268612726,
      "Votes": 52578,
      "Rank (UB)": 143,
      "Model": "llama-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1191.0913268612726,
        "rating_q975": 1195.6783967947636
      }
    },
    {
      "Score": 1180.5131484344556,
      "variance": 1.2996810363546687,
      "rating_q975": 1182.4051141232585,
      "rating_q025": 1178.3936363697405,
      "Votes": 91614,
      "Rank (UB)": 152,
      "Model": "gpt-4-0613",
      "95% CI": {
        "rating_q025": 1178.3936363697405,
        "rating_q975": 1182.4051141232585
      }
    },
    {
      "Score": 1178.7090590637922,
      "variance": 2.7349052826604767,
      "rating_q975": 1181.3829582417536,
      "rating_q025": 1175.220287434445,
      "Votes": 27430,
      "Rank (UB)": 152,
      "Model": "qwen1.5-110b-chat",
      "95% CI": {
        "rating_q025": 1175.220287434445,
        "rating_q975": 1181.3829582417536
      }
    },
    {
      "Score": 1174.8274470170597,
      "variance": 3.872847290303206,
      "rating_q975": 1178.3088802314771,
      "rating_q025": 1170.4483383175843,
      "Votes": 25135,
      "Rank (UB)": 154,
      "Model": "yi-1.5-34b-chat",
      "95% CI": {
        "rating_q025": 1170.4483383175843,
        "rating_q975": 1178.3088802314771
      }
    },
    {
      "Score": 1174.7662509705456,
      "variance": 1.3972448573170788,
      "rating_q975": 1177.1166697316435,
      "rating_q025": 1172.7325397639652,
      "Votes": 64926,
      "Rank (UB)": 154,
      "Model": "mistral-large-2402",
      "95% CI": {
        "rating_q025": 1172.7325397639652,
        "rating_q975": 1177.1166697316435
      }
    },
    {
      "Score": 1173.2421041401626,
      "variance": 6.5570983165928665,
      "rating_q975": 1178.2113796843933,
      "rating_q025": 1168.2895038915192,
      "Votes": 16027,
      "Rank (UB)": 154,
      "Model": "reka-flash-21b-20240226-online",
      "95% CI": {
        "rating_q025": 1168.2895038915192,
        "rating_q975": 1178.2113796843933
      }
    },
    {
      "Score": 1170.2770939427478,
      "variance": 20.638436285735207,
      "rating_q975": 1178.9741963358942,
      "rating_q025": 1162.6747035999904,
      "Votes": 3410,
      "Rank (UB)": 152,
      "Model": "qwq-32b-preview",
      "95% CI": {
        "rating_q025": 1162.6747035999904,
        "rating_q975": 1178.9741963358942
      }
    },
    {
      "Score": 1169.4197543209675,
      "variance": 0.8231365579178462,
      "rating_q975": 1171.0097616526532,
      "rating_q025": 1167.5380626235203,
      "Votes": 109056,
      "Rank (UB)": 156,
      "Model": "llama-3-8b-instruct",
      "95% CI": {
        "rating_q025": 1167.5380626235203,
        "rating_q975": 1171.0097616526532
      }
    },
    {
      "Score": 1166.436356780578,
      "variance": 7.111447856769596,
      "rating_q975": 1171.133004597636,
      "rating_q025": 1160.5699159856842,
      "Votes": 10599,
      "Rank (UB)": 156,
      "Model": "internlm2_5-20b-chat",
      "95% CI": {
        "rating_q025": 1160.5699159856842,
        "rating_q975": 1171.133004597636
      }
    },
    {
      "Score": 1166.0952349587685,
      "variance": 1.5057516833637312,
      "rating_q975": 1168.0848280686942,
      "rating_q025": 1163.5123732830837,
      "Votes": 56398,
      "Rank (UB)": 158,
      "Model": "command-r",
      "95% CI": {
        "rating_q025": 1163.5123732830837,
        "rating_q975": 1168.0848280686942
      }
    },
    {
      "Score": 1165.2976125574055,
      "variance": 2.7080311092888674,
      "rating_q975": 1168.0363327118382,
      "rating_q025": 1162.1141781684046,
      "Votes": 35556,
      "Rank (UB)": 158,
      "Model": "mistral-medium",
      "95% CI": {
        "rating_q025": 1162.1141781684046,
        "rating_q975": 1168.0363327118382
      }
    },
    {
      "Score": 1165.015800395784,
      "variance": 2.102689732771019,
      "rating_q975": 1167.4585927958851,
      "rating_q025": 1161.9428354008237,
      "Votes": 53751,
      "Rank (UB)": 159,
      "Model": "mixtral-8x22b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1161.9428354008237,
        "rating_q975": 1167.4585927958851
      }
    },
    {
      "Score": 1164.8550738370845,
      "variance": 3.30715018515601,
      "rating_q975": 1168.3296953812403,
      "rating_q025": 1161.3999193970783,
      "Votes": 25803,
      "Rank (UB)": 157,
      "Model": "reka-flash-21b-20240226",
      "95% CI": {
        "rating_q025": 1161.3999193970783,
        "rating_q975": 1168.3296953812403
      }
    },
    {
      "Score": 1164.8539929882209,
      "variance": 2.7890621449677466,
      "rating_q975": 1168.1767729361006,
      "rating_q025": 1162.136332914058,
      "Votes": 40658,
      "Rank (UB)": 158,
      "Model": "qwen1.5-72b-chat",
      "95% CI": {
        "rating_q025": 1162.136332914058,
        "rating_q975": 1168.1767729361006
      }
    },
    {
      "Score": 1161.265093387121,
      "variance": 1.5029338513644148,
      "rating_q975": 1163.5408141787082,
      "rating_q025": 1159.3773238103195,
      "Votes": 48892,
      "Rank (UB)": 159,
      "Model": "gemma-2-2b-it",
      "95% CI": {
        "rating_q025": 1159.3773238103195,
        "rating_q975": 1163.5408141787082
      }
    },
    {
      "Score": 1160.266331316823,
      "variance": 23.439932668121543,
      "rating_q975": 1167.855925838624,
      "rating_q025": 1150.7529272948727,
      "Votes": 3289,
      "Rank (UB)": 158,
      "Model": "granite-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1150.7529272948727,
        "rating_q975": 1167.855925838624
      }
    },
    {
      "Score": 1148.7659082186385,
      "variance": 5.743699013243501,
      "rating_q975": 1153.6416697719287,
      "rating_q025": 1144.4537116250988,
      "Votes": 18800,
      "Rank (UB)": 168,
      "Model": "gemini-pro-dev-api",
      "95% CI": {
        "rating_q025": 1144.4537116250988,
        "rating_q975": 1153.6416697719287
      }
    },
    {
      "Score": 1144.5638592888733,
      "variance": 17.59085706406136,
      "rating_q975": 1152.5057705963427,
      "rating_q025": 1135.4555691411663,
      "Votes": 4854,
      "Rank (UB)": 168,
      "Model": "zephyr-orpo-141b-A35b-v0.1",
      "95% CI": {
        "rating_q025": 1135.4555691411663,
        "rating_q975": 1152.5057705963427
      }
    },
    {
      "Score": 1142.7838087742066,
      "variance": 4.053945653617133,
      "rating_q975": 1146.381374999264,
      "rating_q025": 1138.913968570364,
      "Votes": 22765,
      "Rank (UB)": 169,
      "Model": "qwen1.5-32b-chat",
      "95% CI": {
        "rating_q025": 1138.913968570364,
        "rating_q975": 1146.381374999264
      }
    },
    {
      "Score": 1140.3970027115436,
      "variance": 2.842876551455033,
      "rating_q975": 1143.3726121214502,
      "rating_q025": 1136.738490059711,
      "Votes": 26105,
      "Rank (UB)": 171,
      "Model": "phi-3-medium-4k-instruct",
      "95% CI": {
        "rating_q025": 1136.738490059711,
        "rating_q975": 1143.3726121214502
      }
    },
    {
      "Score": 1136.8487045522102,
      "variance": 24.129486040144894,
      "rating_q975": 1143.606191428276,
      "rating_q025": 1128.986202648983,
      "Votes": 3380,
      "Rank (UB)": 171,
      "Model": "granite-3.1-2b-instruct",
      "95% CI": {
        "rating_q025": 1128.986202648983,
        "rating_q975": 1143.606191428276
      }
    },
    {
      "Score": 1136.3583501102544,
      "variance": 5.952829702799031,
      "rating_q975": 1141.3625278777201,
      "rating_q025": 1132.005607707722,
      "Votes": 16676,
      "Rank (UB)": 171,
      "Model": "starling-lm-7b-beta",
      "95% CI": {
        "rating_q025": 1132.005607707722,
        "rating_q975": 1141.3625278777201
      }
    },
    {
      "Score": 1131.4725266519667,
      "variance": 1.4985766100089037,
      "rating_q975": 1133.5457059681132,
      "rating_q025": 1129.0691601520282,
      "Votes": 76126,
      "Rank (UB)": 175,
      "Model": "mixtral-8x7b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1129.0691601520282,
        "rating_q975": 1133.5457059681132
      }
    },
    {
      "Score": 1128.6546462021151,
      "variance": 4.601151624283142,
      "rating_q975": 1133.4399878715187,
      "rating_q025": 1125.2937232189372,
      "Votes": 15917,
      "Rank (UB)": 175,
      "Model": "yi-34b-chat",
      "95% CI": {
        "rating_q025": 1125.2937232189372,
        "rating_q975": 1133.4399878715187
      }
    },
    {
      "Score": 1128.0996525390087,
      "variance": 15.064553827524126,
      "rating_q975": 1135.2296177762637,
      "rating_q025": 1119.9175351231047,
      "Votes": 6557,
      "Rank (UB)": 175,
      "Model": "gemini-pro",
      "95% CI": {
        "rating_q025": 1119.9175351231047,
        "rating_q975": 1135.2296177762637
      }
    },
    {
      "Score": 1126.3916962493508,
      "variance": 4.558074725301311,
      "rating_q975": 1130.3456290322229,
      "rating_q025": 1122.4767958689329,
      "Votes": 18687,
      "Rank (UB)": 178,
      "Model": "qwen1.5-14b-chat",
      "95% CI": {
        "rating_q025": 1122.4767958689329,
        "rating_q975": 1130.3456290322229
      }
    },
    {
      "Score": 1123.9105008474821,
      "variance": 10.03279511935586,
      "rating_q975": 1130.0687063771745,
      "rating_q025": 1118.2052294954544,
      "Votes": 8383,
      "Rank (UB)": 178,
      "Model": "wizardlm-70b",
      "95% CI": {
        "rating_q025": 1118.2052294954544,
        "rating_q975": 1130.0687063771745
      }
    },
    {
      "Score": 1123.3111648324295,
      "variance": 1.6571635257822255,
      "rating_q975": 1125.6193776327652,
      "rating_q025": 1120.9671610641747,
      "Votes": 68867,
      "Rank (UB)": 180,
      "Model": "gpt-3.5-turbo-0125",
      "95% CI": {
        "rating_q025": 1120.9671610641747,
        "rating_q975": 1125.6193776327652
      }
    },
    {
      "Score": 1120.5725937406792,
      "variance": 3.2092814421822924,
      "rating_q975": 1123.7081853204925,
      "rating_q025": 1117.0249738393798,
      "Votes": 33743,
      "Rank (UB)": 182,
      "Model": "dbrx-instruct-preview",
      "95% CI": {
        "rating_q025": 1117.0249738393798,
        "rating_q975": 1123.7081853204925
      }
    },
    {
      "Score": 1120.223158806189,
      "variance": 11.68458574089536,
      "rating_q975": 1127.3881766599586,
      "rating_q025": 1114.0016187295485,
      "Votes": 8390,
      "Rank (UB)": 180,
      "Model": "llama-3.2-3b-instruct",
      "95% CI": {
        "rating_q025": 1114.0016187295485,
        "rating_q975": 1127.3881766599586
      }
    },
    {
      "Score": 1119.434636417132,
      "variance": 3.639326171756199,
      "rating_q975": 1123.4868344836216,
      "rating_q025": 1116.1921722066595,
      "Votes": 18476,
      "Rank (UB)": 182,
      "Model": "phi-3-small-8k-instruct",
      "95% CI": {
        "rating_q025": 1116.1921722066595,
        "rating_q975": 1123.4868344836216
      }
    },
    {
      "Score": 1116.4719810190704,
      "variance": 14.75019865502986,
      "rating_q975": 1123.5482442247644,
      "rating_q025": 1109.9366246430363,
      "Votes": 6658,
      "Rank (UB)": 182,
      "Model": "tulu-2-dpo-70b",
      "95% CI": {
        "rating_q025": 1109.9366246430363,
        "rating_q975": 1123.5482442247644
      }
    },
    {
      "Score": 1110.6015954360823,
      "variance": 13.14157547906438,
      "rating_q975": 1117.7329123795387,
      "rating_q025": 1104.2952411282204,
      "Votes": 7002,
      "Rank (UB)": 186,
      "Model": "granite-3.0-8b-instruct",
      "95% CI": {
        "rating_q025": 1104.2952411282204,
        "rating_q975": 1117.7329123795387
      }
    },
    {
      "Score": 1110.3760243782483,
      "variance": 2.2874115369219203,
      "rating_q975": 1113.547407601238,
      "rating_q025": 1107.6776993437634,
      "Votes": 39595,
      "Rank (UB)": 190,
      "Model": "llama-2-70b-chat",
      "95% CI": {
        "rating_q025": 1107.6776993437634,
        "rating_q975": 1113.547407601238
      }
    },
    {
      "Score": 1108.8556295840353,
      "variance": 5.571746932879748,
      "rating_q975": 1112.7734226687435,
      "rating_q025": 1104.6813182903168,
      "Votes": 12990,
      "Rank (UB)": 190,
      "Model": "openchat-3.5-0106",
      "95% CI": {
        "rating_q025": 1104.6813182903168,
        "rating_q975": 1112.7734226687435
      }
    },
    {
      "Score": 1108.1383364893468,
      "variance": 4.360533554843521,
      "rating_q975": 1112.030763447673,
      "rating_q025": 1103.9075964244796,
      "Votes": 22936,
      "Rank (UB)": 190,
      "Model": "vicuna-33b",
      "95% CI": {
        "rating_q025": 1103.9075964244796,
        "rating_q975": 1112.030763447673
      }
    },
    {
      "Score": 1107.38434428876,
      "variance": 2.276885178861952,
      "rating_q975": 1109.890327272892,
      "rating_q025": 1104.4664138023825,
      "Votes": 34173,
      "Rank (UB)": 191,
      "Model": "snowflake-arctic-instruct",
      "95% CI": {
        "rating_q025": 1104.4664138023825,
        "rating_q975": 1109.890327272892
      }
    },
    {
      "Score": 1105.84490186362,
      "variance": 9.308395121039828,
      "rating_q975": 1111.7431159694154,
      "rating_q025": 1101.282864317069,
      "Votes": 10415,
      "Rank (UB)": 190,
      "Model": "starling-lm-7b-alpha",
      "95% CI": {
        "rating_q025": 1101.282864317069,
        "rating_q975": 1111.7431159694154
      }
    },
    {
      "Score": 1101.5124775785687,
      "variance": 27.418832577340506,
      "rating_q975": 1109.9951658469856,
      "rating_q025": 1092.4186719316356,
      "Votes": 3836,
      "Rank (UB)": 190,
      "Model": "nous-hermes-2-mixtral-8x7b-dpo",
      "95% CI": {
        "rating_q025": 1092.4186719316356,
        "rating_q975": 1109.9951658469856
      }
    },
    {
      "Score": 1101.144434919649,
      "variance": 3.6214494420488292,
      "rating_q975": 1104.456300693511,
      "rating_q025": 1097.662970974017,
      "Votes": 25070,
      "Rank (UB)": 194,
      "Model": "gemma-1.1-7b-it",
      "95% CI": {
        "rating_q025": 1097.662970974017,
        "rating_q975": 1104.456300693511
      }
    },
    {
      "Score": 1098.1150588573373,
      "variance": 22.312904251739425,
      "rating_q975": 1104.7785083382041,
      "rating_q025": 1090.5867072875806,
      "Votes": 3636,
      "Rank (UB)": 192,
      "Model": "llama2-70b-steerlm-chat",
      "95% CI": {
        "rating_q025": 1090.5867072875806,
        "rating_q975": 1104.7785083382041
      }
    },
    {
      "Score": 1094.37841614441,
      "variance": 11.683452067648911,
      "rating_q975": 1099.923650339943,
      "rating_q025": 1087.9400430162973,
      "Votes": 4988,
      "Rank (UB)": 197,
      "Model": "deepseek-llm-67b-chat",
      "95% CI": {
        "rating_q025": 1087.9400430162973,
        "rating_q975": 1099.923650339943
      }
    },
    {
      "Score": 1093.9274811356304,
      "variance": 8.610294568757308,
      "rating_q975": 1099.5734188545346,
      "rating_q025": 1089.3914456623122,
      "Votes": 8106,
      "Rank (UB)": 197,
      "Model": "openchat-3.5",
      "95% CI": {
        "rating_q025": 1089.3914456623122,
        "rating_q975": 1099.5734188545346
      }
    },
    {
      "Score": 1091.6792422237797,
      "variance": 12.992859421578398,
      "rating_q975": 1098.5999485770126,
      "rating_q025": 1084.4199878652887,
      "Votes": 5088,
      "Rank (UB)": 197,
      "Model": "openhermes-2.5-mistral-7b",
      "95% CI": {
        "rating_q025": 1084.4199878652887,
        "rating_q975": 1098.5999485770126
      }
    },
    {
      "Score": 1091.1682874590015,
      "variance": 11.818746867991447,
      "rating_q975": 1097.1609116896134,
      "rating_q025": 1083.793346963778,
      "Votes": 7191,
      "Rank (UB)": 198,
      "Model": "granite-3.0-2b-instruct",
      "95% CI": {
        "rating_q025": 1083.793346963778,
        "rating_q975": 1097.1609116896134
      }
    },
    {
      "Score": 1089.7323459362055,
      "variance": 4.541136407689014,
      "rating_q975": 1093.8772633949995,
      "rating_q025": 1086.0351127546382,
      "Votes": 20067,
      "Rank (UB)": 198,
      "Model": "mistral-7b-instruct-v0.2",
      "95% CI": {
        "rating_q025": 1086.0351127546382,
        "rating_q975": 1093.8772633949995
      }
    },
    {
      "Score": 1088.3020905745461,
      "variance": 8.037726372575163,
      "rating_q975": 1093.1341885370698,
      "rating_q025": 1082.9161514035532,
      "Votes": 12808,
      "Rank (UB)": 198,
      "Model": "phi-3-mini-4k-instruct-june-2024",
      "95% CI": {
        "rating_q025": 1082.9161514035532,
        "rating_q975": 1093.1341885370698
      }
    },
    {
      "Score": 1087.1605929306352,
      "variance": 15.383533498853708,
      "rating_q975": 1094.1713249564966,
      "rating_q025": 1080.4450074557963,
      "Votes": 4872,
      "Rank (UB)": 198,
      "Model": "qwen1.5-7b-chat",
      "95% CI": {
        "rating_q025": 1080.4450074557963,
        "rating_q975": 1094.1713249564966
      }
    },
    {
      "Score": 1085.078594305961,
      "variance": 4.466659083529874,
      "rating_q975": 1089.838721813606,
      "rating_q025": 1081.6603968339973,
      "Votes": 17036,
      "Rank (UB)": 200,
      "Model": "gpt-3.5-turbo-1106",
      "95% CI": {
        "rating_q025": 1081.6603968339973,
        "rating_q975": 1089.838721813606
      }
    },
    {
      "Score": 1083.8853016928197,
      "variance": 4.606766711644862,
      "rating_q975": 1088.0243429906382,
      "rating_q025": 1080.2882807116082,
      "Votes": 21097,
      "Rank (UB)": 202,
      "Model": "phi-3-mini-4k-instruct",
      "95% CI": {
        "rating_q025": 1080.2882807116082,
        "rating_q975": 1088.0243429906382
      }
    },
    {
      "Score": 1080.6304644800546,
      "variance": 4.741205117915176,
      "rating_q975": 1084.537245456846,
      "rating_q025": 1075.9542760717866,
      "Votes": 19722,
      "Rank (UB)": 204,
      "Model": "llama-2-13b-chat",
      "95% CI": {
        "rating_q025": 1075.9542760717866,
        "rating_q975": 1084.537245456846
      }
    },
    {
      "Score": 1079.8469382384842,
      "variance": 54.94434923930787,
      "rating_q975": 1095.7658217444414,
      "rating_q025": 1065.382278477917,
      "Votes": 1714,
      "Rank (UB)": 198,
      "Model": "dolphin-2.2.1-mistral-7b",
      "95% CI": {
        "rating_q025": 1065.382278477917,
        "rating_q975": 1095.7658217444414
      }
    },
    {
      "Score": 1079.5104079166272,
      "variance": 17.116179115705936,
      "rating_q975": 1086.6934956414545,
      "rating_q025": 1072.2687329342084,
      "Votes": 4286,
      "Rank (UB)": 203,
      "Model": "solar-10.7b-instruct-v1.0",
      "95% CI": {
        "rating_q025": 1072.2687329342084,
        "rating_q975": 1086.6934956414545
      }
    },
    {
      "Score": 1076.1685514722583,
      "variance": 14.782228543578823,
      "rating_q975": 1083.0537897912113,
      "rating_q025": 1069.49680519564,
      "Votes": 7176,
      "Rank (UB)": 206,
      "Model": "wizardlm-13b",
      "95% CI": {
        "rating_q025": 1069.49680519564,
        "rating_q975": 1083.0537897912113
      }
    },
    {
      "Score": 1071.1325554226632,
      "variance": 8.943611261863927,
      "rating_q975": 1077.6401829279953,
      "rating_q025": 1066.1312207973103,
      "Votes": 8523,
      "Rank (UB)": 210,
      "Model": "llama-3.2-1b-instruct",
      "95% CI": {
        "rating_q025": 1066.1312207973103,
        "rating_q975": 1077.6401829279953
      }
    },
    {
      "Score": 1070.6744443402836,
      "variance": 10.380137646196578,
      "rating_q975": 1076.337500414378,
      "rating_q025": 1064.968989321997,
      "Votes": 11321,
      "Rank (UB)": 210,
      "Model": "zephyr-7b-beta",
      "95% CI": {
        "rating_q025": 1064.968989321997,
        "rating_q975": 1076.337500414378
      }
    },
    {
      "Score": 1063.669158298045,
      "variance": 34.83918493345402,
      "rating_q975": 1074.3281530557085,
      "rating_q025": 1052.5276548260858,
      "Votes": 2375,
      "Rank (UB)": 211,
      "Model": "smollm2-1.7b-instruct",
      "95% CI": {
        "rating_q025": 1052.5276548260858,
        "rating_q975": 1074.3281530557085
      }
    },
    {
      "Score": 1062.960173628777,
      "variance": 28.14935998412748,
      "rating_q975": 1073.0296448874108,
      "rating_q025": 1053.008716989515,
      "Votes": 2644,
      "Rank (UB)": 211,
      "Model": "mpt-30b-chat",
      "95% CI": {
        "rating_q025": 1053.008716989515,
        "rating_q975": 1073.0296448874108
      }
    },
    {
      "Score": 1060.2116557478814,
      "variance": 14.732474155147505,
      "rating_q975": 1066.934381643014,
      "rating_q025": 1051.3892063920093,
      "Votes": 7509,
      "Rank (UB)": 213,
      "Model": "codellama-34b-instruct",
      "95% CI": {
        "rating_q025": 1051.3892063920093,
        "rating_q975": 1066.934381643014
      }
    },
    {
      "Score": 1059.4708883639064,
      "variance": 5.031110039226218,
      "rating_q975": 1063.6387365708401,
      "rating_q025": 1055.3392252853046,
      "Votes": 19775,
      "Rank (UB)": 216,
      "Model": "vicuna-13b",
      "95% CI": {
        "rating_q025": 1055.3392252853046,
        "rating_q975": 1063.6387365708401
      }
    },
    {
      "Score": 1059.017084592783,
      "variance": 68.29835592516295,
      "rating_q975": 1075.228705926211,
      "rating_q025": 1043.3674715658995,
      "Votes": 1192,
      "Rank (UB)": 211,
      "Model": "codellama-70b-instruct",
      "95% CI": {
        "rating_q025": 1043.3674715658995,
        "rating_q975": 1075.228705926211
      }
    },
    {
      "Score": 1057.9627704944103,
      "variance": 49.378635413286666,
      "rating_q975": 1068.6876669553526,
      "rating_q025": 1042.1301683792456,
      "Votes": 1811,
      "Rank (UB)": 213,
      "Model": "zephyr-7b-alpha",
      "95% CI": {
        "rating_q025": 1042.1301683792456,
        "rating_q975": 1068.6876669553526
      }
    },
    {
      "Score": 1054.8475825949454,
      "variance": 10.041643903767621,
      "rating_q975": 1059.510498359184,
      "rating_q025": 1048.6269589033914,
      "Votes": 9176,
      "Rank (UB)": 216,
      "Model": "gemma-7b-it",
      "95% CI": {
        "rating_q025": 1048.6269589033914,
        "rating_q975": 1059.510498359184
      }
    },
    {
      "Score": 1054.3976609018514,
      "variance": 4.743417348179874,
      "rating_q975": 1058.5614828144553,
      "rating_q025": 1050.2242492029702,
      "Votes": 21622,
      "Rank (UB)": 216,
      "Model": "phi-3-mini-128k-instruct",
      "95% CI": {
        "rating_q025": 1050.2242492029702,
        "rating_q975": 1058.5614828144553
      }
    },
    {
      "Score": 1054.3301826543359,
      "variance": 7.503965094613975,
      "rating_q975": 1058.9777999417768,
      "rating_q025": 1048.4391167199246,
      "Votes": 14532,
      "Rank (UB)": 216,
      "Model": "llama-2-7b-chat",
      "95% CI": {
        "rating_q025": 1048.4391167199246,
        "rating_q975": 1058.9777999417768
      }
    },
    {
      "Score": 1052.3609434520292,
      "variance": 14.633108489315836,
      "rating_q975": 1060.2644725298646,
      "rating_q025": 1045.247838365459,
      "Votes": 5065,
      "Rank (UB)": 216,
      "Model": "qwen-14b-chat",
      "95% CI": {
        "rating_q025": 1045.247838365459,
        "rating_q975": 1060.2644725298646
      }
    },
    {
      "Score": 1051.5700802657987,
      "variance": 67.58490982969577,
      "rating_q975": 1065.3216605118098,
      "rating_q025": 1036.385358718558,
      "Votes": 1327,
      "Rank (UB)": 215,
      "Model": "falcon-180b-chat",
      "95% CI": {
        "rating_q025": 1036.385358718558,
        "rating_q975": 1065.3216605118098
      }
    },
    {
      "Score": 1050.336378221434,
      "variance": 28.91501331104345,
      "rating_q975": 1061.1196579294317,
      "rating_q025": 1040.130533452748,
      "Votes": 2996,
      "Rank (UB)": 216,
      "Model": "guanaco-33b",
      "95% CI": {
        "rating_q025": 1040.130533452748,
        "rating_q975": 1061.1196579294317
      }
    },
    {
      "Score": 1038.0933243628785,
      "variance": 7.348124534732822,
      "rating_q975": 1042.9310207222038,
      "rating_q025": 1032.898116206137,
      "Votes": 11351,
      "Rank (UB)": 226,
      "Model": "gemma-1.1-2b-it",
      "95% CI": {
        "rating_q025": 1032.898116206137,
        "rating_q975": 1042.9310207222038
      }
    },
    {
      "Score": 1034.7162936872005,
      "variance": 15.965471514772963,
      "rating_q975": 1042.1452087309126,
      "rating_q025": 1026.3822165014767,
      "Votes": 5276,
      "Rank (UB)": 226,
      "Model": "stripedhyena-nous-7b",
      "95% CI": {
        "rating_q025": 1026.3822165014767,
        "rating_q975": 1042.1452087309126
      }
    },
    {
      "Score": 1032.595615489265,
      "variance": 13.60234853448481,
      "rating_q975": 1038.7027878420622,
      "rating_q025": 1026.3626126599693,
      "Votes": 6503,
      "Rank (UB)": 228,
      "Model": "olmo-7b-instruct",
      "95% CI": {
        "rating_q025": 1026.3626126599693,
        "rating_q975": 1038.7027878420622
      }
    },
    {
      "Score": 1025.087161962619,
      "variance": 8.77312391109158,
      "rating_q975": 1030.5865512723574,
      "rating_q025": 1019.3273008149954,
      "Votes": 9142,
      "Rank (UB)": 230,
      "Model": "mistral-7b-instruct",
      "95% CI": {
        "rating_q025": 1019.3273008149954,
        "rating_q975": 1030.5865512723574
      }
    },
    {
      "Score": 1022.2605841439319,
      "variance": 10.439541632916614,
      "rating_q975": 1027.9630605091402,
      "rating_q025": 1016.458237221099,
      "Votes": 7017,
      "Rank (UB)": 230,
      "Model": "vicuna-7b",
      "95% CI": {
        "rating_q025": 1016.458237221099,
        "rating_q975": 1027.9630605091402
      }
    },
    {
      "Score": 1020.8690789219503,
      "variance": 10.531568608942571,
      "rating_q975": 1027.4478663586253,
      "rating_q025": 1015.2913025618406,
      "Votes": 8713,
      "Rank (UB)": 230,
      "Model": "palm-2",
      "95% CI": {
        "rating_q025": 1015.2913025618406,
        "rating_q975": 1027.4478663586253
      }
    },
    {
      "Score": 1006.8974016889155,
      "variance": 17.766889866155168,
      "rating_q975": 1015.5883137826636,
      "rating_q025": 1000.3992664102682,
      "Votes": 4918,
      "Rank (UB)": 234,
      "Model": "gemma-2b-it",
      "95% CI": {
        "rating_q025": 1000.3992664102682,
        "rating_q975": 1015.5883137826636
      }
    },
    {
      "Score": 1005.742135483061,
      "variance": 13.908779201799494,
      "rating_q975": 1012.1010726708777,
      "rating_q025": 997.7439870975404,
      "Votes": 7816,
      "Rank (UB)": 235,
      "Model": "qwen1.5-4b-chat",
      "95% CI": {
        "rating_q025": 997.7439870975404,
        "rating_q975": 1012.1010726708777
      }
    },
    {
      "Score": 982.0670480005783,
      "variance": 12.860855083722873,
      "rating_q975": 989.0534859119022,
      "rating_q025": 975.124334322124,
      "Votes": 7020,
      "Rank (UB)": 237,
      "Model": "koala-13b",
      "95% CI": {
        "rating_q025": 975.124334322124,
        "rating_q975": 989.0534859119022
      }
    },
    {
      "Score": 972.2692136296936,
      "variance": 22.2532396986618,
      "rating_q975": 981.036638458332,
      "rating_q025": 963.2059640132377,
      "Votes": 4763,
      "Rank (UB)": 237,
      "Model": "chatglm3-6b",
      "95% CI": {
        "rating_q025": 963.2059640132377,
        "rating_q975": 981.036638458332
      }
    },
    {
      "Score": 949.9289225726975,
      "variance": 73.76962074382875,
      "rating_q975": 964.0722380847186,
      "rating_q025": 934.055664466455,
      "Votes": 1788,
      "Rank (UB)": 238,
      "Model": "gpt4all-13b-snoozy",
      "95% CI": {
        "rating_q025": 934.055664466455,
        "rating_q975": 964.0722380847186
      }
    },
    {
      "Score": 945.6704979706792,
      "variance": 24.752975800918172,
      "rating_q975": 956.1294900889575,
      "rating_q025": 935.0812821727831,
      "Votes": 3997,
      "Rank (UB)": 239,
      "Model": "mpt-7b-chat",
      "95% CI": {
        "rating_q025": 935.0812821727831,
        "rating_q975": 956.1294900889575
      }
    },
    {
      "Score": 941.8043285178967,
      "variance": 34.63336590171036,
      "rating_q975": 954.9454020845604,
      "rating_q025": 931.4146215586887,
      "Votes": 2713,
      "Rank (UB)": 239,
      "Model": "chatglm2-6b",
      "95% CI": {
        "rating_q025": 931.4146215586887,
        "rating_q975": 954.9454020845604
      }
    },
    {
      "Score": 939.1557200183286,
      "variance": 26.066716793561334,
      "rating_q975": 949.4870002705451,
      "rating_q025": 929.9320780377951,
      "Votes": 4920,
      "Rank (UB)": 239,
      "Model": "RWKV-4-Raven-14B",
      "95% CI": {
        "rating_q025": 929.9320780377951,
        "rating_q975": 949.4870002705451
      }
    },
    {
      "Score": 918.969687912742,
      "variance": 18.07532769791537,
      "rating_q975": 927.8528183707252,
      "rating_q025": 910.60040504492,
      "Votes": 5864,
      "Rank (UB)": 243,
      "Model": "alpaca-13b",
      "95% CI": {
        "rating_q025": 910.60040504492,
        "rating_q975": 927.8528183707252
      }
    },
    {
      "Score": 910.5805763076557,
      "variance": 16.752227322063515,
      "rating_q975": 917.4443931978866,
      "rating_q025": 901.5846373508926,
      "Votes": 6368,
      "Rank (UB)": 243,
      "Model": "oasst-pythia-12b",
      "95% CI": {
        "rating_q025": 901.5846373508926,
        "rating_q975": 917.4443931978866
      }
    },
    {
      "Score": 896.5681591617806,
      "variance": 23.302635423116268,
      "rating_q975": 906.6477295009645,
      "rating_q025": 888.221380184092,
      "Votes": 4983,
      "Rank (UB)": 244,
      "Model": "chatglm-6b",
      "95% CI": {
        "rating_q025": 888.221380184092,
        "rating_q975": 906.6477295009645
      }
    },
    {
      "Score": 885.3078782748302,
      "variance": 24.8478882062649,
      "rating_q975": 894.8818491017673,
      "rating_q025": 875.7792002180867,
      "Votes": 4288,
      "Rank (UB)": 245,
      "Model": "fastchat-t5-3b",
      "95% CI": {
        "rating_q025": 875.7792002180867,
        "rating_q975": 894.8818491017673
      }
    },
    {
      "Score": 857.535504554628,
      "variance": 30.442193019950743,
      "rating_q975": 867.0375258255239,
      "rating_q025": 846.7376711106667,
      "Votes": 3336,
      "Rank (UB)": 247,
      "Model": "stablelm-tuned-alpha-7b",
      "95% CI": {
        "rating_q025": 846.7376711106667,
        "rating_q975": 867.0375258255239
      }
    },
    {
      "Score": 839.7569044445515,
      "variance": 30.739047811167136,
      "rating_q975": 850.3280120587765,
      "rating_q025": 827.6029008088303,
      "Votes": 3480,
      "Rank (UB)": 247,
      "Model": "dolly-v2-12b",
      "95% CI": {
        "rating_q025": 827.6029008088303,
        "rating_q975": 850.3280120587765
      }
    },
    {
      "Score": 817.1392187617084,
      "variance": 42.959860907425046,
      "rating_q975": 828.2804273507485,
      "rating_q025": 805.1709832835924,
      "Votes": 2446,
      "Rank (UB)": 248,
      "Model": "llama-13b",
      "95% CI": {
        "rating_q025": 805.1709832835924,
        "rating_q975": 828.2804273507485
      }
    }
  ]
}