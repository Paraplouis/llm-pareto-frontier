{
  "last_updated": "2025-06-17",
  "models": [
    {
      "Score": 1479.016158886852,
      "variance": 13.06161673642211,
      "rating_q975": 1485.7925598476074,
      "rating_q025": 1471.335147540491,
      "Votes": 8454,
      "Rank (UB)": 1,
      "Model": "gemini-2.5-pro-preview-06-05",
      "95% CI": {
        "rating_q025": 1471.335147540491,
        "rating_q975": 1485.7925598476074
      }
    },
    {
      "Score": 1446.0,
      "variance": 5.609432559896081,
      "rating_q975": 1450.7606448223114,
      "rating_q025": 1441.9522574960745,
      "Votes": 12862,
      "Rank (UB)": 2,
      "Model": "gemini-2.5-pro-preview-05-06",
      "95% CI": {
        "rating_q025": 1441.9522574960745,
        "rating_q975": 1450.7606448223114
      }
    },
    {
      "Score": 1426.3419470023339,
      "variance": 7.18983892477359,
      "rating_q975": 1431.0029951177655,
      "rating_q025": 1419.990505732148,
      "Votes": 15817,
      "Rank (UB)": 3,
      "Model": "o3-2025-04-16",
      "95% CI": {
        "rating_q025": 1419.990505732148,
        "rating_q975": 1431.0029951177655
      }
    },
    {
      "Score": 1424.8849645705216,
      "variance": 3.884764705706118,
      "rating_q975": 1428.65323646819,
      "rating_q025": 1420.9804631408915,
      "Votes": 20402,
      "Rank (UB)": 3,
      "Model": "chatgpt-4o-latest-20250326",
      "95% CI": {
        "rating_q025": 1420.9804631408915,
        "rating_q975": 1428.65323646819
      }
    },
    {
      "Score": 1420.2269875497077,
      "variance": 8.85176184304172,
      "rating_q975": 1427.2639123054626,
      "rating_q025": 1415.1331487317661,
      "Votes": 13658,
      "Rank (UB)": 3,
      "Model": "gemini-2.5-flash-preview-05-20",
      "95% CI": {
        "rating_q025": 1415.1331487317661,
        "rating_q975": 1427.2639123054626
      }
    },
    {
      "Score": 1420.0357454337359,
      "variance": 16.750678184440314,
      "rating_q975": 1426.677100006547,
      "rating_q025": 1413.6530641608595,
      "Votes": 8031,
      "Rank (UB)": 3,
      "Model": "deepseek-r1-0528",
      "95% CI": {
        "rating_q025": 1413.6530641608595,
        "rating_q975": 1426.677100006547
      }
    },
    {
      "Score": 1419.2826900268246,
      "variance": 4.177199098656914,
      "rating_q975": 1423.6745233393717,
      "rating_q025": 1415.8258015412919,
      "Votes": 22450,
      "Rank (UB)": 3,
      "Model": "grok-3-preview-02-24",
      "95% CI": {
        "rating_q025": 1415.8258015412919,
        "rating_q975": 1423.6745233393717
      }
    },
    {
      "Score": 1412.8656235912497,
      "variance": 4.696786000045883,
      "rating_q975": 1416.9285467308455,
      "rating_q025": 1409.0837099639386,
      "Votes": 15271,
      "Rank (UB)": 5,
      "Model": "gpt-4.5-preview-2025-02-27",
      "95% CI": {
        "rating_q025": 1409.0837099639386,
        "rating_q975": 1416.9285467308455
      }
    },
    {
      "Score": 1397.259984774901,
      "variance": 5.900889000222997,
      "rating_q975": 1401.9052826888649,
      "rating_q025": 1392.656210464149,
      "Votes": 14634,
      "Rank (UB)": 11,
      "Model": "gemini-2.5-flash-preview-04-17",
      "95% CI": {
        "rating_q025": 1392.656210464149,
        "rating_q975": 1401.9052826888649
      }
    },
    {
      "Score": 1385.863941780267,
      "variance": 11.70513027781861,
      "rating_q975": 1391.4230191457973,
      "rating_q025": 1378.150137372629,
      "Votes": 7369,
      "Rank (UB)": 13,
      "Model": "qwen3-235b-a22b-no-thinking",
      "95% CI": {
        "rating_q025": 1378.150137372629,
        "rating_q975": 1391.4230191457973
      }
    },
    {
      "Score": 1384.6644918711318,
      "variance": 6.400965188164942,
      "rating_q975": 1389.4522626968546,
      "rating_q025": 1379.7151516514082,
      "Votes": 14415,
      "Rank (UB)": 14,
      "Model": "gpt-4.1-2025-04-14",
      "95% CI": {
        "rating_q025": 1379.7151516514082,
        "rating_q975": 1389.4522626968546
      }
    },
    {
      "Score": 1383.3136760215912,
      "variance": 6.416468226505168,
      "rating_q975": 1388.0515255569849,
      "rating_q025": 1378.8225861494363,
      "Votes": 17153,
      "Rank (UB)": 14,
      "Model": "deepseek-v3-0324",
      "95% CI": {
        "rating_q025": 1378.8225861494363,
        "rating_q975": 1388.0515255569849
      }
    },
    {
      "Score": 1373.3164865359856,
      "variance": 14.907780579123976,
      "rating_q975": 1379.3706939522876,
      "rating_q025": 1364.8382568388251,
      "Votes": 6528,
      "Rank (UB)": 17,
      "Model": "hunyuan-turbos-20250416",
      "95% CI": {
        "rating_q025": 1364.8382568388251,
        "rating_q975": 1379.3706939522876
      }
    },
    {
      "Score": 1373.3106695737713,
      "variance": 6.1533345715524845,
      "rating_q975": 1378.270853317255,
      "rating_q025": 1368.960588111352,
      "Votes": 14929,
      "Rank (UB)": 18,
      "Model": "claude-opus-4-20250514",
      "95% CI": {
        "rating_q025": 1368.960588111352,
        "rating_q975": 1378.270853317255
      }
    },
    {
      "Score": 1372.9658864584262,
      "variance": 4.426973181844206,
      "rating_q975": 1377.2129970765025,
      "rating_q025": 1369.2108272278929,
      "Votes": 19430,
      "Rank (UB)": 20,
      "Model": "deepseek-r1",
      "95% CI": {
        "rating_q025": 1369.2108272278929,
        "rating_q975": 1377.2129970765025
      }
    },
    {
      "Score": 1365.1565959724078,
      "variance": 6.060085494174964,
      "rating_q975": 1369.8716140592282,
      "rating_q025": 1360.8492613857113,
      "Votes": 13055,
      "Rank (UB)": 22,
      "Model": "mistral-medium-2505",
      "95% CI": {
        "rating_q025": 1360.8492613857113,
        "rating_q975": 1369.8716140592282
      }
    },
    {
      "Score": 1364.8196422807812,
      "variance": 2.6947032790997247,
      "rating_q975": 1368.0782369058184,
      "rating_q025": 1362.2562350901785,
      "Votes": 29038,
      "Rank (UB)": 24,
      "Model": "o1-2024-12-17",
      "95% CI": {
        "rating_q025": 1362.2562350901785,
        "rating_q975": 1368.0782369058184
      }
    },
    {
      "Score": 1362.9453450344536,
      "variance": 2.9252250499093195,
      "rating_q975": 1365.8531455410318,
      "rating_q025": 1359.30467992232,
      "Votes": 34653,
      "Rank (UB)": 25,
      "Model": "gemini-2.0-flash-001",
      "95% CI": {
        "rating_q025": 1359.30467992232,
        "rating_q975": 1365.8531455410318
      }
    },
    {
      "Score": 1362.3545881836626,
      "variance": 10.250783240944662,
      "rating_q975": 1367.108523868399,
      "rating_q025": 1356.6376759706038,
      "Votes": 11240,
      "Rank (UB)": 24,
      "Model": "qwen3-235b-a22b",
      "95% CI": {
        "rating_q025": 1356.6376759706038,
        "rating_q975": 1367.108523868399
      }
    },
    {
      "Score": 1361.6777866650182,
      "variance": 5.348716265921052,
      "rating_q975": 1366.4154791318654,
      "rating_q025": 1356.5731968423902,
      "Votes": 14169,
      "Rank (UB)": 25,
      "Model": "o4-mini-2025-04-16",
      "95% CI": {
        "rating_q025": 1356.5731968423902,
        "rating_q975": 1366.4154791318654
      }
    },
    {
      "Score": 1361.5714783324825,
      "variance": 12.368245745763874,
      "rating_q975": 1367.8654726328768,
      "rating_q025": 1355.9211933787722,
      "Votes": 6813,
      "Rank (UB)": 24,
      "Model": "grok-3-mini-beta",
      "95% CI": {
        "rating_q025": 1355.9211933787722,
        "rating_q975": 1367.8654726328768
      }
    },
    {
      "Score": 1360.0966872083359,
      "variance": 2.6927141296060464,
      "rating_q975": 1363.1899319694455,
      "rating_q025": 1357.328067773574,
      "Votes": 29902,
      "Rank (UB)": 26,
      "Model": "qwen2.5-max",
      "95% CI": {
        "rating_q025": 1357.328067773574,
        "rating_q975": 1363.1899319694455
      }
    },
    {
      "Score": 1356.2418496762473,
      "variance": 3.874832574071386,
      "rating_q975": 1359.8728288813534,
      "rating_q025": 1352.2148220580823,
      "Votes": 21723,
      "Rank (UB)": 28,
      "Model": "gemma-3-27b-it",
      "95% CI": {
        "rating_q025": 1352.2148220580823,
        "rating_q975": 1359.8728288813534
      }
    },
    {
      "Score": 1349.988423285284,
      "variance": 2.7638769919806117,
      "rating_q975": 1353.2453587512096,
      "rating_q025": 1346.914886686953,
      "Votes": 33177,
      "Rank (UB)": 34,
      "Model": "o1-preview",
      "95% CI": {
        "rating_q025": 1346.914886686953,
        "rating_q975": 1353.2453587512096
      }
    },
    {
      "Score": 1346.3284354333337,
      "variance": 9.973504017497334,
      "rating_q975": 1352.981015884708,
      "rating_q025": 1340.3900106642604,
      "Votes": 11796,
      "Rank (UB)": 34,
      "Model": "claude-sonnet-4-20250514",
      "95% CI": {
        "rating_q025": 1340.3900106642604,
        "rating_q975": 1352.981015884708
      }
    },
    {
      "Score": 1339.5266913941903,
      "variance": 4.369923300494358,
      "rating_q975": 1343.8391605479158,
      "rating_q025": 1334.9322283349013,
      "Votes": 19404,
      "Rank (UB)": 37,
      "Model": "o3-mini-high",
      "95% CI": {
        "rating_q025": 1334.9322283349013,
        "rating_q975": 1343.8391605479158
      }
    },
    {
      "Score": 1337.359284151049,
      "variance": 5.819802005894164,
      "rating_q975": 1342.305793155598,
      "rating_q025": 1333.5397325127053,
      "Votes": 13322,
      "Rank (UB)": 37,
      "Model": "gpt-4.1-mini-2025-04-14",
      "95% CI": {
        "rating_q025": 1333.5397325127053,
        "rating_q975": 1342.305793155598
      }
    },
    {
      "Score": 1335.4665337789845,
      "variance": 24.31463303328492,
      "rating_q975": 1344.6826233445058,
      "rating_q025": 1325.6604532826932,
      "Votes": 3976,
      "Rank (UB)": 37,
      "Model": "gemma-3-12b-it",
      "95% CI": {
        "rating_q025": 1325.6604532826932,
        "rating_q975": 1344.6826233445058
      }
    },
    {
      "Score": 1333.5064022297697,
      "variance": 3.7896343000562625,
      "rating_q975": 1337.321800995407,
      "rating_q025": 1330.2191972381029,
      "Votes": 22841,
      "Rank (UB)": 38,
      "Model": "deepseek-v3",
      "95% CI": {
        "rating_q025": 1330.2191972381029,
        "rating_q975": 1337.321800995407
      }
    },
    {
      "Score": 1331.766641718554,
      "variance": 4.832707561807988,
      "rating_q975": 1335.8389508660025,
      "rating_q025": 1328.5756085447524,
      "Votes": 16280,
      "Rank (UB)": 38,
      "Model": "qwq-32b",
      "95% CI": {
        "rating_q025": 1328.5756085447524,
        "rating_q975": 1335.8389508660025
      }
    },
    {
      "Score": 1327.796942254295,
      "variance": 2.7691418777841306,
      "rating_q975": 1331.2613984277818,
      "rating_q025": 1324.5167939774196,
      "Votes": 26104,
      "Rank (UB)": 40,
      "Model": "gemini-2.0-flash-lite-preview-02-05",
      "95% CI": {
        "rating_q025": 1324.5167939774196,
        "rating_q975": 1331.2613984277818
      }
    },
    {
      "Score": 1326.0086942709747,
      "variance": 14.601223088736738,
      "rating_q975": 1333.3813488225474,
      "rating_q025": 1318.8359853740633,
      "Votes": 6028,
      "Rank (UB)": 40,
      "Model": "glm-4-plus-0111",
      "95% CI": {
        "rating_q025": 1318.8359853740633,
        "rating_q975": 1333.3813488225474
      }
    },
    {
      "Score": 1325.4365548712947,
      "variance": 15.032161596795467,
      "rating_q975": 1332.5447340517894,
      "rating_q025": 1318.1642157582933,
      "Votes": 6055,
      "Rank (UB)": 40,
      "Model": "qwen-plus-0125",
      "95% CI": {
        "rating_q025": 1318.1642157582933,
        "rating_q975": 1332.5447340517894
      }
    },
    {
      "Score": 1324.7023390366228,
      "variance": 4.985985659821702,
      "rating_q975": 1329.0802309308308,
      "rating_q025": 1320.4186376666873,
      "Votes": 20903,
      "Rank (UB)": 41,
      "Model": "command-a-03-2025",
      "95% CI": {
        "rating_q025": 1320.4186376666873,
        "rating_q975": 1329.0802309308308
      }
    },
    {
      "Score": 1320.7941643889833,
      "variance": 2.4018742468979535,
      "rating_q975": 1324.2937334463763,
      "rating_q025": 1318.4251035908703,
      "Votes": 33055,
      "Rank (UB)": 45,
      "Model": "o3-mini",
      "95% CI": {
        "rating_q025": 1318.4251035908703,
        "rating_q975": 1324.2937334463763
      }
    },
    {
      "Score": 1319.7587951197572,
      "variance": 14.237671580973638,
      "rating_q975": 1325.4779140712033,
      "rating_q025": 1312.2252526201275,
      "Votes": 5126,
      "Rank (UB)": 44,
      "Model": "step-2-16k-exp-202412",
      "95% CI": {
        "rating_q025": 1312.2252526201275,
        "rating_q975": 1325.4779140712033
      }
    },
    {
      "Score": 1318.9375037162042,
      "variance": 1.7056304254557872,
      "rating_q975": 1321.2735277265426,
      "rating_q025": 1316.1314070350945,
      "Votes": 54951,
      "Rank (UB)": 45,
      "Model": "o1-mini",
      "95% CI": {
        "rating_q025": 1316.1314070350945,
        "rating_q975": 1321.2735277265426
      }
    },
    {
      "Score": 1317.4799854421199,
      "variance": 25.774246304495374,
      "rating_q975": 1326.0576012599918,
      "rating_q025": 1308.0847367972324,
      "Votes": 2452,
      "Rank (UB)": 43,
      "Model": "hunyuan-turbos-20250226",
      "95% CI": {
        "rating_q025": 1308.0847367972324,
        "rating_q975": 1326.0576012599918
      }
    },
    {
      "Score": 1317.1892706040906,
      "variance": 1.3745821850635622,
      "rating_q975": 1319.095520792028,
      "rating_q025": 1314.9193789032677,
      "Votes": 58645,
      "Rank (UB)": 46,
      "Model": "gemini-1.5-pro-002",
      "95% CI": {
        "rating_q025": 1314.9193789032677,
        "rating_q975": 1319.095520792028
      }
    },
    {
      "Score": 1314.6935689174315,
      "variance": 5.5308092651459875,
      "rating_q975": 1319.431930937071,
      "rating_q025": 1310.7231302296943,
      "Votes": 22170,
      "Rank (UB)": 46,
      "Model": "claude-3-7-sonnet-20250219-thinking-32k",
      "95% CI": {
        "rating_q025": 1310.7231302296943,
        "rating_q975": 1319.431930937071
      }
    },
    {
      "Score": 1311.4443050182335,
      "variance": 35.48309502813421,
      "rating_q975": 1322.3727248033144,
      "rating_q025": 1300.7586546428524,
      "Votes": 2371,
      "Rank (UB)": 45,
      "Model": "llama-3.3-nemotron-49b-super-v1",
      "95% CI": {
        "rating_q025": 1300.7586546428524,
        "rating_q975": 1322.3727248033144
      }
    },
    {
      "Score": 1311.2071653193266,
      "variance": 25.139270965252226,
      "rating_q975": 1321.2054359600177,
      "rating_q025": 1301.5324863600379,
      "Votes": 2510,
      "Rank (UB)": 45,
      "Model": "hunyuan-turbo-0110",
      "95% CI": {
        "rating_q025": 1301.5324863600379,
        "rating_q975": 1321.2054359600177
      }
    },
    {
      "Score": 1307.4604973528742,
      "variance": 3.721818495854507,
      "rating_q975": 1310.9734539713634,
      "rating_q025": 1303.811298349206,
      "Votes": 26662,
      "Rank (UB)": 53,
      "Model": "claude-3-7-sonnet-20250219",
      "95% CI": {
        "rating_q025": 1303.811298349206,
        "rating_q975": 1310.9734539713634
      }
    },
    {
      "Score": 1302.871829457767,
      "variance": 1.56674882933485,
      "rating_q975": 1304.9578266177805,
      "rating_q025": 1300.3956323954487,
      "Votes": 67084,
      "Rank (UB)": 56,
      "Model": "grok-2-2024-08-13",
      "95% CI": {
        "rating_q025": 1300.3956323954487,
        "rating_q975": 1304.9578266177805
      }
    },
    {
      "Score": 1302.8172530955458,
      "variance": 21.966114544241027,
      "rating_q975": 1311.3139682774129,
      "rating_q025": 1294.6813201059026,
      "Votes": 3913,
      "Rank (UB)": 53,
      "Model": "gemma-3n-e4b-it",
      "95% CI": {
        "rating_q025": 1294.6813201059026,
        "rating_q975": 1311.3139682774129
      }
    },
    {
      "Score": 1302.0932163005152,
      "variance": 2.8289787241280875,
      "rating_q975": 1304.896275514983,
      "rating_q025": 1298.5311367441095,
      "Votes": 28968,
      "Rank (UB)": 56,
      "Model": "yi-lightning",
      "95% CI": {
        "rating_q025": 1298.5311367441095,
        "rating_q975": 1304.896275514983
      }
    },
    {
      "Score": 1300.0247136891962,
      "variance": 0.770492007305918,
      "rating_q975": 1301.8173955733305,
      "rating_q025": 1298.426808934337,
      "Votes": 117747,
      "Rank (UB)": 57,
      "Model": "gpt-4o-2024-05-13",
      "95% CI": {
        "rating_q025": 1298.426808934337,
        "rating_q975": 1301.8173955733305
      }
    },
    {
      "Score": 1298.6914486135142,
      "variance": 1.1249140258648869,
      "rating_q975": 1300.5385155140873,
      "rating_q025": 1296.524601576858,
      "Votes": 74002,
      "Rank (UB)": 59,
      "Model": "claude-3-5-sonnet-20241022",
      "95% CI": {
        "rating_q025": 1296.524601576858,
        "rating_q975": 1300.5385155140873
      }
    },
    {
      "Score": 1297.189734494637,
      "variance": 5.309106787694201,
      "rating_q975": 1301.4175117313657,
      "rating_q025": 1292.588942472775,
      "Votes": 10715,
      "Rank (UB)": 58,
      "Model": "qwen2.5-plus-1127",
      "95% CI": {
        "rating_q025": 1292.588942472775,
        "rating_q975": 1301.4175117313657
      }
    },
    {
      "Score": 1294.118082902539,
      "variance": 9.554718569775861,
      "rating_q975": 1300.6658784272474,
      "rating_q025": 1287.7955375156855,
      "Votes": 7243,
      "Rank (UB)": 59,
      "Model": "deepseek-v2.5-1210",
      "95% CI": {
        "rating_q025": 1287.7955375156855,
        "rating_q975": 1300.6658784272474
      }
    },
    {
      "Score": 1290.4217731983242,
      "variance": 2.4641034342256454,
      "rating_q975": 1293.5027086229031,
      "rating_q025": 1287.2928843216996,
      "Votes": 26074,
      "Rank (UB)": 64,
      "Model": "athene-v2-chat",
      "95% CI": {
        "rating_q025": 1287.2928843216996,
        "rating_q975": 1293.5027086229031
      }
    },
    {
      "Score": 1289.9776564488843,
      "variance": 25.756821400943977,
      "rating_q975": 1299.319603642479,
      "rating_q025": 1278.6695295986235,
      "Votes": 4321,
      "Rank (UB)": 60,
      "Model": "gemma-3-4b-it",
      "95% CI": {
        "rating_q025": 1278.6695295986235,
        "rating_q975": 1299.319603642479
      }
    },
    {
      "Score": 1288.995358636352,
      "variance": 2.720449478165988,
      "rating_q975": 1292.4448010322685,
      "rating_q025": 1285.9711594059027,
      "Votes": 27788,
      "Rank (UB)": 65,
      "Model": "glm-4-plus",
      "95% CI": {
        "rating_q025": 1285.9711594059027,
        "rating_q975": 1292.4448010322685
      }
    },
    {
      "Score": 1287.9516888101764,
      "variance": 6.346256847223304,
      "rating_q975": 1292.182734277673,
      "rating_q025": 1282.6233044824125,
      "Votes": 13940,
      "Rank (UB)": 65,
      "Model": "llama-4-maverick-17b-128e-instruct",
      "95% CI": {
        "rating_q025": 1282.6233044824125,
        "rating_q975": 1292.182734277673
      }
    },
    {
      "Score": 1286.9241205455955,
      "variance": 19.934961306093545,
      "rating_q975": 1294.5949082721495,
      "rating_q025": 1276.6815270812162,
      "Votes": 3856,
      "Rank (UB)": 64,
      "Model": "hunyuan-large-2025-02-10",
      "95% CI": {
        "rating_q025": 1276.6815270812162,
        "rating_q975": 1294.5949082721495
      }
    },
    {
      "Score": 1286.7242905170633,
      "variance": 1.3006388583218214,
      "rating_q975": 1288.692186815865,
      "rating_q025": 1284.6765236696747,
      "Votes": 72536,
      "Rank (UB)": 65,
      "Model": "gpt-4o-mini-2024-07-18",
      "95% CI": {
        "rating_q025": 1284.6765236696747,
        "rating_q975": 1288.692186815865
      }
    },
    {
      "Score": 1286.5120499646978,
      "variance": 2.2547339880877972,
      "rating_q975": 1289.0965519681,
      "rating_q025": 1283.113235855403,
      "Votes": 37021,
      "Rank (UB)": 65,
      "Model": "gemini-1.5-flash-002",
      "95% CI": {
        "rating_q025": 1283.113235855403,
        "rating_q975": 1289.0965519681
      }
    },
    {
      "Score": 1285.7321236409803,
      "variance": 12.198267149735535,
      "rating_q975": 1292.495232575474,
      "rating_q025": 1278.955512752179,
      "Votes": 6302,
      "Rank (UB)": 65,
      "Model": "gpt-4.1-nano-2025-04-14",
      "95% CI": {
        "rating_q025": 1278.955512752179,
        "rating_q975": 1292.495232575474
      }
    },
    {
      "Score": 1284.0173229930633,
      "variance": 2.0703327357317716,
      "rating_q975": 1286.2570169678725,
      "rating_q025": 1280.7979181901862,
      "Votes": 43788,
      "Rank (UB)": 67,
      "Model": "llama-3.1-405b-instruct-bf16",
      "95% CI": {
        "rating_q025": 1280.7979181901862,
        "rating_q975": 1286.2570169678725
      }
    },
    {
      "Score": 1283.5893495984828,
      "variance": 10.224260080399292,
      "rating_q975": 1288.8557172490334,
      "rating_q025": 1276.3084469000255,
      "Votes": 7577,
      "Rank (UB)": 65,
      "Model": "llama-3.1-nemotron-70b-instruct",
      "95% CI": {
        "rating_q025": 1276.3084469000255,
        "rating_q975": 1288.8557172490334
      }
    },
    {
      "Score": 1283.262787625026,
      "variance": 1.2976538331552356,
      "rating_q975": 1285.91450642969,
      "rating_q025": 1281.034957878191,
      "Votes": 86159,
      "Rank (UB)": 68,
      "Model": "claude-3-5-sonnet-20240620",
      "95% CI": {
        "rating_q025": 1281.034957878191,
        "rating_q975": 1285.91450642969
      }
    },
    {
      "Score": 1282.5390568168673,
      "variance": 1.4363803097488308,
      "rating_q975": 1285.2814924651684,
      "rating_q025": 1280.3914633208785,
      "Votes": 63038,
      "Rank (UB)": 68,
      "Model": "llama-3.1-405b-instruct-fp8",
      "95% CI": {
        "rating_q025": 1280.3914633208785,
        "rating_q975": 1285.2814924651684
      }
    },
    {
      "Score": 1281.5870254021206,
      "variance": 1.8681903993366016,
      "rating_q975": 1283.9916271890845,
      "rating_q025": 1279.1133899015535,
      "Votes": 52144,
      "Rank (UB)": 69,
      "Model": "gemini-advanced-0514",
      "95% CI": {
        "rating_q025": 1279.1133899015535,
        "rating_q975": 1283.9916271890845
      }
    },
    {
      "Score": 1281.2207534193428,
      "variance": 1.2781323610444077,
      "rating_q975": 1283.3723479891796,
      "rating_q025": 1279.3316938236449,
      "Votes": 55442,
      "Rank (UB)": 69,
      "Model": "grok-2-mini-2024-08-13",
      "95% CI": {
        "rating_q025": 1279.3316938236449,
        "rating_q975": 1283.3723479891796
      }
    },
    {
      "Score": 1280.2865675892076,
      "variance": 1.9962905036769953,
      "rating_q975": 1283.38503068486,
      "rating_q025": 1278.211228013838,
      "Votes": 47973,
      "Rank (UB)": 69,
      "Model": "gpt-4o-2024-08-06",
      "95% CI": {
        "rating_q025": 1278.211228013838,
        "rating_q975": 1283.38503068486
      }
    },
    {
      "Score": 1278.3743657465234,
      "variance": 5.517624840087843,
      "rating_q975": 1282.5486378082333,
      "rating_q025": 1273.7266120335582,
      "Votes": 17432,
      "Rank (UB)": 71,
      "Model": "qwen-max-0919",
      "95% CI": {
        "rating_q025": 1273.7266120335582,
        "rating_q975": 1282.5486378082333
      }
    },
    {
      "Score": 1275.805214613277,
      "variance": 19.46440389440068,
      "rating_q975": 1285.4226627055234,
      "rating_q025": 1267.8632714390656,
      "Votes": 4014,
      "Rank (UB)": 68,
      "Model": "hunyuan-standard-2025-02-10",
      "95% CI": {
        "rating_q025": 1267.8632714390656,
        "rating_q975": 1285.4226627055234
      }
    },
    {
      "Score": 1274.9188746512757,
      "variance": 1.1609880774212569,
      "rating_q975": 1276.8393258306755,
      "rating_q025": 1272.6473244826764,
      "Votes": 82435,
      "Rank (UB)": 80,
      "Model": "gemini-1.5-pro-001",
      "95% CI": {
        "rating_q025": 1272.6473244826764,
        "rating_q975": 1276.8393258306755
      }
    },
    {
      "Score": 1273.1240491234466,
      "variance": 3.7135344442920886,
      "rating_q975": 1276.7960917921466,
      "rating_q025": 1268.7036997077182,
      "Votes": 26344,
      "Rank (UB)": 80,
      "Model": "deepseek-v2.5",
      "95% CI": {
        "rating_q025": 1268.7036997077182,
        "rating_q975": 1276.7960917921466
      }
    },
    {
      "Score": 1272.1197282940554,
      "variance": 2.260624615694589,
      "rating_q975": 1274.6486943849127,
      "rating_q025": 1269.1640876345768,
      "Votes": 41519,
      "Rank (UB)": 83,
      "Model": "qwen2.5-72b-instruct",
      "95% CI": {
        "rating_q025": 1269.1640876345768,
        "rating_q975": 1274.6486943849127
      }
    },
    {
      "Score": 1271.9952253999122,
      "variance": 1.6970815531105288,
      "rating_q975": 1274.7199155722374,
      "rating_q025": 1269.770259030413,
      "Votes": 45238,
      "Rank (UB)": 83,
      "Model": "llama-3.3-70b-instruct",
      "95% CI": {
        "rating_q025": 1269.770259030413,
        "rating_q975": 1274.7199155722374
      }
    },
    {
      "Score": 1271.4887024938687,
      "variance": 1.0326565291700935,
      "rating_q975": 1273.1337574544152,
      "rating_q025": 1269.5808814424859,
      "Votes": 102133,
      "Rank (UB)": 84,
      "Model": "gpt-4-turbo-2024-04-09",
      "95% CI": {
        "rating_q025": 1269.5808814424859,
        "rating_q975": 1273.1337574544152
      }
    },
    {
      "Score": 1266.6073429820458,
      "variance": 1.6208600400725466,
      "rating_q975": 1269.0219901225216,
      "rating_q025": 1263.8475144710349,
      "Votes": 48217,
      "Rank (UB)": 89,
      "Model": "mistral-large-2407",
      "95% CI": {
        "rating_q025": 1263.8475144710349,
        "rating_q975": 1269.0219901225216
      }
    },
    {
      "Score": 1266.3965955377664,
      "variance": 26.851788002566565,
      "rating_q975": 1274.7266517315163,
      "rating_q025": 1256.8967153901851,
      "Votes": 3058,
      "Rank (UB)": 83,
      "Model": "mistral-small-3.1-24b-instruct-2503",
      "95% CI": {
        "rating_q025": 1256.8967153901851,
        "rating_q975": 1274.7266517315163
      }
    },
    {
      "Score": 1265.4036908641287,
      "variance": 3.28311465378864,
      "rating_q975": 1269.086261937478,
      "rating_q025": 1262.476462624734,
      "Votes": 20580,
      "Rank (UB)": 89,
      "Model": "athene-70b-0725",
      "95% CI": {
        "rating_q025": 1262.476462624734,
        "rating_q975": 1269.086261937478
      }
    },
    {
      "Score": 1265.0108000660616,
      "variance": 1.3339407303998336,
      "rating_q975": 1266.9334681714397,
      "rating_q025": 1262.949123799928,
      "Votes": 103748,
      "Rank (UB)": 91,
      "Model": "gpt-4-1106-preview",
      "95% CI": {
        "rating_q025": 1262.949123799928,
        "rating_q975": 1266.9334681714397
      }
    },
    {
      "Score": 1263.9670789833317,
      "variance": 2.1117992991522407,
      "rating_q975": 1266.7860548474218,
      "rating_q025": 1260.7813342703425,
      "Votes": 29633,
      "Rank (UB)": 91,
      "Model": "mistral-large-2411",
      "95% CI": {
        "rating_q025": 1260.7813342703425,
        "rating_q975": 1266.7860548474218
      }
    },
    {
      "Score": 1262.7631279108268,
      "variance": 1.6189857483993293,
      "rating_q975": 1264.8842937467084,
      "rating_q025": 1260.358138871884,
      "Votes": 58637,
      "Rank (UB)": 91,
      "Model": "llama-3.1-70b-instruct",
      "95% CI": {
        "rating_q025": 1260.358138871884,
        "rating_q975": 1264.8842937467084
      }
    },
    {
      "Score": 1262.3174254605237,
      "variance": 0.607681798265259,
      "rating_q975": 1263.766202936358,
      "rating_q025": 1260.8864301064518,
      "Votes": 202641,
      "Rank (UB)": 92,
      "Model": "claude-3-opus-20240229",
      "95% CI": {
        "rating_q025": 1260.8864301064518,
        "rating_q975": 1263.766202936358
      }
    },
    {
      "Score": 1260.0007423269526,
      "variance": 3.605685409235144,
      "rating_q975": 1263.6796683862997,
      "rating_q025": 1256.395993492879,
      "Votes": 26371,
      "Rank (UB)": 92,
      "Model": "amazon-nova-pro-v1.0",
      "95% CI": {
        "rating_q025": 1256.395993492879,
        "rating_q975": 1263.6796683862997
      }
    },
    {
      "Score": 1259.930373487357,
      "variance": 1.0744902661785032,
      "rating_q975": 1261.8683846745091,
      "rating_q025": 1257.695222825141,
      "Votes": 97079,
      "Rank (UB)": 94,
      "Model": "gpt-4-0125-preview",
      "95% CI": {
        "rating_q025": 1257.695222825141,
        "rating_q975": 1261.8683846745091
      }
    },
    {
      "Score": 1259.266493823331,
      "variance": 24.863564212530964,
      "rating_q975": 1267.360992243206,
      "rating_q025": 1249.1500290011882,
      "Votes": 3010,
      "Rank (UB)": 91,
      "Model": "llama-3.1-tulu-3-70b",
      "95% CI": {
        "rating_q025": 1249.1500290011882,
        "rating_q975": 1267.360992243206
      }
    },
    {
      "Score": 1252.9277395320391,
      "variance": 1.74181770189257,
      "rating_q975": 1255.6421278372113,
      "rating_q025": 1250.6338008061189,
      "Votes": 45508,
      "Rank (UB)": 100,
      "Model": "claude-3-5-haiku-20241022",
      "95% CI": {
        "rating_q025": 1250.6338008061189,
        "rating_q975": 1255.6421278372113
      }
    },
    {
      "Score": 1250.1557142541878,
      "variance": 9.660269197302156,
      "rating_q975": 1255.9195912944656,
      "rating_q025": 1243.006107548521,
      "Votes": 7948,
      "Rank (UB)": 100,
      "Model": "reka-core-20240904",
      "95% CI": {
        "rating_q025": 1243.006107548521,
        "rating_q975": 1255.9195912944656
      }
    },
    {
      "Score": 1241.9173349873558,
      "variance": 1.048784207432302,
      "rating_q975": 1243.7940352068936,
      "rating_q025": 1239.7235197477928,
      "Votes": 65661,
      "Rank (UB)": 103,
      "Model": "gemini-1.5-flash-001",
      "95% CI": {
        "rating_q025": 1239.7235197477928,
        "rating_q975": 1243.7940352068936
      }
    },
    {
      "Score": 1236.679867142704,
      "variance": 8.609620168903383,
      "rating_q975": 1242.006919425738,
      "rating_q025": 1231.6808479046629,
      "Votes": 9125,
      "Rank (UB)": 104,
      "Model": "jamba-1.5-large",
      "95% CI": {
        "rating_q025": 1231.6808479046629,
        "rating_q975": 1242.006919425738
      }
    },
    {
      "Score": 1234.8730987551535,
      "variance": 1.182210105295302,
      "rating_q975": 1237.5146410971204,
      "rating_q025": 1233.125009835579,
      "Votes": 79538,
      "Rank (UB)": 107,
      "Model": "gemma-2-27b-it",
      "95% CI": {
        "rating_q025": 1233.125009835579,
        "rating_q975": 1237.5146410971204
      }
    },
    {
      "Score": 1232.4405593270526,
      "variance": 4.672559983263073,
      "rating_q975": 1236.511130785563,
      "rating_q025": 1228.2935315871712,
      "Votes": 15321,
      "Rank (UB)": 107,
      "Model": "mistral-small-24b-instruct-2501",
      "95% CI": {
        "rating_q025": 1228.2935315871712,
        "rating_q975": 1236.511130785563
      }
    },
    {
      "Score": 1232.3033599435742,
      "variance": 12.92707049125001,
      "rating_q975": 1239.8101068731794,
      "rating_q025": 1226.205072049699,
      "Votes": 5730,
      "Rank (UB)": 105,
      "Model": "qwen2.5-coder-32b-instruct",
      "95% CI": {
        "rating_q025": 1226.205072049699,
        "rating_q975": 1239.8101068731794
      }
    },
    {
      "Score": 1231.9323561004155,
      "variance": 3.5607601807245555,
      "rating_q975": 1236.150273791583,
      "rating_q025": 1228.6733881208766,
      "Votes": 20646,
      "Rank (UB)": 107,
      "Model": "amazon-nova-lite-v1.0",
      "95% CI": {
        "rating_q025": 1228.6733881208766,
        "rating_q975": 1236.150273791583
      }
    },
    {
      "Score": 1231.1380800101606,
      "variance": 7.81746402904424,
      "rating_q975": 1237.1190107208824,
      "rating_q025": 1227.3557051341004,
      "Votes": 10548,
      "Rank (UB)": 107,
      "Model": "gemma-2-9b-it-simpo",
      "95% CI": {
        "rating_q025": 1227.3557051341004,
        "rating_q975": 1237.1190107208824
      }
    },
    {
      "Score": 1230.3406340952788,
      "variance": 7.412373210526104,
      "rating_q975": 1235.871347223503,
      "rating_q025": 1225.1381765291792,
      "Votes": 10535,
      "Rank (UB)": 107,
      "Model": "command-r-plus-08-2024",
      "95% CI": {
        "rating_q025": 1225.1381765291792,
        "rating_q975": 1235.871347223503
      }
    },
    {
      "Score": 1227.6427173731067,
      "variance": 2.105795008105379,
      "rating_q975": 1229.7220333616608,
      "rating_q025": 1224.5236810802692,
      "Votes": 37697,
      "Rank (UB)": 110,
      "Model": "gemini-1.5-flash-8b-001",
      "95% CI": {
        "rating_q025": 1224.5236810802692,
        "rating_q975": 1229.7220333616608
      }
    },
    {
      "Score": 1226.8638938279505,
      "variance": 21.623448971690845,
      "rating_q975": 1235.6028873449532,
      "rating_q025": 1217.5814259595613,
      "Votes": 3889,
      "Rank (UB)": 107,
      "Model": "llama-3.1-nemotron-51b-instruct",
      "95% CI": {
        "rating_q025": 1217.5814259595613,
        "rating_q975": 1235.6028873449532
      }
    },
    {
      "Score": 1224.2461736775713,
      "variance": 2.432625164746345,
      "rating_q975": 1226.7403468265409,
      "rating_q025": 1221.1330010058157,
      "Votes": 28768,
      "Rank (UB)": 113,
      "Model": "c4ai-aya-expanse-32b",
      "95% CI": {
        "rating_q025": 1221.1330010058157,
        "rating_q975": 1226.7403468265409
      }
    },
    {
      "Score": 1224.1467042741556,
      "variance": 4.5644182257192805,
      "rating_q975": 1228.109564982036,
      "rating_q025": 1220.3615411818782,
      "Votes": 20608,
      "Rank (UB)": 112,
      "Model": "nemotron-4-340b-instruct",
      "95% CI": {
        "rating_q025": 1220.3615411818782,
        "rating_q975": 1228.109564982036
      }
    },
    {
      "Score": 1221.6860301345532,
      "variance": 8.701353657080054,
      "rating_q975": 1227.8451266904874,
      "rating_q025": 1217.0935749784862,
      "Votes": 10221,
      "Rank (UB)": 112,
      "Model": "glm-4-0520",
      "95% CI": {
        "rating_q025": 1217.0935749784862,
        "rating_q975": 1227.8451266904874
      }
    },
    {
      "Score": 1221.5691375560502,
      "variance": 0.802274788976254,
      "rating_q975": 1222.8153997885006,
      "rating_q025": 1219.9734642757028,
      "Votes": 163629,
      "Rank (UB)": 118,
      "Model": "llama-3-70b-instruct",
      "95% CI": {
        "rating_q025": 1219.9734642757028,
        "rating_q975": 1222.8153997885006
      }
    },
    {
      "Score": 1220.6170109997797,
      "variance": 25.60553758711164,
      "rating_q975": 1228.8677472760548,
      "rating_q025": 1211.0030152632671,
      "Votes": 3460,
      "Rank (UB)": 110,
      "Model": "olmo-2-0325-32b-instruct",
      "95% CI": {
        "rating_q025": 1211.0030152632671,
        "rating_q975": 1228.8677472760548
      }
    },
    {
      "Score": 1220.572581521636,
      "variance": 10.164689423217595,
      "rating_q975": 1226.2394031833705,
      "rating_q025": 1214.273083660749,
      "Votes": 8132,
      "Rank (UB)": 113,
      "Model": "reka-flash-20240904",
      "95% CI": {
        "rating_q025": 1214.273083660749,
        "rating_q975": 1226.2394031833705
      }
    },
    {
      "Score": 1220.409055173789,
      "variance": 2.6979349465365363,
      "rating_q975": 1223.9753271352804,
      "rating_q025": 1217.766218371062,
      "Votes": 25213,
      "Rank (UB)": 118,
      "Model": "phi-4",
      "95% CI": {
        "rating_q025": 1217.766218371062,
        "rating_q975": 1223.9753271352804
      }
    },
    {
      "Score": 1216.0338401858144,
      "variance": 1.0984876942314667,
      "rating_q975": 1218.062888778582,
      "rating_q025": 1214.241243932197,
      "Votes": 113067,
      "Rank (UB)": 121,
      "Model": "claude-3-sonnet-20240229",
      "95% CI": {
        "rating_q025": 1214.241243932197,
        "rating_q975": 1218.062888778582
      }
    },
    {
      "Score": 1213.0128368025569,
      "variance": 3.6095431090641887,
      "rating_q975": 1217.225601407079,
      "rating_q025": 1209.9423929253744,
      "Votes": 20654,
      "Rank (UB)": 124,
      "Model": "amazon-nova-micro-v1.0",
      "95% CI": {
        "rating_q025": 1209.9423929253744,
        "rating_q975": 1217.225601407079
      }
    },
    {
      "Score": 1207.0688612421773,
      "variance": 1.7401178389738476,
      "rating_q975": 1209.6063740155923,
      "rating_q025": 1204.651555655327,
      "Votes": 57197,
      "Rank (UB)": 132,
      "Model": "gemma-2-9b-it",
      "95% CI": {
        "rating_q025": 1204.651555655327,
        "rating_q975": 1209.6063740155923
      }
    },
    {
      "Score": 1204.9706715541058,
      "variance": 1.488905255196125,
      "rating_q975": 1207.363078872053,
      "rating_q025": 1202.9932360709372,
      "Votes": 80846,
      "Rank (UB)": 132,
      "Model": "command-r-plus",
      "95% CI": {
        "rating_q025": 1202.9932360709372,
        "rating_q975": 1207.363078872053
      }
    },
    {
      "Score": 1203.9452598881894,
      "variance": 27.825713173476284,
      "rating_q975": 1213.361842190032,
      "rating_q025": 1193.0592165058574,
      "Votes": 2901,
      "Rank (UB)": 128,
      "Model": "hunyuan-standard-256k",
      "95% CI": {
        "rating_q025": 1193.0592165058574,
        "rating_q975": 1213.361842190032
      }
    },
    {
      "Score": 1202.1107451778157,
      "variance": 2.200176730578102,
      "rating_q975": 1204.8412195320734,
      "rating_q025": 1199.308202641293,
      "Votes": 38872,
      "Rank (UB)": 132,
      "Model": "qwen2-72b-instruct",
      "95% CI": {
        "rating_q025": 1199.308202641293,
        "rating_q975": 1204.8412195320734
      }
    },
    {
      "Score": 1201.197035442416,
      "variance": 2.0595659240937634,
      "rating_q975": 1203.849105594408,
      "rating_q025": 1198.8321670620644,
      "Votes": 55962,
      "Rank (UB)": 133,
      "Model": "gpt-4-0314",
      "95% CI": {
        "rating_q025": 1198.8321670620644,
        "rating_q975": 1203.849105594408
      }
    },
    {
      "Score": 1200.344980735989,
      "variance": 23.29299839899214,
      "rating_q975": 1208.1353441097767,
      "rating_q025": 1190.7219385471742,
      "Votes": 3074,
      "Rank (UB)": 132,
      "Model": "llama-3.1-tulu-3-8b",
      "95% CI": {
        "rating_q025": 1190.7219385471742,
        "rating_q975": 1208.1353441097767
      }
    },
    {
      "Score": 1197.3206945440443,
      "variance": 15.882100761293465,
      "rating_q975": 1204.513283104619,
      "rating_q025": 1190.4383154344134,
      "Votes": 5111,
      "Rank (UB)": 133,
      "Model": "ministral-8b-2410",
      "95% CI": {
        "rating_q025": 1190.4383154344134,
        "rating_q975": 1204.513283104619
      }
    },
    {
      "Score": 1195.030542360612,
      "variance": 8.264606458579449,
      "rating_q975": 1200.2609716176278,
      "rating_q025": 1189.2371771362377,
      "Votes": 10391,
      "Rank (UB)": 134,
      "Model": "c4ai-aya-expanse-8b",
      "95% CI": {
        "rating_q025": 1189.2371771362377,
        "rating_q975": 1200.2609716176278
      }
    },
    {
      "Score": 1194.6896342594569,
      "variance": 7.193592732333109,
      "rating_q975": 1200.187932418849,
      "rating_q025": 1190.4139511061564,
      "Votes": 10851,
      "Rank (UB)": 134,
      "Model": "command-r-08-2024",
      "95% CI": {
        "rating_q025": 1190.4139511061564,
        "rating_q975": 1200.187932418849
      }
    },
    {
      "Score": 1194.322180917775,
      "variance": 1.2653631723242718,
      "rating_q975": 1196.5784047840605,
      "rating_q025": 1192.1771547258531,
      "Votes": 122309,
      "Rank (UB)": 136,
      "Model": "claude-3-haiku-20240307",
      "95% CI": {
        "rating_q025": 1192.1771547258531,
        "rating_q975": 1196.5784047840605
      }
    },
    {
      "Score": 1193.408376388117,
      "variance": 6.086366750609206,
      "rating_q975": 1198.2921034468536,
      "rating_q025": 1188.9874284095044,
      "Votes": 15753,
      "Rank (UB)": 136,
      "Model": "deepseek-coder-v2",
      "95% CI": {
        "rating_q025": 1188.9874284095044,
        "rating_q975": 1198.2921034468536
      }
    },
    {
      "Score": 1190.9596226722672,
      "variance": 8.435441913004277,
      "rating_q975": 1195.979518752877,
      "rating_q025": 1185.8090790798685,
      "Votes": 9274,
      "Rank (UB)": 136,
      "Model": "jamba-1.5-mini",
      "95% CI": {
        "rating_q025": 1185.8090790798685,
        "rating_q975": 1195.979518752877
      }
    },
    {
      "Score": 1190.7056265159995,
      "variance": 2.05972681857117,
      "rating_q975": 1193.1965627978302,
      "rating_q025": 1187.9931923085157,
      "Votes": 52578,
      "Rank (UB)": 136,
      "Model": "llama-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1187.9931923085157,
        "rating_q975": 1193.1965627978302
      }
    },
    {
      "Score": 1178.1056246398175,
      "variance": 1.2947089273177241,
      "rating_q975": 1180.000863756771,
      "rating_q025": 1175.6477669923229,
      "Votes": 91614,
      "Rank (UB)": 147,
      "Model": "gpt-4-0613",
      "95% CI": {
        "rating_q025": 1175.6477669923229,
        "rating_q975": 1180.000863756771
      }
    },
    {
      "Score": 1176.3064997752294,
      "variance": 3.1454055589424734,
      "rating_q975": 1179.959747501623,
      "rating_q025": 1172.9313173421729,
      "Votes": 27430,
      "Rank (UB)": 147,
      "Model": "qwen1.5-110b-chat",
      "95% CI": {
        "rating_q025": 1172.9313173421729,
        "rating_q975": 1179.959747501623
      }
    },
    {
      "Score": 1172.416301306354,
      "variance": 3.575047242710593,
      "rating_q975": 1175.6934316873742,
      "rating_q025": 1168.1334350392601,
      "Votes": 25135,
      "Rank (UB)": 148,
      "Model": "yi-1.5-34b-chat",
      "95% CI": {
        "rating_q025": 1168.1334350392601,
        "rating_q975": 1175.6934316873742
      }
    },
    {
      "Score": 1172.362876128545,
      "variance": 1.7846939889169602,
      "rating_q975": 1174.7693929282261,
      "rating_q025": 1169.9659315389258,
      "Votes": 64926,
      "Rank (UB)": 149,
      "Model": "mistral-large-2402",
      "95% CI": {
        "rating_q025": 1169.9659315389258,
        "rating_q975": 1174.7693929282261
      }
    },
    {
      "Score": 1170.8646108232474,
      "variance": 6.66340249671616,
      "rating_q975": 1175.7881679285597,
      "rating_q025": 1165.684556918046,
      "Votes": 16027,
      "Rank (UB)": 148,
      "Model": "reka-flash-21b-20240226-online",
      "95% CI": {
        "rating_q025": 1165.684556918046,
        "rating_q975": 1175.7881679285597
      }
    },
    {
      "Score": 1167.8718106926626,
      "variance": 26.940140633413584,
      "rating_q975": 1176.4841945876176,
      "rating_q025": 1158.0754775534597,
      "Votes": 3410,
      "Rank (UB)": 148,
      "Model": "qwq-32b-preview",
      "95% CI": {
        "rating_q025": 1158.0754775534597,
        "rating_q975": 1176.4841945876176
      }
    },
    {
      "Score": 1167.0099325876763,
      "variance": 1.0341318303720084,
      "rating_q975": 1168.7910755183802,
      "rating_q025": 1164.9813339823493,
      "Votes": 109056,
      "Rank (UB)": 151,
      "Model": "llama-3-8b-instruct",
      "95% CI": {
        "rating_q025": 1164.9813339823493,
        "rating_q975": 1168.7910755183802
      }
    },
    {
      "Score": 1164.041999183602,
      "variance": 6.826824982718666,
      "rating_q975": 1169.3219625349986,
      "rating_q025": 1158.541535281909,
      "Votes": 10599,
      "Rank (UB)": 151,
      "Model": "internlm2_5-20b-chat",
      "95% CI": {
        "rating_q025": 1158.541535281909,
        "rating_q975": 1169.3219625349986
      }
    },
    {
      "Score": 1163.692178603711,
      "variance": 1.8945718500347748,
      "rating_q975": 1165.8928452249297,
      "rating_q025": 1161.0909636029562,
      "Votes": 56398,
      "Rank (UB)": 152,
      "Model": "command-r",
      "95% CI": {
        "rating_q025": 1161.0909636029562,
        "rating_q975": 1165.8928452249297
      }
    },
    {
      "Score": 1162.8913785717045,
      "variance": 2.7971474459844905,
      "rating_q975": 1165.9727158544308,
      "rating_q025": 1159.5024256963643,
      "Votes": 35556,
      "Rank (UB)": 152,
      "Model": "mistral-medium",
      "95% CI": {
        "rating_q025": 1159.5024256963643,
        "rating_q975": 1165.9727158544308
      }
    },
    {
      "Score": 1162.6095791648982,
      "variance": 2.377755425761238,
      "rating_q975": 1165.319530057883,
      "rating_q025": 1159.56937709833,
      "Votes": 53751,
      "Rank (UB)": 153,
      "Model": "mixtral-8x22b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1159.56937709833,
        "rating_q975": 1165.319530057883
      }
    },
    {
      "Score": 1162.4488240416617,
      "variance": 2.3946251639895455,
      "rating_q975": 1166.1642045528085,
      "rating_q025": 1160.4856449482618,
      "Votes": 40658,
      "Rank (UB)": 152,
      "Model": "qwen1.5-72b-chat",
      "95% CI": {
        "rating_q025": 1160.4856449482618,
        "rating_q975": 1166.1642045528085
      }
    },
    {
      "Score": 1162.448371575983,
      "variance": 4.035936083792431,
      "rating_q975": 1165.8937148721557,
      "rating_q025": 1158.2682166898123,
      "Votes": 25803,
      "Rank (UB)": 152,
      "Model": "reka-flash-21b-20240226",
      "95% CI": {
        "rating_q025": 1158.2682166898123,
        "rating_q975": 1165.8937148721557
      }
    },
    {
      "Score": 1158.8563855417199,
      "variance": 1.4494643942108065,
      "rating_q975": 1161.121507889876,
      "rating_q025": 1156.6451495238887,
      "Votes": 48892,
      "Rank (UB)": 154,
      "Model": "gemma-2-2b-it",
      "95% CI": {
        "rating_q025": 1156.6451495238887,
        "rating_q975": 1161.121507889876
      }
    },
    {
      "Score": 1157.8057242183272,
      "variance": 22.445513056133272,
      "rating_q975": 1166.7945870449116,
      "rating_q025": 1148.4821863479583,
      "Votes": 3289,
      "Rank (UB)": 152,
      "Model": "granite-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1148.4821863479583,
        "rating_q975": 1166.7945870449116
      }
    },
    {
      "Score": 1146.3874821239056,
      "variance": 4.717891206501807,
      "rating_q975": 1150.4249589783828,
      "rating_q025": 1142.9225729237712,
      "Votes": 18800,
      "Rank (UB)": 163,
      "Model": "gemini-pro-dev-api",
      "95% CI": {
        "rating_q025": 1142.9225729237712,
        "rating_q975": 1150.4249589783828
      }
    },
    {
      "Score": 1142.1777769852088,
      "variance": 19.52605389655469,
      "rating_q975": 1150.5304135043107,
      "rating_q025": 1134.5128065776607,
      "Votes": 4854,
      "Rank (UB)": 163,
      "Model": "zephyr-orpo-141b-A35b-v0.1",
      "95% CI": {
        "rating_q025": 1134.5128065776607,
        "rating_q975": 1150.5304135043107
      }
    },
    {
      "Score": 1140.3773346217752,
      "variance": 2.984728648242062,
      "rating_q975": 1143.2902890025625,
      "rating_q025": 1136.8074532099174,
      "Votes": 22765,
      "Rank (UB)": 164,
      "Model": "qwen1.5-32b-chat",
      "95% CI": {
        "rating_q025": 1136.8074532099174,
        "rating_q975": 1143.2902890025625
      }
    },
    {
      "Score": 1137.9752597320112,
      "variance": 3.3318038052742294,
      "rating_q975": 1141.4742722748272,
      "rating_q025": 1134.765356234087,
      "Votes": 26105,
      "Rank (UB)": 165,
      "Model": "phi-3-medium-4k-instruct",
      "95% CI": {
        "rating_q025": 1134.765356234087,
        "rating_q975": 1141.4742722748272
      }
    },
    {
      "Score": 1134.3813630972018,
      "variance": 26.381719030495134,
      "rating_q975": 1144.4205838345636,
      "rating_q025": 1124.225166604492,
      "Votes": 3380,
      "Rank (UB)": 164,
      "Model": "granite-3.1-2b-instruct",
      "95% CI": {
        "rating_q025": 1124.225166604492,
        "rating_q975": 1144.4205838345636
      }
    },
    {
      "Score": 1133.970716570287,
      "variance": 5.139549999861537,
      "rating_q975": 1138.2277223785004,
      "rating_q025": 1129.724854089986,
      "Votes": 16676,
      "Rank (UB)": 166,
      "Model": "starling-lm-7b-beta",
      "95% CI": {
        "rating_q025": 1129.724854089986,
        "rating_q975": 1138.2277223785004
      }
    },
    {
      "Score": 1129.0705216396307,
      "variance": 1.42075365267571,
      "rating_q975": 1130.871397951086,
      "rating_q025": 1126.531730903829,
      "Votes": 76126,
      "Rank (UB)": 170,
      "Model": "mixtral-8x7b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1126.531730903829,
        "rating_q975": 1130.871397951086
      }
    },
    {
      "Score": 1126.2600224908115,
      "variance": 6.776115180824426,
      "rating_q975": 1129.7314069716158,
      "rating_q025": 1120.1399662982456,
      "Votes": 15917,
      "Rank (UB)": 170,
      "Model": "yi-34b-chat",
      "95% CI": {
        "rating_q025": 1120.1399662982456,
        "rating_q975": 1129.7314069716158
      }
    },
    {
      "Score": 1125.6968198313311,
      "variance": 14.008610959092579,
      "rating_q975": 1133.6233550718407,
      "rating_q025": 1119.261438320834,
      "Votes": 6557,
      "Rank (UB)": 170,
      "Model": "gemini-pro",
      "95% CI": {
        "rating_q025": 1119.261438320834,
        "rating_q975": 1133.6233550718407
      }
    },
    {
      "Score": 1123.9971201154663,
      "variance": 5.0252036137146865,
      "rating_q975": 1128.3815700760958,
      "rating_q025": 1119.4427432693558,
      "Votes": 18687,
      "Rank (UB)": 172,
      "Model": "qwen1.5-14b-chat",
      "95% CI": {
        "rating_q025": 1119.4427432693558,
        "rating_q975": 1128.3815700760958
      }
    },
    {
      "Score": 1121.4981753881968,
      "variance": 9.080151217173796,
      "rating_q975": 1127.946386228932,
      "rating_q025": 1115.338537757971,
      "Votes": 8383,
      "Rank (UB)": 172,
      "Model": "wizardlm-70b",
      "95% CI": {
        "rating_q025": 1115.338537757971,
        "rating_q975": 1127.946386228932
      }
    },
    {
      "Score": 1120.9087370273921,
      "variance": 1.671051749115732,
      "rating_q975": 1123.3320696639516,
      "rating_q025": 1117.9921120550289,
      "Votes": 68867,
      "Rank (UB)": 175,
      "Model": "gpt-3.5-turbo-0125",
      "95% CI": {
        "rating_q025": 1117.9921120550289,
        "rating_q975": 1123.3320696639516
      }
    },
    {
      "Score": 1118.1794422613968,
      "variance": 2.846054520229467,
      "rating_q975": 1120.9563825902076,
      "rating_q025": 1114.4272197753626,
      "Votes": 33743,
      "Rank (UB)": 176,
      "Model": "dbrx-instruct-preview",
      "95% CI": {
        "rating_q025": 1114.4272197753626,
        "rating_q975": 1120.9563825902076
      }
    },
    {
      "Score": 1117.8300553453132,
      "variance": 10.538725616978873,
      "rating_q975": 1123.628428536511,
      "rating_q025": 1111.7688826801277,
      "Votes": 8390,
      "Rank (UB)": 175,
      "Model": "llama-3.2-3b-instruct",
      "95% CI": {
        "rating_q025": 1111.7688826801277,
        "rating_q975": 1123.628428536511
      }
    },
    {
      "Score": 1117.0206526189768,
      "variance": 3.5735826813577503,
      "rating_q975": 1120.5562445598396,
      "rating_q025": 1113.8057258509857,
      "Votes": 18476,
      "Rank (UB)": 176,
      "Model": "phi-3-small-8k-instruct",
      "95% CI": {
        "rating_q025": 1113.8057258509857,
        "rating_q975": 1120.5562445598396
      }
    },
    {
      "Score": 1114.069789100654,
      "variance": 11.691430905126003,
      "rating_q975": 1121.369421348917,
      "rating_q025": 1107.6710505829446,
      "Votes": 6658,
      "Rank (UB)": 176,
      "Model": "tulu-2-dpo-70b",
      "95% CI": {
        "rating_q025": 1107.6710505829446,
        "rating_q975": 1121.369421348917
      }
    },
    {
      "Score": 1108.2148140036902,
      "variance": 13.10647481909292,
      "rating_q975": 1115.234816941238,
      "rating_q025": 1101.7781927832593,
      "Votes": 7002,
      "Rank (UB)": 181,
      "Model": "granite-3.0-8b-instruct",
      "95% CI": {
        "rating_q025": 1101.7781927832593,
        "rating_q975": 1115.234816941238
      }
    },
    {
      "Score": 1107.9766538874553,
      "variance": 2.2224862527824896,
      "rating_q975": 1110.5381706326152,
      "rating_q025": 1104.5958410688354,
      "Votes": 39595,
      "Rank (UB)": 185,
      "Model": "llama-2-70b-chat",
      "95% CI": {
        "rating_q025": 1104.5958410688354,
        "rating_q975": 1110.5381706326152
      }
    },
    {
      "Score": 1106.4463523605964,
      "variance": 7.060455225272786,
      "rating_q975": 1111.713149072584,
      "rating_q025": 1101.007329616866,
      "Votes": 12990,
      "Rank (UB)": 184,
      "Model": "openchat-3.5-0106",
      "95% CI": {
        "rating_q025": 1101.007329616866,
        "rating_q975": 1111.713149072584
      }
    },
    {
      "Score": 1105.7413181583925,
      "variance": 3.9269970395521105,
      "rating_q975": 1109.5720394962727,
      "rating_q025": 1102.1983487516363,
      "Votes": 22936,
      "Rank (UB)": 185,
      "Model": "vicuna-33b",
      "95% CI": {
        "rating_q025": 1102.1983487516363,
        "rating_q975": 1109.5720394962727
      }
    },
    {
      "Score": 1104.9867647561605,
      "variance": 3.1868645104757283,
      "rating_q975": 1107.9663669930123,
      "rating_q025": 1101.6409574255329,
      "Votes": 34173,
      "Rank (UB)": 185,
      "Model": "snowflake-arctic-instruct",
      "95% CI": {
        "rating_q025": 1101.6409574255329,
        "rating_q975": 1107.9663669930123
      }
    },
    {
      "Score": 1103.4421378168881,
      "variance": 10.568446055635233,
      "rating_q975": 1109.9918206202738,
      "rating_q025": 1097.984947840495,
      "Votes": 10415,
      "Rank (UB)": 185,
      "Model": "starling-lm-7b-alpha",
      "95% CI": {
        "rating_q025": 1097.984947840495,
        "rating_q975": 1109.9918206202738
      }
    },
    {
      "Score": 1099.1044581299675,
      "variance": 19.172303116340917,
      "rating_q975": 1107.052369197681,
      "rating_q025": 1089.7745423690508,
      "Votes": 3836,
      "Rank (UB)": 186,
      "Model": "nous-hermes-2-mixtral-8x7b-dpo",
      "95% CI": {
        "rating_q025": 1089.7745423690508,
        "rating_q975": 1107.052369197681
      }
    },
    {
      "Score": 1098.735654741474,
      "variance": 4.051762456797943,
      "rating_q975": 1102.1444628176268,
      "rating_q025": 1094.4266203822067,
      "Votes": 25070,
      "Rank (UB)": 188,
      "Model": "gemma-1.1-7b-it",
      "95% CI": {
        "rating_q025": 1094.4266203822067,
        "rating_q975": 1102.1444628176268
      }
    },
    {
      "Score": 1095.6981149147168,
      "variance": 24.080607171110497,
      "rating_q975": 1104.2557663937432,
      "rating_q025": 1085.982317797526,
      "Votes": 3636,
      "Rank (UB)": 187,
      "Model": "llama2-70b-steerlm-chat",
      "95% CI": {
        "rating_q025": 1085.982317797526,
        "rating_q975": 1104.2557663937432
      }
    },
    {
      "Score": 1091.9776687120425,
      "variance": 14.715283130076585,
      "rating_q975": 1099.1383276280274,
      "rating_q025": 1083.7668557175539,
      "Votes": 4988,
      "Rank (UB)": 191,
      "Model": "deepseek-llm-67b-chat",
      "95% CI": {
        "rating_q025": 1083.7668557175539,
        "rating_q975": 1099.1383276280274
      }
    },
    {
      "Score": 1091.530714646738,
      "variance": 10.235446810259031,
      "rating_q975": 1098.1499600775956,
      "rating_q025": 1085.1226960817846,
      "Votes": 8106,
      "Rank (UB)": 191,
      "Model": "openchat-3.5",
      "95% CI": {
        "rating_q025": 1085.1226960817846,
        "rating_q975": 1098.1499600775956
      }
    },
    {
      "Score": 1089.276919413283,
      "variance": 12.776597393305133,
      "rating_q975": 1094.9747915844066,
      "rating_q025": 1081.634310933642,
      "Votes": 5088,
      "Rank (UB)": 192,
      "Model": "openhermes-2.5-mistral-7b",
      "95% CI": {
        "rating_q025": 1081.634310933642,
        "rating_q975": 1094.9747915844066
      }
    },
    {
      "Score": 1088.7842778047857,
      "variance": 10.801533814784706,
      "rating_q975": 1095.144624389922,
      "rating_q025": 1082.9695725607542,
      "Votes": 7191,
      "Rank (UB)": 192,
      "Model": "granite-3.0-2b-instruct",
      "95% CI": {
        "rating_q025": 1082.9695725607542,
        "rating_q975": 1095.144624389922
      }
    },
    {
      "Score": 1087.3384902909397,
      "variance": 4.967591102715632,
      "rating_q975": 1091.4430671881707,
      "rating_q025": 1083.042968744692,
      "Votes": 20067,
      "Rank (UB)": 193,
      "Model": "mistral-7b-instruct-v0.2",
      "95% CI": {
        "rating_q025": 1083.042968744692,
        "rating_q975": 1091.4430671881707
      }
    },
    {
      "Score": 1085.8793923109372,
      "variance": 8.54735027066964,
      "rating_q975": 1090.4135029395034,
      "rating_q025": 1079.2983674437794,
      "Votes": 12808,
      "Rank (UB)": 193,
      "Model": "phi-3-mini-4k-instruct-june-2024",
      "95% CI": {
        "rating_q025": 1079.2983674437794,
        "rating_q975": 1090.4135029395034
      }
    },
    {
      "Score": 1084.790078613502,
      "variance": 14.645902750599175,
      "rating_q975": 1090.5809883532754,
      "rating_q025": 1077.0722380201687,
      "Votes": 4872,
      "Rank (UB)": 193,
      "Model": "qwen1.5-7b-chat",
      "95% CI": {
        "rating_q025": 1077.0722380201687,
        "rating_q975": 1090.5809883532754
      }
    },
    {
      "Score": 1082.677596397227,
      "variance": 7.933343966502132,
      "rating_q975": 1087.8459457249216,
      "rating_q025": 1076.2349425539896,
      "Votes": 17036,
      "Rank (UB)": 194,
      "Model": "gpt-3.5-turbo-1106",
      "95% CI": {
        "rating_q025": 1076.2349425539896,
        "rating_q975": 1087.8459457249216
      }
    },
    {
      "Score": 1081.4685085233286,
      "variance": 5.017911822269697,
      "rating_q975": 1085.5192100466388,
      "rating_q025": 1077.2139367220552,
      "Votes": 21097,
      "Rank (UB)": 196,
      "Model": "phi-3-mini-4k-instruct",
      "95% CI": {
        "rating_q025": 1077.2139367220552,
        "rating_q975": 1085.5192100466388
      }
    },
    {
      "Score": 1078.235068712635,
      "variance": 5.282224711295922,
      "rating_q975": 1082.9297453357392,
      "rating_q025": 1073.8685371169393,
      "Votes": 19722,
      "Rank (UB)": 200,
      "Model": "llama-2-13b-chat",
      "95% CI": {
        "rating_q025": 1073.8685371169393,
        "rating_q975": 1082.9297453357392
      }
    },
    {
      "Score": 1077.421112777306,
      "variance": 44.60130912531588,
      "rating_q975": 1090.284340295519,
      "rating_q025": 1065.2393220223264,
      "Votes": 1714,
      "Rank (UB)": 193,
      "Model": "dolphin-2.2.1-mistral-7b",
      "95% CI": {
        "rating_q025": 1065.2393220223264,
        "rating_q975": 1090.284340295519
      }
    },
    {
      "Score": 1077.1050766196063,
      "variance": 19.052024975199178,
      "rating_q975": 1085.3213162561199,
      "rating_q025": 1068.2759023360848,
      "Votes": 4286,
      "Rank (UB)": 196,
      "Model": "solar-10.7b-instruct-v1.0",
      "95% CI": {
        "rating_q025": 1068.2759023360848,
        "rating_q975": 1085.3213162561199
      }
    },
    {
      "Score": 1073.754421099482,
      "variance": 11.388970753189446,
      "rating_q975": 1080.2021727921388,
      "rating_q025": 1067.935333639561,
      "Votes": 7176,
      "Rank (UB)": 201,
      "Model": "wizardlm-13b",
      "95% CI": {
        "rating_q025": 1067.935333639561,
        "rating_q975": 1080.2021727921388
      }
    },
    {
      "Score": 1068.7441425083707,
      "variance": 10.724626994010748,
      "rating_q975": 1075.6031646365507,
      "rating_q025": 1063.478060971675,
      "Votes": 8523,
      "Rank (UB)": 205,
      "Model": "llama-3.2-1b-instruct",
      "95% CI": {
        "rating_q025": 1063.478060971675,
        "rating_q975": 1075.6031646365507
      }
    },
    {
      "Score": 1068.2707414391214,
      "variance": 7.839647880635834,
      "rating_q975": 1073.4788708863039,
      "rating_q025": 1062.9808601223192,
      "Votes": 11321,
      "Rank (UB)": 206,
      "Model": "zephyr-7b-beta",
      "95% CI": {
        "rating_q025": 1062.9808601223192,
        "rating_q975": 1073.4788708863039
      }
    },
    {
      "Score": 1061.3444551503635,
      "variance": 45.59259162648684,
      "rating_q975": 1073.4852480160957,
      "rating_q025": 1048.05911595307,
      "Votes": 2375,
      "Rank (UB)": 206,
      "Model": "smollm2-1.7b-instruct",
      "95% CI": {
        "rating_q025": 1048.05911595307,
        "rating_q975": 1073.4852480160957
      }
    },
    {
      "Score": 1060.5512071086114,
      "variance": 34.07837472205696,
      "rating_q975": 1071.6023749632047,
      "rating_q025": 1049.9108690616058,
      "Votes": 2644,
      "Rank (UB)": 206,
      "Model": "mpt-30b-chat",
      "95% CI": {
        "rating_q025": 1049.9108690616058,
        "rating_q975": 1071.6023749632047
      }
    },
    {
      "Score": 1057.814710965571,
      "variance": 13.919416652445468,
      "rating_q975": 1065.0327274215272,
      "rating_q025": 1051.6544090918671,
      "Votes": 7509,
      "Rank (UB)": 209,
      "Model": "codellama-34b-instruct",
      "95% CI": {
        "rating_q025": 1051.6544090918671,
        "rating_q975": 1065.0327274215272
      }
    },
    {
      "Score": 1057.070064361552,
      "variance": 5.5466348018363325,
      "rating_q975": 1061.3240226353246,
      "rating_q025": 1052.1417539484708,
      "Votes": 19775,
      "Rank (UB)": 211,
      "Model": "vicuna-13b",
      "95% CI": {
        "rating_q025": 1052.1417539484708,
        "rating_q975": 1061.3240226353246
      }
    },
    {
      "Score": 1056.6007598981116,
      "variance": 60.35614787262691,
      "rating_q975": 1070.892642655381,
      "rating_q025": 1041.350918002717,
      "Votes": 1192,
      "Rank (UB)": 206,
      "Model": "codellama-70b-instruct",
      "95% CI": {
        "rating_q025": 1041.350918002717,
        "rating_q975": 1070.892642655381
      }
    },
    {
      "Score": 1055.5275560805753,
      "variance": 52.60514821420557,
      "rating_q975": 1068.9595263363803,
      "rating_q025": 1040.901320900242,
      "Votes": 1811,
      "Rank (UB)": 206,
      "Model": "zephyr-7b-alpha",
      "95% CI": {
        "rating_q025": 1040.901320900242,
        "rating_q975": 1068.9595263363803
      }
    },
    {
      "Score": 1052.4703060244638,
      "variance": 10.562323732605424,
      "rating_q975": 1059.0600783090067,
      "rating_q025": 1045.843981821523,
      "Votes": 9176,
      "Rank (UB)": 211,
      "Model": "gemma-7b-it",
      "95% CI": {
        "rating_q025": 1045.843981821523,
        "rating_q975": 1059.0600783090067
      }
    },
    {
      "Score": 1051.9780681750915,
      "variance": 4.145372610823108,
      "rating_q975": 1056.4836863910996,
      "rating_q025": 1048.5456461856531,
      "Votes": 21622,
      "Rank (UB)": 211,
      "Model": "phi-3-mini-128k-instruct",
      "95% CI": {
        "rating_q025": 1048.5456461856531,
        "rating_q975": 1056.4836863910996
      }
    },
    {
      "Score": 1051.9545647703562,
      "variance": 5.94838808475493,
      "rating_q975": 1057.2001795364076,
      "rating_q025": 1047.7786015375855,
      "Votes": 14532,
      "Rank (UB)": 211,
      "Model": "llama-2-7b-chat",
      "95% CI": {
        "rating_q025": 1047.7786015375855,
        "rating_q975": 1057.2001795364076
      }
    },
    {
      "Score": 1049.9636891860762,
      "variance": 19.051824046256947,
      "rating_q975": 1056.9974577472672,
      "rating_q025": 1041.6114547917628,
      "Votes": 5065,
      "Rank (UB)": 211,
      "Model": "qwen-14b-chat",
      "95% CI": {
        "rating_q025": 1041.6114547917628,
        "rating_q975": 1056.9974577472672
      }
    },
    {
      "Score": 1049.12837775171,
      "variance": 57.11779279075522,
      "rating_q975": 1063.5157977131819,
      "rating_q025": 1033.455437113679,
      "Votes": 1327,
      "Rank (UB)": 209,
      "Model": "falcon-180b-chat",
      "95% CI": {
        "rating_q025": 1033.455437113679,
        "rating_q975": 1063.5157977131819
      }
    },
    {
      "Score": 1047.9230926143691,
      "variance": 23.8463044302578,
      "rating_q975": 1057.2008647462214,
      "rating_q025": 1038.0165521138654,
      "Votes": 2996,
      "Rank (UB)": 211,
      "Model": "guanaco-33b",
      "95% CI": {
        "rating_q025": 1038.0165521138654,
        "rating_q975": 1057.2008647462214
      }
    },
    {
      "Score": 1035.729625828213,
      "variance": 9.473692252186543,
      "rating_q975": 1040.7837321872069,
      "rating_q025": 1029.7853948782606,
      "Votes": 11351,
      "Rank (UB)": 222,
      "Model": "gemma-1.1-2b-it",
      "95% CI": {
        "rating_q025": 1029.7853948782606,
        "rating_q975": 1040.7837321872069
      }
    },
    {
      "Score": 1032.3244399953242,
      "variance": 19.927779776219595,
      "rating_q975": 1040.7949785190146,
      "rating_q025": 1024.1172302415869,
      "Votes": 5276,
      "Rank (UB)": 222,
      "Model": "stripedhyena-nous-7b",
      "95% CI": {
        "rating_q025": 1024.1172302415869,
        "rating_q975": 1040.7949785190146
      }
    },
    {
      "Score": 1030.2236049966127,
      "variance": 16.29231494372325,
      "rating_q975": 1036.9873994053094,
      "rating_q025": 1021.1642009039412,
      "Votes": 6503,
      "Rank (UB)": 223,
      "Model": "olmo-7b-instruct",
      "95% CI": {
        "rating_q025": 1021.1642009039412,
        "rating_q975": 1036.9873994053094
      }
    },
    {
      "Score": 1022.6950093491121,
      "variance": 9.53414895399274,
      "rating_q975": 1029.401237279817,
      "rating_q025": 1017.498014686595,
      "Votes": 9142,
      "Rank (UB)": 225,
      "Model": "mistral-7b-instruct",
      "95% CI": {
        "rating_q025": 1017.498014686595,
        "rating_q975": 1029.401237279817
      }
    },
    {
      "Score": 1019.8440351871545,
      "variance": 13.752439126776975,
      "rating_q975": 1025.7814238215128,
      "rating_q025": 1012.7153643235919,
      "Votes": 7017,
      "Rank (UB)": 225,
      "Model": "vicuna-7b",
      "95% CI": {
        "rating_q025": 1012.7153643235919,
        "rating_q975": 1025.7814238215128
      }
    },
    {
      "Score": 1018.4562582244337,
      "variance": 13.826575160429714,
      "rating_q975": 1024.2929459925265,
      "rating_q025": 1011.601482007512,
      "Votes": 8713,
      "Rank (UB)": 225,
      "Model": "palm-2",
      "95% CI": {
        "rating_q025": 1011.601482007512,
        "rating_q975": 1024.2929459925265
      }
    },
    {
      "Score": 1004.5670207927672,
      "variance": 14.520363904598497,
      "rating_q975": 1011.467374914753,
      "rating_q025": 997.8831437608484,
      "Votes": 4918,
      "Rank (UB)": 230,
      "Model": "gemma-2b-it",
      "95% CI": {
        "rating_q025": 997.8831437608484,
        "rating_q975": 1011.467374914753
      }
    },
    {
      "Score": 1003.366229450864,
      "variance": 9.39822412863439,
      "rating_q975": 1009.7725056707332,
      "rating_q025": 998.9584696157954,
      "Votes": 7816,
      "Rank (UB)": 230,
      "Model": "qwen1.5-4b-chat",
      "95% CI": {
        "rating_q025": 998.9584696157954,
        "rating_q975": 1009.7725056707332
      }
    },
    {
      "Score": 979.6288796997892,
      "variance": 17.301197268418893,
      "rating_q975": 985.938556290149,
      "rating_q025": 970.1872702492047,
      "Votes": 7020,
      "Rank (UB)": 232,
      "Model": "koala-13b",
      "95% CI": {
        "rating_q025": 970.1872702492047,
        "rating_q975": 985.938556290149
      }
    },
    {
      "Score": 969.8881083470192,
      "variance": 15.073846739110602,
      "rating_q975": 976.7998294218917,
      "rating_q025": 962.1722622350399,
      "Votes": 4763,
      "Rank (UB)": 232,
      "Model": "chatglm3-6b",
      "95% CI": {
        "rating_q025": 962.1722622350399,
        "rating_q975": 976.7998294218917
      }
    },
    {
      "Score": 947.4143013545836,
      "variance": 59.71461542183258,
      "rating_q975": 961.6910095203098,
      "rating_q025": 930.5147267073751,
      "Votes": 1788,
      "Rank (UB)": 234,
      "Model": "gpt4all-13b-snoozy",
      "95% CI": {
        "rating_q025": 930.5147267073751,
        "rating_q975": 961.6910095203098
      }
    },
    {
      "Score": 943.2525596068404,
      "variance": 19.892863298448074,
      "rating_q975": 951.1511699176307,
      "rating_q025": 934.830528384348,
      "Votes": 3997,
      "Rank (UB)": 234,
      "Model": "mpt-7b-chat",
      "95% CI": {
        "rating_q025": 934.830528384348,
        "rating_q975": 951.1511699176307
      }
    },
    {
      "Score": 939.4619564425643,
      "variance": 34.12924387151473,
      "rating_q975": 949.6302653100795,
      "rating_q025": 928.6140092405857,
      "Votes": 2713,
      "Rank (UB)": 234,
      "Model": "chatglm2-6b",
      "95% CI": {
        "rating_q025": 928.6140092405857,
        "rating_q975": 949.6302653100795
      }
    },
    {
      "Score": 936.7241221357499,
      "variance": 15.159410351798881,
      "rating_q975": 944.7887888289443,
      "rating_q025": 928.4844192573365,
      "Votes": 4920,
      "Rank (UB)": 234,
      "Model": "RWKV-4-Raven-14B",
      "95% CI": {
        "rating_q025": 928.4844192573365,
        "rating_q975": 944.7887888289443
      }
    },
    {
      "Score": 916.5351426507549,
      "variance": 18.12773192228333,
      "rating_q975": 924.8692179521122,
      "rating_q025": 908.7678559322013,
      "Votes": 5864,
      "Rank (UB)": 238,
      "Model": "alpaca-13b",
      "95% CI": {
        "rating_q025": 908.7678559322013,
        "rating_q975": 924.8692179521122
      }
    },
    {
      "Score": 908.1487915503312,
      "variance": 15.6356623410483,
      "rating_q975": 915.8259598544286,
      "rating_q025": 900.1153906919458,
      "Votes": 6368,
      "Rank (UB)": 238,
      "Model": "oasst-pythia-12b",
      "95% CI": {
        "rating_q025": 900.1153906919458,
        "rating_q975": 915.8259598544286
      }
    },
    {
      "Score": 894.1452302953113,
      "variance": 22.45407748090372,
      "rating_q975": 904.1610082870199,
      "rating_q025": 884.7044975261227,
      "Votes": 4983,
      "Rank (UB)": 239,
      "Model": "chatglm-6b",
      "95% CI": {
        "rating_q025": 884.7044975261227,
        "rating_q975": 904.1610082870199
      }
    },
    {
      "Score": 882.8960274785532,
      "variance": 22.425190230304494,
      "rating_q975": 890.0211684199065,
      "rating_q025": 873.0670806433317,
      "Votes": 4288,
      "Rank (UB)": 240,
      "Model": "fastchat-t5-3b",
      "95% CI": {
        "rating_q025": 873.0670806433317,
        "rating_q975": 890.0211684199065
      }
    },
    {
      "Score": 855.0931974618011,
      "variance": 29.263405294743368,
      "rating_q975": 863.8125817596978,
      "rating_q025": 844.0363915240716,
      "Votes": 3336,
      "Rank (UB)": 242,
      "Model": "stablelm-tuned-alpha-7b",
      "95% CI": {
        "rating_q025": 844.0363915240716,
        "rating_q975": 863.8125817596978
      }
    },
    {
      "Score": 837.3148647780149,
      "variance": 31.12135666335457,
      "rating_q975": 847.6509826351503,
      "rating_q025": 826.2784599060845,
      "Votes": 3480,
      "Rank (UB)": 242,
      "Model": "dolly-v2-12b",
      "95% CI": {
        "rating_q025": 826.2784599060845,
        "rating_q975": 847.6509826351503
      }
    },
    {
      "Score": 814.6757122420952,
      "variance": 36.13193292226522,
      "rating_q975": 824.3228146821173,
      "rating_q025": 801.9449749174586,
      "Votes": 2446,
      "Rank (UB)": 244,
      "Model": "llama-13b",
      "95% CI": {
        "rating_q025": 801.9449749174586,
        "rating_q975": 824.3228146821173
      }
    }
  ]
}