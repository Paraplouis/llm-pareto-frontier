{
  "last_updated": "2025-06-05",
  "models": [
    {
      "Score": 1476.3420803800166,
      "variance": 18.594226063873183,
      "rating_q975": 1484.2091640620117,
      "rating_q025": 1468.689697388923,
      "Votes": 4701,
      "Rank (UB)": 1,
      "Model": "gemini-2.5-pro-preview-06-05",
      "95% CI": {
        "rating_q025": 1468.689697388923,
        "rating_q975": 1484.2091640620117
      }
    },
    {
      "Score": 1446.0,
      "variance": 8.996757734186799,
      "rating_q975": 1450.9132409320741,
      "rating_q025": 1438.7285479906138,
      "Votes": 10386,
      "Rank (UB)": 2,
      "Model": "gemini-2.5-pro-preview-05-06",
      "95% CI": {
        "rating_q025": 1438.7285479906138,
        "rating_q975": 1450.9132409320741
      }
    },
    {
      "Score": 1420.3019780873603,
      "variance": 12.458715489721694,
      "rating_q975": 1425.7992673236604,
      "rating_q025": 1411.722094606655,
      "Votes": 9970,
      "Rank (UB)": 3,
      "Model": "gemini-2.5-flash-preview-05-20",
      "95% CI": {
        "rating_q025": 1411.722094606655,
        "rating_q975": 1425.7992673236604
      }
    },
    {
      "Score": 1420.2369417333446,
      "variance": 5.41296604478375,
      "rating_q975": 1424.6097844499618,
      "rating_q025": 1416.4695148003248,
      "Votes": 13808,
      "Rank (UB)": 3,
      "Model": "o3-2025-04-16",
      "95% CI": {
        "rating_q025": 1416.4695148003248,
        "rating_q975": 1424.6097844499618
      }
    },
    {
      "Score": 1416.8610079078715,
      "variance": 4.787989577386147,
      "rating_q975": 1420.8321688900025,
      "rating_q025": 1412.683544788997,
      "Votes": 18302,
      "Rank (UB)": 3,
      "Model": "chatgpt-4o-latest-20250326",
      "95% CI": {
        "rating_q025": 1412.683544788997,
        "rating_q975": 1420.8321688900025
      }
    },
    {
      "Score": 1412.2712159517382,
      "variance": 4.934539425069475,
      "rating_q975": 1416.181839442359,
      "rating_q025": 1407.8939740391884,
      "Votes": 20585,
      "Rank (UB)": 4,
      "Model": "grok-3-preview-02-24",
      "95% CI": {
        "rating_q025": 1407.8939740391884,
        "rating_q975": 1416.181839442359
      }
    },
    {
      "Score": 1406.123325194831,
      "variance": 5.745236744451512,
      "rating_q975": 1411.5493730525602,
      "rating_q025": 1402.0748811557116,
      "Votes": 15271,
      "Rank (UB)": 6,
      "Model": "gpt-4.5-preview-2025-02-27",
      "95% CI": {
        "rating_q025": 1402.0748811557116,
        "rating_q975": 1411.5493730525602
      }
    },
    {
      "Score": 1392.3573712913644,
      "variance": 5.846201492930744,
      "rating_q975": 1396.376864890942,
      "rating_q025": 1387.4621253879307,
      "Votes": 13320,
      "Rank (UB)": 10,
      "Model": "gemini-2.5-flash-preview-04-17",
      "95% CI": {
        "rating_q025": 1387.4621253879307,
        "rating_q975": 1396.376864890942
      }
    },
    {
      "Score": 1377.0665965882126,
      "variance": 7.341408651721758,
      "rating_q975": 1382.3555503043785,
      "rating_q025": 1372.4795078761856,
      "Votes": 12421,
      "Rank (UB)": 13,
      "Model": "gpt-4.1-2025-04-14",
      "95% CI": {
        "rating_q025": 1372.4795078761856,
        "rating_q975": 1382.3555503043785
      }
    },
    {
      "Score": 1376.9630912440512,
      "variance": 4.918265100117205,
      "rating_q975": 1381.1075294717693,
      "rating_q025": 1373.3118392837034,
      "Votes": 15122,
      "Rank (UB)": 13,
      "Model": "deepseek-v3-0324",
      "95% CI": {
        "rating_q025": 1373.3118392837034,
        "rating_q975": 1381.1075294717693
      }
    },
    {
      "Score": 1366.4005695071587,
      "variance": 9.304080094527313,
      "rating_q975": 1372.9359279557827,
      "rating_q025": 1361.1985958459718,
      "Votes": 10402,
      "Rank (UB)": 16,
      "Model": "claude-opus-4-20250514",
      "95% CI": {
        "rating_q025": 1361.1985958459718,
        "rating_q975": 1372.9359279557827
      }
    },
    {
      "Score": 1366.3196134001078,
      "variance": 4.7756801389369254,
      "rating_q975": 1370.399143913912,
      "rating_q025": 1362.1448228646977,
      "Votes": 19430,
      "Rank (UB)": 18,
      "Model": "deepseek-r1",
      "95% CI": {
        "rating_q025": 1362.1448228646977,
        "rating_q975": 1370.399143913912
      }
    },
    {
      "Score": 1366.0381298256432,
      "variance": 15.783188564741787,
      "rating_q975": 1374.6980371447842,
      "rating_q025": 1358.3036444656987,
      "Votes": 5230,
      "Rank (UB)": 15,
      "Model": "hunyuan-turbos-20250416",
      "95% CI": {
        "rating_q025": 1358.3036444656987,
        "rating_q975": 1374.6980371447842
      }
    },
    {
      "Score": 1358.041236846653,
      "variance": 3.066999943503029,
      "rating_q975": 1361.2004087232917,
      "rating_q025": 1354.5800961001269,
      "Votes": 29038,
      "Rank (UB)": 21,
      "Model": "o1-2024-12-17",
      "95% CI": {
        "rating_q025": 1354.5800961001269,
        "rating_q975": 1361.2004087232917
      }
    },
    {
      "Score": 1357.6673251667635,
      "variance": 2.5499033133710443,
      "rating_q975": 1360.173346558241,
      "rating_q025": 1354.11501539579,
      "Votes": 33324,
      "Rank (UB)": 23,
      "Model": "gemini-2.0-flash-001",
      "95% CI": {
        "rating_q025": 1354.11501539579,
        "rating_q975": 1360.173346558241
      }
    },
    {
      "Score": 1356.565487717255,
      "variance": 6.793955521174242,
      "rating_q975": 1362.2051700273546,
      "rating_q025": 1351.7665410277841,
      "Votes": 12123,
      "Rank (UB)": 20,
      "Model": "o4-mini-2025-04-16",
      "95% CI": {
        "rating_q025": 1351.7665410277841,
        "rating_q975": 1362.2051700273546
      }
    },
    {
      "Score": 1356.1118766038674,
      "variance": 9.944492654313672,
      "rating_q975": 1360.6907879919584,
      "rating_q025": 1350.2126308800985,
      "Votes": 9327,
      "Rank (UB)": 22,
      "Model": "qwen3-235b-a22b",
      "95% CI": {
        "rating_q025": 1350.2126308800985,
        "rating_q975": 1360.6907879919584
      }
    },
    {
      "Score": 1355.6074663090724,
      "variance": 12.427948491092735,
      "rating_q975": 1361.6789877445042,
      "rating_q025": 1348.4371784497068,
      "Votes": 5445,
      "Rank (UB)": 21,
      "Model": "grok-3-mini-beta",
      "95% CI": {
        "rating_q025": 1348.4371784497068,
        "rating_q975": 1361.6789877445042
      }
    },
    {
      "Score": 1355.0318573224808,
      "variance": 8.052206208472018,
      "rating_q975": 1360.9288384609556,
      "rating_q025": 1349.9394773321376,
      "Votes": 9446,
      "Rank (UB)": 22,
      "Model": "mistral-medium-2505",
      "95% CI": {
        "rating_q025": 1349.9394773321376,
        "rating_q975": 1360.9288384609556
      }
    },
    {
      "Score": 1352.393492806763,
      "variance": 2.7759291579425165,
      "rating_q975": 1355.8682990722464,
      "rating_q025": 1349.4218384064354,
      "Votes": 28612,
      "Rank (UB)": 24,
      "Model": "qwen2.5-max",
      "95% CI": {
        "rating_q025": 1349.4218384064354,
        "rating_q975": 1355.8682990722464
      }
    },
    {
      "Score": 1350.4447126453645,
      "variance": 4.874668712152919,
      "rating_q975": 1354.5880890934386,
      "rating_q025": 1346.5316111240943,
      "Votes": 18934,
      "Rank (UB)": 24,
      "Model": "gemma-3-27b-it",
      "95% CI": {
        "rating_q025": 1346.5316111240943,
        "rating_q975": 1354.5880890934386
      }
    },
    {
      "Score": 1343.2008733739856,
      "variance": 3.1986509332226563,
      "rating_q975": 1346.5280479070502,
      "rating_q025": 1339.5221071900116,
      "Votes": 33177,
      "Rank (UB)": 33,
      "Model": "o1-preview",
      "95% CI": {
        "rating_q025": 1339.5221071900116,
        "rating_q975": 1346.5280479070502
      }
    },
    {
      "Score": 1339.269265088273,
      "variance": 12.077331776340058,
      "rating_q975": 1345.5686986602998,
      "rating_q025": 1331.4104263876775,
      "Votes": 8087,
      "Rank (UB)": 34,
      "Model": "claude-sonnet-4-20250514",
      "95% CI": {
        "rating_q025": 1331.4104263876775,
        "rating_q975": 1345.5686986602998
      }
    },
    {
      "Score": 1332.8128087360665,
      "variance": 4.625190318870553,
      "rating_q975": 1336.5458785951769,
      "rating_q025": 1329.1540979876418,
      "Votes": 19404,
      "Rank (UB)": 35,
      "Model": "o3-mini-high",
      "95% CI": {
        "rating_q025": 1329.1540979876418,
        "rating_q975": 1336.5458785951769
      }
    },
    {
      "Score": 1330.8489952412524,
      "variance": 8.26045580733877,
      "rating_q975": 1337.121134618953,
      "rating_q025": 1326.0383703033324,
      "Votes": 11210,
      "Rank (UB)": 35,
      "Model": "gpt-4.1-mini-2025-04-14",
      "95% CI": {
        "rating_q025": 1326.0383703033324,
        "rating_q975": 1337.121134618953
      }
    },
    {
      "Score": 1329.1224170559665,
      "variance": 17.941434726666706,
      "rating_q975": 1337.3556493592466,
      "rating_q025": 1321.0801473622637,
      "Votes": 3976,
      "Rank (UB)": 35,
      "Model": "gemma-3-12b-it",
      "95% CI": {
        "rating_q025": 1321.0801473622637,
        "rating_q975": 1337.3556493592466
      }
    },
    {
      "Score": 1326.7503226009453,
      "variance": 2.897352020284216,
      "rating_q975": 1330.2108409663836,
      "rating_q025": 1323.8358634817182,
      "Votes": 22841,
      "Rank (UB)": 36,
      "Model": "deepseek-v3",
      "95% CI": {
        "rating_q025": 1323.8358634817182,
        "rating_q975": 1330.2108409663836
      }
    },
    {
      "Score": 1323.7880153813176,
      "variance": 6.286597390608491,
      "rating_q975": 1328.01992504295,
      "rating_q025": 1318.458319545813,
      "Votes": 15130,
      "Rank (UB)": 37,
      "Model": "qwq-32b",
      "95% CI": {
        "rating_q025": 1318.458319545813,
        "rating_q975": 1328.01992504295
      }
    },
    {
      "Score": 1321.1276927388783,
      "variance": 2.6241110197035535,
      "rating_q975": 1324.4668856340325,
      "rating_q025": 1318.8616173759265,
      "Votes": 26104,
      "Rank (UB)": 38,
      "Model": "gemini-2.0-flash-lite-preview-02-05",
      "95% CI": {
        "rating_q025": 1318.8616173759265,
        "rating_q975": 1324.4668856340325
      }
    },
    {
      "Score": 1319.2501920496675,
      "variance": 10.73293738194437,
      "rating_q975": 1325.7472723071955,
      "rating_q025": 1313.2401515239087,
      "Votes": 6028,
      "Rank (UB)": 38,
      "Model": "glm-4-plus-0111",
      "95% CI": {
        "rating_q025": 1313.2401515239087,
        "rating_q975": 1325.7472723071955
      }
    },
    {
      "Score": 1318.69979045453,
      "variance": 11.951511739261598,
      "rating_q975": 1324.9574665284317,
      "rating_q025": 1312.7726998296166,
      "Votes": 6055,
      "Rank (UB)": 38,
      "Model": "qwen-plus-0125",
      "95% CI": {
        "rating_q025": 1312.7726998296166,
        "rating_q975": 1324.9574665284317
      }
    },
    {
      "Score": 1317.5181244596204,
      "variance": 4.049650351746292,
      "rating_q975": 1321.5768857237292,
      "rating_q025": 1314.0085058320462,
      "Votes": 17572,
      "Rank (UB)": 40,
      "Model": "command-a-03-2025",
      "95% CI": {
        "rating_q025": 1314.0085058320462,
        "rating_q975": 1321.5768857237292
      }
    },
    {
      "Score": 1314.2039950964822,
      "variance": 2.7215882691227193,
      "rating_q975": 1316.9193438882858,
      "rating_q025": 1310.4265215706455,
      "Votes": 31005,
      "Rank (UB)": 43,
      "Model": "o3-mini",
      "95% CI": {
        "rating_q025": 1310.4265215706455,
        "rating_q975": 1316.9193438882858
      }
    },
    {
      "Score": 1312.9735783320477,
      "variance": 13.680903466472438,
      "rating_q975": 1319.416233339067,
      "rating_q025": 1305.8570990446508,
      "Votes": 5126,
      "Rank (UB)": 41,
      "Model": "step-2-16k-exp-202412",
      "95% CI": {
        "rating_q025": 1305.8570990446508,
        "rating_q975": 1319.416233339067
      }
    },
    {
      "Score": 1312.15622370992,
      "variance": 1.661392554053559,
      "rating_q975": 1314.8906163751824,
      "rating_q025": 1310.0108803494845,
      "Votes": 54951,
      "Rank (UB)": 43,
      "Model": "o1-mini",
      "95% CI": {
        "rating_q025": 1310.0108803494845,
        "rating_q975": 1314.8906163751824
      }
    },
    {
      "Score": 1310.7715863218677,
      "variance": 30.60846003874818,
      "rating_q975": 1321.4723756507626,
      "rating_q025": 1300.941580779332,
      "Votes": 2452,
      "Rank (UB)": 40,
      "Model": "hunyuan-turbos-20250226",
      "95% CI": {
        "rating_q025": 1300.941580779332,
        "rating_q975": 1321.4723756507626
      }
    },
    {
      "Score": 1310.4121884030194,
      "variance": 1.463965227859897,
      "rating_q975": 1312.6223617975515,
      "rating_q025": 1308.1528600261306,
      "Votes": 58645,
      "Rank (UB)": 46,
      "Model": "gemini-1.5-pro-002",
      "95% CI": {
        "rating_q025": 1308.1528600261306,
        "rating_q975": 1312.6223617975515
      }
    },
    {
      "Score": 1307.0640027594463,
      "variance": 5.094024903649778,
      "rating_q975": 1311.1415149280149,
      "rating_q025": 1302.6285090896386,
      "Votes": 18682,
      "Rank (UB)": 46,
      "Model": "claude-3-7-sonnet-20250219-thinking-32k",
      "95% CI": {
        "rating_q025": 1302.6285090896386,
        "rating_q975": 1311.1415149280149
      }
    },
    {
      "Score": 1304.7198187049457,
      "variance": 36.021381468432985,
      "rating_q975": 1315.7612674907032,
      "rating_q025": 1293.6488038352359,
      "Votes": 2371,
      "Rank (UB)": 43,
      "Model": "llama-3.3-nemotron-49b-super-v1",
      "95% CI": {
        "rating_q025": 1293.6488038352359,
        "rating_q975": 1315.7612674907032
      }
    },
    {
      "Score": 1304.478872140265,
      "variance": 30.086614576596777,
      "rating_q975": 1315.051909147248,
      "rating_q025": 1294.4292357911138,
      "Votes": 2510,
      "Rank (UB)": 43,
      "Model": "hunyuan-turbo-0110",
      "95% CI": {
        "rating_q025": 1294.4292357911138,
        "rating_q975": 1315.051909147248
      }
    },
    {
      "Score": 1300.457750305014,
      "variance": 4.107561045090479,
      "rating_q975": 1303.9348282389362,
      "rating_q025": 1297.0903578128946,
      "Votes": 24525,
      "Rank (UB)": 51,
      "Model": "claude-3-7-sonnet-20250219",
      "95% CI": {
        "rating_q025": 1297.0903578128946,
        "rating_q975": 1303.9348282389362
      }
    },
    {
      "Score": 1296.4347217423783,
      "variance": 22.331161570357988,
      "rating_q975": 1305.8588554277055,
      "rating_q025": 1286.0211183761694,
      "Votes": 3913,
      "Rank (UB)": 49,
      "Model": "gemma-3n-e4b-it",
      "95% CI": {
        "rating_q025": 1286.0211183761694,
        "rating_q975": 1305.8588554277055
      }
    },
    {
      "Score": 1296.0816521414547,
      "variance": 1.2481865470095639,
      "rating_q975": 1297.9569745721376,
      "rating_q025": 1293.7262869057722,
      "Votes": 67084,
      "Rank (UB)": 54,
      "Model": "grok-2-2024-08-13",
      "95% CI": {
        "rating_q025": 1293.7262869057722,
        "rating_q975": 1297.9569745721376
      }
    },
    {
      "Score": 1295.2955401804236,
      "variance": 2.5447715322006195,
      "rating_q975": 1298.3425052601456,
      "rating_q025": 1292.3148530332878,
      "Votes": 28968,
      "Rank (UB)": 54,
      "Model": "yi-lightning",
      "95% CI": {
        "rating_q025": 1292.3148530332878,
        "rating_q975": 1298.3425052601456
      }
    },
    {
      "Score": 1293.216726071008,
      "variance": 0.8006246340768794,
      "rating_q975": 1295.1037660798147,
      "rating_q025": 1291.7554296198905,
      "Votes": 117747,
      "Rank (UB)": 55,
      "Model": "gpt-4o-2024-05-13",
      "95% CI": {
        "rating_q025": 1291.7554296198905,
        "rating_q975": 1295.1037660798147
      }
    },
    {
      "Score": 1291.9577569719586,
      "variance": 1.4605956424261037,
      "rating_q975": 1294.7762618305326,
      "rating_q025": 1289.957940839425,
      "Votes": 72401,
      "Rank (UB)": 55,
      "Model": "claude-3-5-sonnet-20241022",
      "95% CI": {
        "rating_q025": 1289.957940839425,
        "rating_q975": 1294.7762618305326
      }
    },
    {
      "Score": 1290.426471101222,
      "variance": 6.787775919131335,
      "rating_q975": 1294.9609399391888,
      "rating_q025": 1284.6569697707446,
      "Votes": 10715,
      "Rank (UB)": 55,
      "Model": "qwen2.5-plus-1127",
      "95% CI": {
        "rating_q025": 1284.6569697707446,
        "rating_q975": 1294.9609399391888
      }
    },
    {
      "Score": 1287.3438781159873,
      "variance": 9.780841485010283,
      "rating_q975": 1292.220132983283,
      "rating_q025": 1281.452656943861,
      "Votes": 7243,
      "Rank (UB)": 59,
      "Model": "deepseek-v2.5-1210",
      "95% CI": {
        "rating_q025": 1281.452656943861,
        "rating_q975": 1292.220132983283
      }
    },
    {
      "Score": 1283.677067180175,
      "variance": 18.339137221176642,
      "rating_q975": 1290.4244048934406,
      "rating_q025": 1275.0198709942504,
      "Votes": 4321,
      "Rank (UB)": 60,
      "Model": "gemma-3-4b-it",
      "95% CI": {
        "rating_q025": 1275.0198709942504,
        "rating_q975": 1290.4244048934406
      }
    },
    {
      "Score": 1283.6618352816824,
      "variance": 2.3367205244985727,
      "rating_q975": 1286.8975468271867,
      "rating_q025": 1281.457578321653,
      "Votes": 26074,
      "Rank (UB)": 61,
      "Model": "athene-v2-chat",
      "95% CI": {
        "rating_q025": 1281.457578321653,
        "rating_q975": 1286.8975468271867
      }
    },
    {
      "Score": 1282.200952348011,
      "variance": 2.9138485588717544,
      "rating_q975": 1285.1049334671077,
      "rating_q025": 1278.806394177823,
      "Votes": 27788,
      "Rank (UB)": 62,
      "Model": "glm-4-plus",
      "95% CI": {
        "rating_q025": 1278.806394177823,
        "rating_q975": 1285.1049334671077
      }
    },
    {
      "Score": 1281.6872076395389,
      "variance": 6.49347764545274,
      "rating_q975": 1287.1930376599378,
      "rating_q025": 1276.6209707206879,
      "Votes": 13749,
      "Rank (UB)": 61,
      "Model": "llama-4-maverick-17b-128e-instruct",
      "95% CI": {
        "rating_q025": 1276.6209707206879,
        "rating_q975": 1287.1930376599378
      }
    },
    {
      "Score": 1280.1586078995542,
      "variance": 22.07993566532233,
      "rating_q975": 1287.7099301243354,
      "rating_q025": 1270.8790215137299,
      "Votes": 3856,
      "Rank (UB)": 61,
      "Model": "hunyuan-large-2025-02-10",
      "95% CI": {
        "rating_q025": 1270.8790215137299,
        "rating_q975": 1287.7099301243354
      }
    },
    {
      "Score": 1279.9370984958753,
      "variance": 1.2075112030499167,
      "rating_q975": 1282.0326305846331,
      "rating_q025": 1278.0018115487935,
      "Votes": 72536,
      "Rank (UB)": 63,
      "Model": "gpt-4o-mini-2024-07-18",
      "95% CI": {
        "rating_q025": 1278.0018115487935,
        "rating_q975": 1282.0326305846331
      }
    },
    {
      "Score": 1279.728869857209,
      "variance": 1.5657608155683564,
      "rating_q975": 1282.20463071147,
      "rating_q025": 1277.1310397350426,
      "Votes": 37021,
      "Rank (UB)": 63,
      "Model": "gemini-1.5-flash-002",
      "95% CI": {
        "rating_q025": 1277.1310397350426,
        "rating_q975": 1282.20463071147
      }
    },
    {
      "Score": 1279.404407285401,
      "variance": 14.187825980788702,
      "rating_q975": 1286.111870861423,
      "rating_q025": 1271.5709662049014,
      "Votes": 6302,
      "Rank (UB)": 61,
      "Model": "gpt-4.1-nano-2025-04-14",
      "95% CI": {
        "rating_q025": 1271.5709662049014,
        "rating_q975": 1286.111870861423
      }
    },
    {
      "Score": 1277.2381828052671,
      "variance": 1.6746190651803323,
      "rating_q975": 1279.9743843547105,
      "rating_q025": 1275.320815800768,
      "Votes": 43788,
      "Rank (UB)": 65,
      "Model": "llama-3.1-405b-instruct-bf16",
      "95% CI": {
        "rating_q025": 1275.320815800768,
        "rating_q975": 1279.9743843547105
      }
    },
    {
      "Score": 1276.8063449241374,
      "variance": 9.986837715475952,
      "rating_q975": 1284.466396356508,
      "rating_q025": 1272.1307765012916,
      "Votes": 7577,
      "Rank (UB)": 63,
      "Model": "llama-3.1-nemotron-70b-instruct",
      "95% CI": {
        "rating_q025": 1272.1307765012916,
        "rating_q975": 1284.466396356508
      }
    },
    {
      "Score": 1276.4672998906078,
      "variance": 1.2175171544494245,
      "rating_q975": 1278.8002057754272,
      "rating_q025": 1274.3422043382911,
      "Votes": 86159,
      "Rank (UB)": 66,
      "Model": "claude-3-5-sonnet-20240620",
      "95% CI": {
        "rating_q025": 1274.3422043382911,
        "rating_q975": 1278.8002057754272
      }
    },
    {
      "Score": 1275.7459700115335,
      "variance": 1.1777834861957064,
      "rating_q975": 1277.657569680053,
      "rating_q025": 1273.6009130028854,
      "Votes": 63038,
      "Rank (UB)": 67,
      "Model": "llama-3.1-405b-instruct-fp8",
      "95% CI": {
        "rating_q025": 1273.6009130028854,
        "rating_q975": 1277.657569680053
      }
    },
    {
      "Score": 1274.7851903395815,
      "variance": 2.00140808322138,
      "rating_q975": 1277.864086982041,
      "rating_q025": 1272.3445690081348,
      "Votes": 52144,
      "Rank (UB)": 67,
      "Model": "gemini-advanced-0514",
      "95% CI": {
        "rating_q025": 1272.3445690081348,
        "rating_q975": 1277.864086982041
      }
    },
    {
      "Score": 1274.4301262094366,
      "variance": 1.8839793944068595,
      "rating_q975": 1276.75379490562,
      "rating_q025": 1271.3887637187804,
      "Votes": 55442,
      "Rank (UB)": 68,
      "Model": "grok-2-mini-2024-08-13",
      "95% CI": {
        "rating_q025": 1271.3887637187804,
        "rating_q975": 1276.75379490562
      }
    },
    {
      "Score": 1273.4947275901397,
      "variance": 1.3665332306737585,
      "rating_q975": 1275.601173274231,
      "rating_q025": 1271.372671605956,
      "Votes": 47973,
      "Rank (UB)": 69,
      "Model": "gpt-4o-2024-08-06",
      "95% CI": {
        "rating_q025": 1271.372671605956,
        "rating_q975": 1275.601173274231
      }
    },
    {
      "Score": 1271.5872403584208,
      "variance": 4.843181789527392,
      "rating_q975": 1276.2963527264196,
      "rating_q025": 1267.5923558144195,
      "Votes": 17432,
      "Rank (UB)": 69,
      "Model": "qwen-max-0919",
      "95% CI": {
        "rating_q025": 1267.5923558144195,
        "rating_q975": 1276.2963527264196
      }
    },
    {
      "Score": 1269.046960564824,
      "variance": 17.11545673689001,
      "rating_q975": 1275.8243479979699,
      "rating_q025": 1260.3762351291518,
      "Votes": 4014,
      "Rank (UB)": 69,
      "Model": "hunyuan-standard-2025-02-10",
      "95% CI": {
        "rating_q025": 1260.3762351291518,
        "rating_q975": 1275.8243479979699
      }
    },
    {
      "Score": 1268.1179818529183,
      "variance": 1.1435665989597648,
      "rating_q975": 1270.2811362977704,
      "rating_q025": 1266.3198459295556,
      "Votes": 82435,
      "Rank (UB)": 80,
      "Model": "gemini-1.5-pro-001",
      "95% CI": {
        "rating_q025": 1266.3198459295556,
        "rating_q975": 1270.2811362977704
      }
    },
    {
      "Score": 1266.335918912353,
      "variance": 3.0295937629506735,
      "rating_q975": 1269.7423758578543,
      "rating_q025": 1263.1049257150585,
      "Votes": 26344,
      "Rank (UB)": 80,
      "Model": "deepseek-v2.5",
      "95% CI": {
        "rating_q025": 1263.1049257150585,
        "rating_q975": 1269.7423758578543
      }
    },
    {
      "Score": 1265.3330063531034,
      "variance": 2.25193770232843,
      "rating_q975": 1268.2778593047885,
      "rating_q025": 1262.3811708134288,
      "Votes": 41519,
      "Rank (UB)": 81,
      "Model": "qwen2.5-72b-instruct",
      "95% CI": {
        "rating_q025": 1262.3811708134288,
        "rating_q975": 1268.2778593047885
      }
    },
    {
      "Score": 1264.7344572599754,
      "variance": 1.9715723660004898,
      "rating_q975": 1267.536875245546,
      "rating_q025": 1262.254294386739,
      "Votes": 43904,
      "Rank (UB)": 82,
      "Model": "llama-3.3-70b-instruct",
      "95% CI": {
        "rating_q025": 1262.254294386739,
        "rating_q975": 1267.536875245546
      }
    },
    {
      "Score": 1264.682794295748,
      "variance": 1.085624667292776,
      "rating_q975": 1266.6961409770875,
      "rating_q025": 1262.8059822020284,
      "Votes": 102133,
      "Rank (UB)": 82,
      "Model": "gpt-4-turbo-2024-04-09",
      "95% CI": {
        "rating_q025": 1262.8059822020284,
        "rating_q975": 1266.6961409770875
      }
    },
    {
      "Score": 1259.8146995083107,
      "variance": 1.6475214151288406,
      "rating_q975": 1262.4174436012065,
      "rating_q025": 1257.276532705507,
      "Votes": 48217,
      "Rank (UB)": 86,
      "Model": "mistral-large-2407",
      "95% CI": {
        "rating_q025": 1257.276532705507,
        "rating_q975": 1262.4174436012065
      }
    },
    {
      "Score": 1258.5954429984547,
      "variance": 3.7010999480207665,
      "rating_q975": 1262.4184143041925,
      "rating_q025": 1254.8065820493473,
      "Votes": 20580,
      "Rank (UB)": 86,
      "Model": "athene-70b-0725",
      "95% CI": {
        "rating_q025": 1254.8065820493473,
        "rating_q975": 1262.4184143041925
      }
    },
    {
      "Score": 1258.2935738075132,
      "variance": 42.794151019304856,
      "rating_q975": 1270.0443251628694,
      "rating_q025": 1246.1829162795789,
      "Votes": 2484,
      "Rank (UB)": 80,
      "Model": "mistral-small-3.1-24b-instruct-2503",
      "95% CI": {
        "rating_q025": 1246.1829162795789,
        "rating_q975": 1270.0443251628694
      }
    },
    {
      "Score": 1258.1965921886258,
      "variance": 1.2346452743566014,
      "rating_q975": 1260.8500113606287,
      "rating_q025": 1256.0178516885708,
      "Votes": 103748,
      "Rank (UB)": 88,
      "Model": "gpt-4-1106-preview",
      "95% CI": {
        "rating_q025": 1256.0178516885708,
        "rating_q975": 1260.8500113606287
      }
    },
    {
      "Score": 1257.2280212125902,
      "variance": 2.7066892356602508,
      "rating_q975": 1260.5643047285814,
      "rating_q025": 1254.1156028031714,
      "Votes": 29633,
      "Rank (UB)": 88,
      "Model": "mistral-large-2411",
      "95% CI": {
        "rating_q025": 1254.1156028031714,
        "rating_q975": 1260.5643047285814
      }
    },
    {
      "Score": 1255.9708956154072,
      "variance": 1.43603214030458,
      "rating_q975": 1258.3431528728695,
      "rating_q025": 1253.4117627542387,
      "Votes": 58637,
      "Rank (UB)": 89,
      "Model": "llama-3.1-70b-instruct",
      "95% CI": {
        "rating_q025": 1253.4117627542387,
        "rating_q975": 1258.3431528728695
      }
    },
    {
      "Score": 1255.5222048208652,
      "variance": 0.6777894463137306,
      "rating_q975": 1256.9342734164402,
      "rating_q025": 1253.8996698685776,
      "Votes": 202641,
      "Rank (UB)": 90,
      "Model": "claude-3-opus-20240229",
      "95% CI": {
        "rating_q025": 1253.8996698685776,
        "rating_q975": 1256.9342734164402
      }
    },
    {
      "Score": 1253.2956337970973,
      "variance": 3.179769387852337,
      "rating_q975": 1256.1726229419069,
      "rating_q025": 1250.1415966756645,
      "Votes": 26371,
      "Rank (UB)": 90,
      "Model": "amazon-nova-pro-v1.0",
      "95% CI": {
        "rating_q025": 1250.1415966756645,
        "rating_q975": 1256.1726229419069
      }
    },
    {
      "Score": 1253.1223898766737,
      "variance": 1.379504901985519,
      "rating_q975": 1255.5540890296822,
      "rating_q025": 1251.009293344071,
      "Votes": 97079,
      "Rank (UB)": 91,
      "Model": "gpt-4-0125-preview",
      "95% CI": {
        "rating_q025": 1251.009293344071,
        "rating_q975": 1255.5540890296822
      }
    },
    {
      "Score": 1252.475057689725,
      "variance": 22.279234073425762,
      "rating_q975": 1260.2262273742024,
      "rating_q025": 1243.4739253853145,
      "Votes": 3010,
      "Rank (UB)": 89,
      "Model": "llama-3.1-tulu-3-70b",
      "95% CI": {
        "rating_q025": 1243.4739253853145,
        "rating_q975": 1260.2262273742024
      }
    },
    {
      "Score": 1246.2441408509424,
      "variance": 1.978727606285945,
      "rating_q975": 1248.630626237266,
      "rating_q025": 1243.6667920792763,
      "Votes": 43441,
      "Rank (UB)": 97,
      "Model": "claude-3-5-haiku-20241022",
      "95% CI": {
        "rating_q025": 1243.6667920792763,
        "rating_q975": 1248.630626237266
      }
    },
    {
      "Score": 1243.3536684416488,
      "variance": 11.215205722794446,
      "rating_q975": 1249.7264005967413,
      "rating_q025": 1237.639092938493,
      "Votes": 7948,
      "Rank (UB)": 97,
      "Model": "reka-core-20240904",
      "95% CI": {
        "rating_q025": 1237.639092938493,
        "rating_q975": 1249.7264005967413
      }
    },
    {
      "Score": 1235.115591994448,
      "variance": 1.5115117979904582,
      "rating_q975": 1237.6930008242846,
      "rating_q025": 1233.2911905900378,
      "Votes": 65661,
      "Rank (UB)": 101,
      "Model": "gemini-1.5-flash-001",
      "95% CI": {
        "rating_q025": 1233.2911905900378,
        "rating_q975": 1237.6930008242846
      }
    },
    {
      "Score": 1229.8879105790197,
      "variance": 8.463767332256976,
      "rating_q975": 1235.3274053777006,
      "rating_q025": 1225.3522427655553,
      "Votes": 9125,
      "Rank (UB)": 102,
      "Model": "jamba-1.5-large",
      "95% CI": {
        "rating_q025": 1225.3522427655553,
        "rating_q975": 1235.3274053777006
      }
    },
    {
      "Score": 1228.0827482474242,
      "variance": 0.6849936805415118,
      "rating_q975": 1229.4656114417924,
      "rating_q025": 1226.645201528169,
      "Votes": 79538,
      "Rank (UB)": 105,
      "Model": "gemma-2-27b-it",
      "95% CI": {
        "rating_q025": 1226.645201528169,
        "rating_q975": 1229.4656114417924
      }
    },
    {
      "Score": 1225.647682341208,
      "variance": 5.408118427322437,
      "rating_q975": 1229.597595404722,
      "rating_q025": 1220.2057558345937,
      "Votes": 15321,
      "Rank (UB)": 105,
      "Model": "mistral-small-24b-instruct-2501",
      "95% CI": {
        "rating_q025": 1220.2057558345937,
        "rating_q975": 1229.597595404722
      }
    },
    {
      "Score": 1225.50636604275,
      "variance": 13.516124137583946,
      "rating_q975": 1231.9042068277834,
      "rating_q025": 1217.5964102137016,
      "Votes": 5730,
      "Rank (UB)": 104,
      "Model": "qwen2.5-coder-32b-instruct",
      "95% CI": {
        "rating_q025": 1217.5964102137016,
        "rating_q975": 1231.9042068277834
      }
    },
    {
      "Score": 1225.1634586505925,
      "variance": 3.1576008607537087,
      "rating_q975": 1228.1604175678026,
      "rating_q025": 1221.5005214266407,
      "Votes": 20646,
      "Rank (UB)": 105,
      "Model": "amazon-nova-lite-v1.0",
      "95% CI": {
        "rating_q025": 1221.5005214266407,
        "rating_q975": 1228.1604175678026
      }
    },
    {
      "Score": 1224.3428467454287,
      "variance": 6.64657725118541,
      "rating_q975": 1229.3996998574253,
      "rating_q025": 1219.3487347120827,
      "Votes": 10548,
      "Rank (UB)": 105,
      "Model": "gemma-2-9b-it-simpo",
      "95% CI": {
        "rating_q025": 1219.3487347120827,
        "rating_q975": 1229.3996998574253
      }
    },
    {
      "Score": 1223.5485055726945,
      "variance": 6.991221911418421,
      "rating_q975": 1228.8283891469127,
      "rating_q025": 1219.1103413671053,
      "Votes": 10535,
      "Rank (UB)": 105,
      "Model": "command-r-plus-08-2024",
      "95% CI": {
        "rating_q025": 1219.1103413671053,
        "rating_q975": 1228.8283891469127
      }
    },
    {
      "Score": 1220.8524577592698,
      "variance": 2.305724226716148,
      "rating_q975": 1223.4467072617108,
      "rating_q025": 1217.879475408446,
      "Votes": 37697,
      "Rank (UB)": 108,
      "Model": "gemini-1.5-flash-8b-001",
      "95% CI": {
        "rating_q025": 1217.879475408446,
        "rating_q975": 1223.4467072617108
      }
    },
    {
      "Score": 1220.0672919436051,
      "variance": 19.92901851436122,
      "rating_q975": 1229.1485833831337,
      "rating_q025": 1212.9018893264786,
      "Votes": 3889,
      "Rank (UB)": 105,
      "Model": "llama-3.1-nemotron-51b-instruct",
      "95% CI": {
        "rating_q025": 1212.9018893264786,
        "rating_q975": 1229.1485833831337
      }
    },
    {
      "Score": 1217.4842162159332,
      "variance": 2.742523434016064,
      "rating_q975": 1220.400859873173,
      "rating_q025": 1214.1518200380985,
      "Votes": 28768,
      "Rank (UB)": 109,
      "Model": "c4ai-aya-expanse-32b",
      "95% CI": {
        "rating_q025": 1214.1518200380985,
        "rating_q975": 1220.400859873173
      }
    },
    {
      "Score": 1217.3403430849767,
      "variance": 3.9813865093816947,
      "rating_q975": 1220.9146926272747,
      "rating_q025": 1213.3172273875614,
      "Votes": 20608,
      "Rank (UB)": 109,
      "Model": "nemotron-4-340b-instruct",
      "95% CI": {
        "rating_q025": 1213.3172273875614,
        "rating_q975": 1220.9146926272747
      }
    },
    {
      "Score": 1214.8713363670076,
      "variance": 8.418438587808305,
      "rating_q975": 1220.130821817415,
      "rating_q025": 1209.248581653806,
      "Votes": 10221,
      "Rank (UB)": 110,
      "Model": "glm-4-0520",
      "95% CI": {
        "rating_q025": 1209.248581653806,
        "rating_q975": 1220.130821817415
      }
    },
    {
      "Score": 1214.7707884125944,
      "variance": 0.8410289385444675,
      "rating_q975": 1216.5410341077868,
      "rating_q025": 1213.030786956027,
      "Votes": 163629,
      "Rank (UB)": 116,
      "Model": "llama-3-70b-instruct",
      "95% CI": {
        "rating_q025": 1213.030786956027,
        "rating_q975": 1216.5410341077868
      }
    },
    {
      "Score": 1213.873739193079,
      "variance": 22.275435991344022,
      "rating_q975": 1224.2123100538886,
      "rating_q025": 1204.4260030017472,
      "Votes": 3460,
      "Rank (UB)": 108,
      "Model": "olmo-2-0325-32b-instruct",
      "95% CI": {
        "rating_q025": 1204.4260030017472,
        "rating_q975": 1224.2123100538886
      }
    },
    {
      "Score": 1213.7753122100949,
      "variance": 9.790856273436965,
      "rating_q975": 1219.3487434695348,
      "rating_q025": 1208.2261644177481,
      "Votes": 8132,
      "Rank (UB)": 110,
      "Model": "reka-flash-20240904",
      "95% CI": {
        "rating_q025": 1208.2261644177481,
        "rating_q975": 1219.3487434695348
      }
    },
    {
      "Score": 1213.6157087129782,
      "variance": 2.398360917151886,
      "rating_q975": 1216.4723431540187,
      "rating_q025": 1210.3116672976478,
      "Votes": 25213,
      "Rank (UB)": 116,
      "Model": "phi-4",
      "95% CI": {
        "rating_q025": 1210.3116672976478,
        "rating_q975": 1216.4723431540187
      }
    },
    {
      "Score": 1209.2388726671672,
      "variance": 1.051838891807345,
      "rating_q975": 1210.9089231856365,
      "rating_q025": 1207.381572680006,
      "Votes": 113067,
      "Rank (UB)": 120,
      "Model": "claude-3-sonnet-20240229",
      "95% CI": {
        "rating_q025": 1207.381572680006,
        "rating_q975": 1210.9089231856365
      }
    },
    {
      "Score": 1206.2609467400248,
      "variance": 3.827033165947535,
      "rating_q975": 1209.7658406227538,
      "rating_q025": 1202.619823003003,
      "Votes": 20654,
      "Rank (UB)": 123,
      "Model": "amazon-nova-micro-v1.0",
      "95% CI": {
        "rating_q025": 1202.619823003003,
        "rating_q975": 1209.7658406227538
      }
    },
    {
      "Score": 1200.2788711063263,
      "variance": 1.5416722937229923,
      "rating_q975": 1202.476820496355,
      "rating_q025": 1198.1841095189952,
      "Votes": 57197,
      "Rank (UB)": 130,
      "Model": "gemma-2-9b-it",
      "95% CI": {
        "rating_q025": 1198.1841095189952,
        "rating_q975": 1202.476820496355
      }
    },
    {
      "Score": 1198.1619645649391,
      "variance": 1.4858901578986938,
      "rating_q975": 1200.2338431897724,
      "rating_q025": 1195.9195511227263,
      "Votes": 80846,
      "Rank (UB)": 130,
      "Model": "command-r-plus",
      "95% CI": {
        "rating_q025": 1195.9195511227263,
        "rating_q975": 1200.2338431897724
      }
    },
    {
      "Score": 1197.1648232745265,
      "variance": 34.124862127245514,
      "rating_q975": 1207.6184978022093,
      "rating_q025": 1186.8036183595377,
      "Votes": 2901,
      "Rank (UB)": 125,
      "Model": "hunyuan-standard-256k",
      "95% CI": {
        "rating_q025": 1186.8036183595377,
        "rating_q975": 1207.6184978022093
      }
    },
    {
      "Score": 1195.3114261914207,
      "variance": 2.437381785057979,
      "rating_q975": 1198.2216249912042,
      "rating_q025": 1192.4165533848552,
      "Votes": 38872,
      "Rank (UB)": 130,
      "Model": "qwen2-72b-instruct",
      "95% CI": {
        "rating_q025": 1192.4165533848552,
        "rating_q975": 1198.2216249912042
      }
    },
    {
      "Score": 1194.3822122054048,
      "variance": 2.099659226436607,
      "rating_q975": 1197.374837480052,
      "rating_q025": 1192.0831795249742,
      "Votes": 55962,
      "Rank (UB)": 131,
      "Model": "gpt-4-0314",
      "95% CI": {
        "rating_q025": 1192.0831795249742,
        "rating_q975": 1197.374837480052
      }
    },
    {
      "Score": 1193.5618572064598,
      "variance": 28.24968397970162,
      "rating_q975": 1205.468843463471,
      "rating_q025": 1184.721787295317,
      "Votes": 3074,
      "Rank (UB)": 126,
      "Model": "llama-3.1-tulu-3-8b",
      "95% CI": {
        "rating_q025": 1184.721787295317,
        "rating_q975": 1205.468843463471
      }
    },
    {
      "Score": 1190.5119772588537,
      "variance": 17.495271242926783,
      "rating_q975": 1198.1962040038843,
      "rating_q025": 1180.4291274813997,
      "Votes": 5111,
      "Rank (UB)": 130,
      "Model": "ministral-8b-2410",
      "95% CI": {
        "rating_q025": 1180.4291274813997,
        "rating_q975": 1198.1962040038843
      }
    },
    {
      "Score": 1188.2743302413483,
      "variance": 7.369671653891021,
      "rating_q975": 1193.5370715400948,
      "rating_q025": 1183.0623200806053,
      "Votes": 10391,
      "Rank (UB)": 132,
      "Model": "c4ai-aya-expanse-8b",
      "95% CI": {
        "rating_q025": 1183.0623200806053,
        "rating_q975": 1193.5370715400948
      }
    },
    {
      "Score": 1187.904276886623,
      "variance": 5.5625613577364454,
      "rating_q975": 1193.5601926015768,
      "rating_q025": 1184.237359497241,
      "Votes": 10851,
      "Rank (UB)": 132,
      "Model": "command-r-08-2024",
      "95% CI": {
        "rating_q025": 1184.237359497241,
        "rating_q975": 1193.5601926015768
      }
    },
    {
      "Score": 1187.5177485565794,
      "variance": 0.701991772776273,
      "rating_q975": 1189.1771184934184,
      "rating_q025": 1185.6590351254529,
      "Votes": 122309,
      "Rank (UB)": 134,
      "Model": "claude-3-haiku-20240307",
      "95% CI": {
        "rating_q025": 1185.6590351254529,
        "rating_q975": 1189.1771184934184
      }
    },
    {
      "Score": 1186.6163442224292,
      "variance": 7.119452039047732,
      "rating_q975": 1191.4605938061504,
      "rating_q025": 1181.589709363175,
      "Votes": 15753,
      "Rank (UB)": 134,
      "Model": "deepseek-coder-v2",
      "95% CI": {
        "rating_q025": 1181.589709363175,
        "rating_q975": 1191.4605938061504
      }
    },
    {
      "Score": 1184.1772286654623,
      "variance": 6.410893275013854,
      "rating_q975": 1189.3221833203552,
      "rating_q025": 1179.3405563463887,
      "Votes": 9274,
      "Rank (UB)": 134,
      "Model": "jamba-1.5-mini",
      "95% CI": {
        "rating_q025": 1179.3405563463887,
        "rating_q975": 1189.3221833203552
      }
    },
    {
      "Score": 1183.9130756255195,
      "variance": 1.3897703859146322,
      "rating_q975": 1186.2045670736723,
      "rating_q025": 1181.9506575826972,
      "Votes": 52578,
      "Rank (UB)": 136,
      "Model": "llama-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1181.9506575826972,
        "rating_q975": 1186.2045670736723
      }
    },
    {
      "Score": 1171.2941939501056,
      "variance": 1.3521230445750907,
      "rating_q975": 1173.472315438002,
      "rating_q025": 1169.440001887931,
      "Votes": 91614,
      "Rank (UB)": 145,
      "Model": "gpt-4-0613",
      "95% CI": {
        "rating_q025": 1169.440001887931,
        "rating_q975": 1173.472315438002
      }
    },
    {
      "Score": 1169.502253600633,
      "variance": 3.7077279249384323,
      "rating_q975": 1173.017150735314,
      "rating_q025": 1166.0750246177413,
      "Votes": 27430,
      "Rank (UB)": 145,
      "Model": "qwen1.5-110b-chat",
      "95% CI": {
        "rating_q025": 1166.0750246177413,
        "rating_q975": 1173.017150735314
      }
    },
    {
      "Score": 1165.61824244328,
      "variance": 2.998261392121318,
      "rating_q975": 1168.491686704208,
      "rating_q025": 1162.7004093159626,
      "Votes": 25135,
      "Rank (UB)": 147,
      "Model": "yi-1.5-34b-chat",
      "95% CI": {
        "rating_q025": 1162.7004093159626,
        "rating_q975": 1168.491686704208
      }
    },
    {
      "Score": 1165.5528833881249,
      "variance": 1.4590166407920133,
      "rating_q975": 1167.9408174664468,
      "rating_q025": 1163.081352682248,
      "Votes": 64926,
      "Rank (UB)": 147,
      "Model": "mistral-large-2402",
      "95% CI": {
        "rating_q025": 1163.081352682248,
        "rating_q975": 1167.9408174664468
      }
    },
    {
      "Score": 1164.0386915924187,
      "variance": 4.705914604365029,
      "rating_q975": 1167.496236256087,
      "rating_q025": 1159.9168049401317,
      "Votes": 16027,
      "Rank (UB)": 147,
      "Model": "reka-flash-21b-20240226-online",
      "95% CI": {
        "rating_q025": 1159.9168049401317,
        "rating_q975": 1167.496236256087
      }
    },
    {
      "Score": 1161.0888683204062,
      "variance": 27.19485079651468,
      "rating_q975": 1172.5619217492633,
      "rating_q025": 1152.76175816191,
      "Votes": 3410,
      "Rank (UB)": 145,
      "Model": "qwq-32b-preview",
      "95% CI": {
        "rating_q025": 1152.76175816191,
        "rating_q975": 1172.5619217492633
      }
    },
    {
      "Score": 1160.2105769082366,
      "variance": 0.9572620765990004,
      "rating_q975": 1162.1277384950702,
      "rating_q025": 1158.6825384461351,
      "Votes": 109056,
      "Rank (UB)": 150,
      "Model": "llama-3-8b-instruct",
      "95% CI": {
        "rating_q025": 1158.6825384461351,
        "rating_q975": 1162.1277384950702
      }
    },
    {
      "Score": 1157.2535427889202,
      "variance": 8.05616424094389,
      "rating_q975": 1162.5707409256431,
      "rating_q025": 1152.156006902488,
      "Votes": 10599,
      "Rank (UB)": 150,
      "Model": "internlm2_5-20b-chat",
      "95% CI": {
        "rating_q025": 1152.156006902488,
        "rating_q975": 1162.5707409256431
      }
    },
    {
      "Score": 1156.8833369450158,
      "variance": 2.056833065051178,
      "rating_q975": 1159.6811382527978,
      "rating_q025": 1154.3007396955898,
      "Votes": 56398,
      "Rank (UB)": 151,
      "Model": "command-r",
      "95% CI": {
        "rating_q025": 1154.3007396955898,
        "rating_q975": 1159.6811382527978
      }
    },
    {
      "Score": 1156.0786851750581,
      "variance": 2.3455845186590714,
      "rating_q975": 1159.0952550640277,
      "rating_q025": 1152.9537070903507,
      "Votes": 35556,
      "Rank (UB)": 151,
      "Model": "mistral-medium",
      "95% CI": {
        "rating_q025": 1152.9537070903507,
        "rating_q975": 1159.0952550640277
      }
    },
    {
      "Score": 1155.805239529028,
      "variance": 1.712878287716022,
      "rating_q975": 1158.8123046636301,
      "rating_q025": 1153.6897885209705,
      "Votes": 53751,
      "Rank (UB)": 151,
      "Model": "mixtral-8x22b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1153.6897885209705,
        "rating_q975": 1158.8123046636301
      }
    },
    {
      "Score": 1155.6465699015812,
      "variance": 3.123812654772985,
      "rating_q975": 1158.8083143844478,
      "rating_q025": 1151.869312695694,
      "Votes": 25803,
      "Rank (UB)": 151,
      "Model": "reka-flash-21b-20240226",
      "95% CI": {
        "rating_q025": 1151.869312695694,
        "rating_q975": 1158.8083143844478
      }
    },
    {
      "Score": 1155.6382799824746,
      "variance": 2.051165115426754,
      "rating_q975": 1158.5779366890397,
      "rating_q025": 1152.8903488434187,
      "Votes": 40658,
      "Rank (UB)": 152,
      "Model": "qwen1.5-72b-chat",
      "95% CI": {
        "rating_q025": 1152.8903488434187,
        "rating_q975": 1158.5779366890397
      }
    },
    {
      "Score": 1152.0693545975298,
      "variance": 1.8059477329537545,
      "rating_q975": 1154.2659868908522,
      "rating_q025": 1149.3526537912858,
      "Votes": 48892,
      "Rank (UB)": 153,
      "Model": "gemma-2-2b-it",
      "95% CI": {
        "rating_q025": 1149.3526537912858,
        "rating_q975": 1154.2659868908522
      }
    },
    {
      "Score": 1151.0279720698236,
      "variance": 23.93514685600179,
      "rating_q975": 1158.8549622825556,
      "rating_q025": 1141.1863286778116,
      "Votes": 3289,
      "Rank (UB)": 151,
      "Model": "granite-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1141.1863286778116,
        "rating_q975": 1158.8549622825556
      }
    },
    {
      "Score": 1139.5548844338564,
      "variance": 4.730448813469912,
      "rating_q975": 1144.108035489205,
      "rating_q025": 1136.1467215623186,
      "Votes": 18800,
      "Rank (UB)": 161,
      "Model": "gemini-pro-dev-api",
      "95% CI": {
        "rating_q025": 1136.1467215623186,
        "rating_q975": 1144.108035489205
      }
    },
    {
      "Score": 1135.3574604120251,
      "variance": 14.031426804560477,
      "rating_q975": 1142.2123057660456,
      "rating_q025": 1127.1984828478217,
      "Votes": 4854,
      "Rank (UB)": 161,
      "Model": "zephyr-orpo-141b-A35b-v0.1",
      "95% CI": {
        "rating_q025": 1127.1984828478217,
        "rating_q975": 1142.2123057660456
      }
    },
    {
      "Score": 1133.5700084929072,
      "variance": 3.287560706658284,
      "rating_q975": 1137.4048866052563,
      "rating_q025": 1130.7688792551453,
      "Votes": 22765,
      "Rank (UB)": 162,
      "Model": "qwen1.5-32b-chat",
      "95% CI": {
        "rating_q025": 1130.7688792551453,
        "rating_q975": 1137.4048866052563
      }
    },
    {
      "Score": 1131.1869543155985,
      "variance": 4.100685980488863,
      "rating_q975": 1135.4322351196492,
      "rating_q025": 1127.5009403392241,
      "Votes": 26105,
      "Rank (UB)": 163,
      "Model": "phi-3-medium-4k-instruct",
      "95% CI": {
        "rating_q025": 1127.5009403392241,
        "rating_q975": 1135.4322351196492
      }
    },
    {
      "Score": 1127.6024160586583,
      "variance": 28.79261239456306,
      "rating_q975": 1137.976066930513,
      "rating_q025": 1117.8172128335805,
      "Votes": 3380,
      "Rank (UB)": 162,
      "Model": "granite-3.1-2b-instruct",
      "95% CI": {
        "rating_q025": 1117.8172128335805,
        "rating_q975": 1137.976066930513
      }
    },
    {
      "Score": 1127.149951150128,
      "variance": 4.850788053860942,
      "rating_q975": 1131.6236319257036,
      "rating_q025": 1123.3674608142153,
      "Votes": 16676,
      "Rank (UB)": 164,
      "Model": "starling-lm-7b-beta",
      "95% CI": {
        "rating_q025": 1123.3674608142153,
        "rating_q975": 1131.6236319257036
      }
    },
    {
      "Score": 1122.2576692486546,
      "variance": 1.358326502984271,
      "rating_q975": 1124.6953882843893,
      "rating_q025": 1119.9865260048584,
      "Votes": 76126,
      "Rank (UB)": 168,
      "Model": "mixtral-8x7b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1119.9865260048584,
        "rating_q975": 1124.6953882843893
      }
    },
    {
      "Score": 1119.4381837326605,
      "variance": 5.136746950068182,
      "rating_q975": 1123.4229583400786,
      "rating_q025": 1115.225162425755,
      "Votes": 15917,
      "Rank (UB)": 168,
      "Model": "yi-34b-chat",
      "95% CI": {
        "rating_q025": 1115.225162425755,
        "rating_q975": 1123.4229583400786
      }
    },
    {
      "Score": 1118.8852268781936,
      "variance": 14.939346392042355,
      "rating_q975": 1126.1362193585867,
      "rating_q025": 1111.231808213152,
      "Votes": 6557,
      "Rank (UB)": 168,
      "Model": "gemini-pro",
      "95% CI": {
        "rating_q025": 1111.231808213152,
        "rating_q975": 1126.1362193585867
      }
    },
    {
      "Score": 1117.1801906274995,
      "variance": 4.590294616369208,
      "rating_q975": 1121.047586779149,
      "rating_q025": 1112.8793578412083,
      "Votes": 18687,
      "Rank (UB)": 171,
      "Model": "qwen1.5-14b-chat",
      "95% CI": {
        "rating_q025": 1112.8793578412083,
        "rating_q975": 1121.047586779149
      }
    },
    {
      "Score": 1114.6632452222166,
      "variance": 12.442006684458548,
      "rating_q975": 1120.1799358166954,
      "rating_q025": 1108.546383521419,
      "Votes": 8383,
      "Rank (UB)": 171,
      "Model": "wizardlm-70b",
      "95% CI": {
        "rating_q025": 1108.546383521419,
        "rating_q975": 1120.1799358166954
      }
    },
    {
      "Score": 1114.0979425808518,
      "variance": 1.564960117381674,
      "rating_q975": 1116.8519364582457,
      "rating_q025": 1111.9428536068642,
      "Votes": 68867,
      "Rank (UB)": 173,
      "Model": "gpt-3.5-turbo-0125",
      "95% CI": {
        "rating_q025": 1111.9428536068642,
        "rating_q975": 1116.8519364582457
      }
    },
    {
      "Score": 1111.3668319438643,
      "variance": 2.2419373593977014,
      "rating_q975": 1113.912289471837,
      "rating_q025": 1108.6337449574396,
      "Votes": 33743,
      "Rank (UB)": 175,
      "Model": "dbrx-instruct-preview",
      "95% CI": {
        "rating_q025": 1108.6337449574396,
        "rating_q975": 1113.912289471837
      }
    },
    {
      "Score": 1111.0221478726896,
      "variance": 8.432783990328923,
      "rating_q975": 1116.5194642784159,
      "rating_q025": 1105.8672898895213,
      "Votes": 8390,
      "Rank (UB)": 173,
      "Model": "llama-3.2-3b-instruct",
      "95% CI": {
        "rating_q025": 1105.8672898895213,
        "rating_q975": 1116.5194642784159
      }
    },
    {
      "Score": 1110.2216007690145,
      "variance": 3.7229445086619535,
      "rating_q975": 1113.6383317401453,
      "rating_q025": 1106.7483581633096,
      "Votes": 18476,
      "Rank (UB)": 175,
      "Model": "phi-3-small-8k-instruct",
      "95% CI": {
        "rating_q025": 1106.7483581633096,
        "rating_q975": 1113.6383317401453
      }
    },
    {
      "Score": 1107.2569339958109,
      "variance": 13.345619009020297,
      "rating_q975": 1115.3504101379742,
      "rating_q025": 1101.0205875311517,
      "Votes": 6658,
      "Rank (UB)": 173,
      "Model": "tulu-2-dpo-70b",
      "95% CI": {
        "rating_q025": 1101.0205875311517,
        "rating_q975": 1115.3504101379742
      }
    },
    {
      "Score": 1101.3847718185718,
      "variance": 9.657173515459803,
      "rating_q975": 1107.555138224259,
      "rating_q025": 1094.8785666639815,
      "Votes": 7002,
      "Rank (UB)": 180,
      "Model": "granite-3.0-8b-instruct",
      "95% CI": {
        "rating_q025": 1094.8785666639815,
        "rating_q975": 1107.555138224259
      }
    },
    {
      "Score": 1101.158024650916,
      "variance": 2.415324965787466,
      "rating_q975": 1103.7581980940527,
      "rating_q025": 1098.2015584402552,
      "Votes": 39595,
      "Rank (UB)": 183,
      "Model": "llama-2-70b-chat",
      "95% CI": {
        "rating_q025": 1098.2015584402552,
        "rating_q975": 1103.7581980940527
      }
    },
    {
      "Score": 1099.6350887959384,
      "variance": 7.1724115512574125,
      "rating_q975": 1105.0196039042346,
      "rating_q025": 1094.2765778820676,
      "Votes": 12990,
      "Rank (UB)": 183,
      "Model": "openchat-3.5-0106",
      "95% CI": {
        "rating_q025": 1094.2765778820676,
        "rating_q975": 1105.0196039042346
      }
    },
    {
      "Score": 1098.9135444371473,
      "variance": 3.7586049726051787,
      "rating_q975": 1101.962142255778,
      "rating_q025": 1095.2110173193048,
      "Votes": 22936,
      "Rank (UB)": 183,
      "Model": "vicuna-33b",
      "95% CI": {
        "rating_q025": 1095.2110173193048,
        "rating_q975": 1101.962142255778
      }
    },
    {
      "Score": 1098.1785275324437,
      "variance": 2.6341653864178487,
      "rating_q975": 1101.102435802764,
      "rating_q025": 1095.6307189353324,
      "Votes": 34173,
      "Rank (UB)": 183,
      "Model": "snowflake-arctic-instruct",
      "95% CI": {
        "rating_q025": 1095.6307189353324,
        "rating_q975": 1101.102435802764
      }
    },
    {
      "Score": 1096.6236102926882,
      "variance": 7.123784321543011,
      "rating_q975": 1102.4836893714964,
      "rating_q025": 1092.5874502483418,
      "Votes": 10415,
      "Rank (UB)": 183,
      "Model": "starling-lm-7b-alpha",
      "95% CI": {
        "rating_q025": 1092.5874502483418,
        "rating_q975": 1102.4836893714964
      }
    },
    {
      "Score": 1092.2890402011938,
      "variance": 17.628324262400252,
      "rating_q975": 1100.1758369014085,
      "rating_q025": 1083.19297218266,
      "Votes": 3836,
      "Rank (UB)": 184,
      "Model": "nous-hermes-2-mixtral-8x7b-dpo",
      "95% CI": {
        "rating_q025": 1083.19297218266,
        "rating_q975": 1100.1758369014085
      }
    },
    {
      "Score": 1091.9272605682436,
      "variance": 3.533966620642914,
      "rating_q975": 1095.6805104481575,
      "rating_q025": 1088.7039196780456,
      "Votes": 25070,
      "Rank (UB)": 185,
      "Model": "gemma-1.1-7b-it",
      "95% CI": {
        "rating_q025": 1088.7039196780456,
        "rating_q975": 1095.6805104481575
      }
    },
    {
      "Score": 1088.8761003109344,
      "variance": 25.159106914486905,
      "rating_q975": 1098.3691790086507,
      "rating_q025": 1079.908253723734,
      "Votes": 3636,
      "Rank (UB)": 184,
      "Model": "llama2-70b-steerlm-chat",
      "95% CI": {
        "rating_q025": 1079.908253723734,
        "rating_q975": 1098.3691790086507
      }
    },
    {
      "Score": 1085.1618274172038,
      "variance": 19.95300556935858,
      "rating_q975": 1093.0408577314984,
      "rating_q025": 1076.4297558810742,
      "Votes": 4988,
      "Rank (UB)": 189,
      "Model": "deepseek-llm-67b-chat",
      "95% CI": {
        "rating_q025": 1076.4297558810742,
        "rating_q975": 1093.0408577314984
      }
    },
    {
      "Score": 1084.70010977682,
      "variance": 9.127334923497628,
      "rating_q975": 1090.0470985646818,
      "rating_q025": 1079.2688677651029,
      "Votes": 8106,
      "Rank (UB)": 190,
      "Model": "openchat-3.5",
      "95% CI": {
        "rating_q025": 1079.2688677651029,
        "rating_q975": 1090.0470985646818
      }
    },
    {
      "Score": 1082.4642291530035,
      "variance": 19.343332919685167,
      "rating_q975": 1091.256724583629,
      "rating_q025": 1073.942728257255,
      "Votes": 5088,
      "Rank (UB)": 190,
      "Model": "openhermes-2.5-mistral-7b",
      "95% CI": {
        "rating_q025": 1073.942728257255,
        "rating_q975": 1091.256724583629
      }
    },
    {
      "Score": 1081.949158379321,
      "variance": 14.562576923932772,
      "rating_q975": 1088.3251600074507,
      "rating_q025": 1075.2281856424127,
      "Votes": 7191,
      "Rank (UB)": 191,
      "Model": "granite-3.0-2b-instruct",
      "95% CI": {
        "rating_q025": 1075.2281856424127,
        "rating_q975": 1088.3251600074507
      }
    },
    {
      "Score": 1080.519575061713,
      "variance": 3.464998887616581,
      "rating_q975": 1084.2851644312439,
      "rating_q025": 1077.081710648989,
      "Votes": 20067,
      "Rank (UB)": 191,
      "Model": "mistral-7b-instruct-v0.2",
      "95% CI": {
        "rating_q025": 1077.081710648989,
        "rating_q975": 1084.2851644312439
      }
    },
    {
      "Score": 1079.1013432645989,
      "variance": 6.669498415999727,
      "rating_q975": 1084.6873468625574,
      "rating_q025": 1074.8337464105682,
      "Votes": 12808,
      "Rank (UB)": 191,
      "Model": "phi-3-mini-4k-instruct-june-2024",
      "95% CI": {
        "rating_q025": 1074.8337464105682,
        "rating_q975": 1084.6873468625574
      }
    },
    {
      "Score": 1077.958695359368,
      "variance": 18.634017029259297,
      "rating_q975": 1087.6589928888975,
      "rating_q025": 1071.0366506066584,
      "Votes": 4872,
      "Rank (UB)": 191,
      "Model": "qwen1.5-7b-chat",
      "95% CI": {
        "rating_q025": 1071.0366506066584,
        "rating_q975": 1087.6589928888975
      }
    },
    {
      "Score": 1075.8558599907292,
      "variance": 5.419049777425998,
      "rating_q975": 1079.9338867864315,
      "rating_q025": 1070.4619809206574,
      "Votes": 17036,
      "Rank (UB)": 192,
      "Model": "gpt-3.5-turbo-1106",
      "95% CI": {
        "rating_q025": 1070.4619809206574,
        "rating_q975": 1079.9338867864315
      }
    },
    {
      "Score": 1074.6702217246584,
      "variance": 4.380950736743142,
      "rating_q975": 1077.915935018764,
      "rating_q025": 1070.843738637054,
      "Votes": 21097,
      "Rank (UB)": 195,
      "Model": "phi-3-mini-4k-instruct",
      "95% CI": {
        "rating_q025": 1070.843738637054,
        "rating_q975": 1077.915935018764
      }
    },
    {
      "Score": 1071.407718049478,
      "variance": 4.6861200781036825,
      "rating_q975": 1075.412123175614,
      "rating_q025": 1067.680602747829,
      "Votes": 19722,
      "Rank (UB)": 197,
      "Model": "llama-2-13b-chat",
      "95% CI": {
        "rating_q025": 1067.680602747829,
        "rating_q975": 1075.412123175614
      }
    },
    {
      "Score": 1070.6235503808653,
      "variance": 35.20011718244084,
      "rating_q975": 1083.3017007386532,
      "rating_q025": 1060.06730517655,
      "Votes": 1714,
      "Rank (UB)": 191,
      "Model": "dolphin-2.2.1-mistral-7b",
      "95% CI": {
        "rating_q025": 1060.06730517655,
        "rating_q975": 1083.3017007386532
      }
    },
    {
      "Score": 1070.2838585340203,
      "variance": 21.63001161236292,
      "rating_q975": 1078.5308941748692,
      "rating_q025": 1060.332100030282,
      "Votes": 4286,
      "Rank (UB)": 195,
      "Model": "solar-10.7b-instruct-v1.0",
      "95% CI": {
        "rating_q025": 1060.332100030282,
        "rating_q975": 1078.5308941748692
      }
    },
    {
      "Score": 1066.9166879690583,
      "variance": 14.064613732639563,
      "rating_q975": 1074.6711437058414,
      "rating_q025": 1060.7983892179554,
      "Votes": 7176,
      "Rank (UB)": 199,
      "Model": "wizardlm-13b",
      "95% CI": {
        "rating_q025": 1060.7983892179554,
        "rating_q975": 1074.6711437058414
      }
    },
    {
      "Score": 1061.8983966257533,
      "variance": 11.058408041400535,
      "rating_q975": 1068.427965286077,
      "rating_q025": 1055.7079766554182,
      "Votes": 8523,
      "Rank (UB)": 203,
      "Model": "llama-3.2-1b-instruct",
      "95% CI": {
        "rating_q025": 1055.7079766554182,
        "rating_q975": 1068.427965286077
      }
    },
    {
      "Score": 1061.450411437043,
      "variance": 5.448852639370521,
      "rating_q975": 1065.2585649591601,
      "rating_q025": 1057.0555908557192,
      "Votes": 11321,
      "Rank (UB)": 204,
      "Model": "zephyr-7b-beta",
      "95% CI": {
        "rating_q025": 1057.0555908557192,
        "rating_q975": 1065.2585649591601
      }
    },
    {
      "Score": 1054.5174606871788,
      "variance": 48.210023395468916,
      "rating_q975": 1068.5330133807518,
      "rating_q025": 1040.740179884586,
      "Votes": 2375,
      "Rank (UB)": 203,
      "Model": "smollm2-1.7b-instruct",
      "95% CI": {
        "rating_q025": 1040.740179884586,
        "rating_q975": 1068.5330133807518
      }
    },
    {
      "Score": 1053.695265646399,
      "variance": 29.770514791664056,
      "rating_q975": 1063.0716086724628,
      "rating_q025": 1043.5032418383946,
      "Votes": 2644,
      "Rank (UB)": 204,
      "Model": "mpt-30b-chat",
      "95% CI": {
        "rating_q025": 1043.5032418383946,
        "rating_q975": 1063.0716086724628
      }
    },
    {
      "Score": 1050.9910829498435,
      "variance": 10.568227816056567,
      "rating_q975": 1056.857128932393,
      "rating_q025": 1043.552245538695,
      "Votes": 7509,
      "Rank (UB)": 208,
      "Model": "codellama-34b-instruct",
      "95% CI": {
        "rating_q025": 1043.552245538695,
        "rating_q975": 1056.857128932393
      }
    },
    {
      "Score": 1050.226436453288,
      "variance": 3.787937943798583,
      "rating_q975": 1053.5022218774207,
      "rating_q025": 1045.9690771766632,
      "Votes": 19775,
      "Rank (UB)": 209,
      "Model": "vicuna-13b",
      "95% CI": {
        "rating_q025": 1045.9690771766632,
        "rating_q975": 1053.5022218774207
      }
    },
    {
      "Score": 1049.7754636402346,
      "variance": 68.47328605918864,
      "rating_q975": 1064.3823980402854,
      "rating_q025": 1033.2273193742012,
      "Votes": 1192,
      "Rank (UB)": 204,
      "Model": "codellama-70b-instruct",
      "95% CI": {
        "rating_q025": 1033.2273193742012,
        "rating_q975": 1064.3823980402854
      }
    },
    {
      "Score": 1048.7232155000181,
      "variance": 51.978169619066016,
      "rating_q975": 1064.705707262459,
      "rating_q025": 1035.9270366238507,
      "Votes": 1811,
      "Rank (UB)": 204,
      "Model": "zephyr-7b-alpha",
      "95% CI": {
        "rating_q025": 1035.9270366238507,
        "rating_q975": 1064.705707262459
      }
    },
    {
      "Score": 1045.6500011971755,
      "variance": 9.998891680511212,
      "rating_q975": 1051.4219490484606,
      "rating_q025": 1039.711157757171,
      "Votes": 9176,
      "Rank (UB)": 209,
      "Model": "gemma-7b-it",
      "95% CI": {
        "rating_q025": 1039.711157757171,
        "rating_q975": 1051.4219490484606
      }
    },
    {
      "Score": 1045.1786841718917,
      "variance": 3.5346239579174634,
      "rating_q975": 1048.430606969248,
      "rating_q025": 1041.5975890293432,
      "Votes": 21622,
      "Rank (UB)": 209,
      "Model": "phi-3-mini-128k-instruct",
      "95% CI": {
        "rating_q025": 1041.5975890293432,
        "rating_q975": 1048.430606969248
      }
    },
    {
      "Score": 1045.1186697019457,
      "variance": 7.723211902902515,
      "rating_q975": 1049.9799879250754,
      "rating_q025": 1039.8867571306305,
      "Votes": 14532,
      "Rank (UB)": 209,
      "Model": "llama-2-7b-chat",
      "95% CI": {
        "rating_q025": 1039.8867571306305,
        "rating_q975": 1049.9799879250754
      }
    },
    {
      "Score": 1043.1375829615608,
      "variance": 16.990239719978234,
      "rating_q975": 1049.8125776382767,
      "rating_q025": 1034.031804860909,
      "Votes": 5065,
      "Rank (UB)": 209,
      "Model": "qwen-14b-chat",
      "95% CI": {
        "rating_q025": 1034.031804860909,
        "rating_q975": 1049.8125776382767
      }
    },
    {
      "Score": 1042.2865696226486,
      "variance": 75.8166659400282,
      "rating_q975": 1055.667863811536,
      "rating_q025": 1027.2162179919555,
      "Votes": 1327,
      "Rank (UB)": 209,
      "Model": "falcon-180b-chat",
      "95% CI": {
        "rating_q025": 1027.2162179919555,
        "rating_q975": 1055.667863811536
      }
    },
    {
      "Score": 1041.0476338094527,
      "variance": 37.678389169836606,
      "rating_q975": 1054.0236862580757,
      "rating_q025": 1029.4068299090086,
      "Votes": 2996,
      "Rank (UB)": 209,
      "Model": "guanaco-33b",
      "95% CI": {
        "rating_q025": 1029.4068299090086,
        "rating_q975": 1054.0236862580757
      }
    },
    {
      "Score": 1028.936676205064,
      "variance": 10.161259136420663,
      "rating_q975": 1035.8976067963968,
      "rating_q025": 1024.2394927121134,
      "Votes": 11351,
      "Rank (UB)": 218,
      "Model": "gemma-1.1-2b-it",
      "95% CI": {
        "rating_q025": 1024.2394927121134,
        "rating_q975": 1035.8976067963968
      }
    },
    {
      "Score": 1025.4886330751165,
      "variance": 14.574261492459879,
      "rating_q975": 1032.3199422024782,
      "rating_q025": 1018.3629178257444,
      "Votes": 5276,
      "Rank (UB)": 220,
      "Model": "stripedhyena-nous-7b",
      "95% CI": {
        "rating_q025": 1018.3629178257444,
        "rating_q975": 1032.3199422024782
      }
    },
    {
      "Score": 1023.3675667070409,
      "variance": 14.28432928482074,
      "rating_q975": 1029.4766157113793,
      "rating_q025": 1015.366405585333,
      "Votes": 6503,
      "Rank (UB)": 220,
      "Model": "olmo-7b-instruct",
      "95% CI": {
        "rating_q025": 1015.366405585333,
        "rating_q975": 1029.4766157113793
      }
    },
    {
      "Score": 1015.8778988130457,
      "variance": 9.672634947427165,
      "rating_q975": 1021.7845435573528,
      "rating_q025": 1010.2837959735352,
      "Votes": 9142,
      "Rank (UB)": 223,
      "Model": "mistral-7b-instruct",
      "95% CI": {
        "rating_q025": 1010.2837959735352,
        "rating_q975": 1021.7845435573528
      }
    },
    {
      "Score": 1013.000568243808,
      "variance": 9.91488686763284,
      "rating_q975": 1018.4278888553921,
      "rating_q025": 1006.5879761314924,
      "Votes": 7017,
      "Rank (UB)": 223,
      "Model": "vicuna-7b",
      "95% CI": {
        "rating_q025": 1006.5879761314924,
        "rating_q975": 1018.4278888553921
      }
    },
    {
      "Score": 1011.6184438218936,
      "variance": 11.479794588798493,
      "rating_q975": 1018.0715630342985,
      "rating_q025": 1005.350818400104,
      "Votes": 8713,
      "Rank (UB)": 224,
      "Model": "palm-2",
      "95% CI": {
        "rating_q025": 1005.350818400104,
        "rating_q975": 1018.0715630342985
      }
    },
    {
      "Score": 997.7229104972236,
      "variance": 23.061521895791373,
      "rating_q975": 1004.9122566341961,
      "rating_q025": 988.606597300761,
      "Votes": 4918,
      "Rank (UB)": 228,
      "Model": "gemma-2b-it",
      "95% CI": {
        "rating_q025": 988.606597300761,
        "rating_q975": 1004.9122566341961
      }
    },
    {
      "Score": 996.5097333522602,
      "variance": 14.072418171925774,
      "rating_q975": 1003.5552307117052,
      "rating_q025": 989.6428283401848,
      "Votes": 7816,
      "Rank (UB)": 228,
      "Model": "qwen1.5-4b-chat",
      "95% CI": {
        "rating_q025": 989.6428283401848,
        "rating_q975": 1003.5552307117052
      }
    },
    {
      "Score": 972.7694096229395,
      "variance": 13.883014115072683,
      "rating_q975": 980.4380686900475,
      "rating_q025": 967.2423684076472,
      "Votes": 7020,
      "Rank (UB)": 230,
      "Model": "koala-13b",
      "95% CI": {
        "rating_q025": 967.2423684076472,
        "rating_q975": 980.4380686900475
      }
    },
    {
      "Score": 963.0468581837431,
      "variance": 20.414765380010508,
      "rating_q975": 973.1767739425118,
      "rating_q025": 953.7860132971983,
      "Votes": 4763,
      "Rank (UB)": 230,
      "Model": "chatglm3-6b",
      "95% CI": {
        "rating_q025": 953.7860132971983,
        "rating_q975": 973.1767739425118
      }
    },
    {
      "Score": 940.5517603443072,
      "variance": 62.104344560784575,
      "rating_q975": 953.937577903684,
      "rating_q025": 923.722996826205,
      "Votes": 1788,
      "Rank (UB)": 231,
      "Model": "gpt4all-13b-snoozy",
      "95% CI": {
        "rating_q025": 923.722996826205,
        "rating_q975": 953.937577903684
      }
    },
    {
      "Score": 936.3748992987944,
      "variance": 22.461537439063473,
      "rating_q975": 944.804840693061,
      "rating_q025": 927.40081900172,
      "Votes": 3997,
      "Rank (UB)": 232,
      "Model": "mpt-7b-chat",
      "95% CI": {
        "rating_q025": 927.40081900172,
        "rating_q975": 944.804840693061
      }
    },
    {
      "Score": 932.576964597954,
      "variance": 40.06186776451118,
      "rating_q975": 942.6674889984415,
      "rating_q025": 918.470292243781,
      "Votes": 2713,
      "Rank (UB)": 232,
      "Model": "chatglm2-6b",
      "95% CI": {
        "rating_q025": 918.470292243781,
        "rating_q975": 942.6674889984415
      }
    },
    {
      "Score": 929.8506694455286,
      "variance": 16.706491605020226,
      "rating_q975": 937.4443176465917,
      "rating_q025": 923.5215486952901,
      "Votes": 4920,
      "Rank (UB)": 232,
      "Model": "RWKV-4-Raven-14B",
      "95% CI": {
        "rating_q025": 923.5215486952901,
        "rating_q975": 937.4443176465917
      }
    },
    {
      "Score": 909.6605332203562,
      "variance": 15.640546041045637,
      "rating_q975": 917.553373912992,
      "rating_q025": 902.3969816664937,
      "Votes": 5864,
      "Rank (UB)": 236,
      "Model": "alpaca-13b",
      "95% CI": {
        "rating_q025": 902.3969816664937,
        "rating_q975": 917.553373912992
      }
    },
    {
      "Score": 901.2718478092579,
      "variance": 17.89571792489289,
      "rating_q975": 909.2153826246803,
      "rating_q025": 893.8332225666607,
      "Votes": 6368,
      "Rank (UB)": 236,
      "Model": "oasst-pythia-12b",
      "95% CI": {
        "rating_q025": 893.8332225666607,
        "rating_q975": 909.2153826246803
      }
    },
    {
      "Score": 887.263730939399,
      "variance": 18.925205560124557,
      "rating_q975": 896.1048778193004,
      "rating_q025": 878.4817963247175,
      "Votes": 4983,
      "Rank (UB)": 237,
      "Model": "chatglm-6b",
      "95% CI": {
        "rating_q025": 878.4817963247175,
        "rating_q975": 896.1048778193004
      }
    },
    {
      "Score": 876.0112397003761,
      "variance": 18.326232503765137,
      "rating_q975": 883.8174348925447,
      "rating_q025": 868.4103295063702,
      "Votes": 4288,
      "Rank (UB)": 238,
      "Model": "fastchat-t5-3b",
      "95% CI": {
        "rating_q025": 868.4103295063702,
        "rating_q975": 883.8174348925447
      }
    },
    {
      "Score": 848.2115170135622,
      "variance": 25.84405417290621,
      "rating_q975": 858.8669431139626,
      "rating_q025": 839.6192419088522,
      "Votes": 3336,
      "Rank (UB)": 240,
      "Model": "stablelm-tuned-alpha-7b",
      "95% CI": {
        "rating_q025": 839.6192419088522,
        "rating_q975": 858.8669431139626
      }
    },
    {
      "Score": 830.4363157161376,
      "variance": 29.986295936264337,
      "rating_q975": 840.4607166350397,
      "rating_q025": 821.5034025486798,
      "Votes": 3480,
      "Rank (UB)": 240,
      "Model": "dolly-v2-12b",
      "95% CI": {
        "rating_q025": 821.5034025486798,
        "rating_q975": 840.4607166350397
      }
    },
    {
      "Score": 807.7723199995062,
      "variance": 50.38074660532716,
      "rating_q975": 821.1479792754369,
      "rating_q025": 793.782966899935,
      "Votes": 2446,
      "Rank (UB)": 242,
      "Model": "llama-13b",
      "95% CI": {
        "rating_q025": 793.782966899935,
        "rating_q975": 821.1479792754369
      }
    }
  ]
}