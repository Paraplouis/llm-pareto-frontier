{
  "last_updated": "2025-06-18",
  "models": [
    {
      "Score": 1480.2553882079274,
      "variance": 10.119807336103435,
      "rating_q975": 1486.2499723887288,
      "rating_q025": 1474.2629085107596,
      "Votes": 8825,
      "Rank (UB)": 1,
      "Model": "gemini-2.5-pro-preview-06-05",
      "95% CI": {
        "rating_q025": 1474.2629085107596,
        "rating_q975": 1486.2499723887288
      }
    },
    {
      "Score": 1446.0,
      "variance": 7.006488387371308,
      "rating_q975": 1451.2400586589752,
      "rating_q025": 1440.8229955738366,
      "Votes": 13025,
      "Rank (UB)": 2,
      "Model": "gemini-2.5-pro-preview-05-06",
      "95% CI": {
        "rating_q025": 1440.8229955738366,
        "rating_q975": 1451.2400586589752
      }
    },
    {
      "Score": 1426.5989650210745,
      "variance": 5.062697601277578,
      "rating_q975": 1430.9690778284196,
      "rating_q025": 1422.6761457592213,
      "Votes": 16019,
      "Rank (UB)": 3,
      "Model": "o3-2025-04-16",
      "95% CI": {
        "rating_q025": 1422.6761457592213,
        "rating_q975": 1430.9690778284196
      }
    },
    {
      "Score": 1425.565023346508,
      "variance": 5.419527480861444,
      "rating_q975": 1430.4398350292909,
      "rating_q025": 1420.9868176746554,
      "Votes": 20638,
      "Rank (UB)": 3,
      "Model": "chatgpt-4o-latest-20250326",
      "95% CI": {
        "rating_q025": 1420.9868176746554,
        "rating_q975": 1430.4398350292909
      }
    },
    {
      "Score": 1420.8873181957533,
      "variance": 12.664657236568432,
      "rating_q975": 1427.8537697630118,
      "rating_q025": 1414.3320493503263,
      "Votes": 8423,
      "Rank (UB)": 3,
      "Model": "deepseek-r1-0528",
      "95% CI": {
        "rating_q025": 1414.3320493503263,
        "rating_q975": 1427.8537697630118
      }
    },
    {
      "Score": 1419.5419847230278,
      "variance": 6.384169957367004,
      "rating_q975": 1424.5947202617806,
      "rating_q025": 1414.710991999716,
      "Votes": 14034,
      "Rank (UB)": 3,
      "Model": "gemini-2.5-flash-preview-05-20",
      "95% CI": {
        "rating_q025": 1414.710991999716,
        "rating_q975": 1424.5947202617806
      }
    },
    {
      "Score": 1419.4467534187447,
      "variance": 4.567237995250875,
      "rating_q975": 1423.1477455240065,
      "rating_q025": 1414.8904102570611,
      "Votes": 22643,
      "Rank (UB)": 3,
      "Model": "grok-3-preview-02-24",
      "95% CI": {
        "rating_q025": 1414.8904102570611,
        "rating_q975": 1423.1477455240065
      }
    },
    {
      "Score": 1413.057139404899,
      "variance": 4.971350492296813,
      "rating_q975": 1417.5551011564935,
      "rating_q025": 1409.1316618579083,
      "Votes": 15271,
      "Rank (UB)": 5,
      "Model": "gpt-4.5-preview-2025-02-27",
      "95% CI": {
        "rating_q025": 1409.1316618579083,
        "rating_q975": 1417.5551011564935
      }
    },
    {
      "Score": 1397.574319380301,
      "variance": 5.404625001523624,
      "rating_q975": 1401.7606433749597,
      "rating_q025": 1393.3609430562854,
      "Votes": 14812,
      "Rank (UB)": 11,
      "Model": "gemini-2.5-flash-preview-04-17",
      "95% CI": {
        "rating_q025": 1393.3609430562854,
        "rating_q975": 1401.7606433749597
      }
    },
    {
      "Score": 1386.5421840984527,
      "variance": 14.108533316366978,
      "rating_q975": 1393.8751507586592,
      "rating_q025": 1380.425098676063,
      "Votes": 7837,
      "Rank (UB)": 11,
      "Model": "qwen3-235b-a22b-no-thinking",
      "95% CI": {
        "rating_q025": 1380.425098676063,
        "rating_q975": 1393.8751507586592
      }
    },
    {
      "Score": 1384.6347683676183,
      "variance": 6.1574125530764485,
      "rating_q975": 1390.248646549808,
      "rating_q025": 1380.595808539499,
      "Votes": 14635,
      "Rank (UB)": 14,
      "Model": "gpt-4.1-2025-04-14",
      "95% CI": {
        "rating_q025": 1380.595808539499,
        "rating_q975": 1390.248646549808
      }
    },
    {
      "Score": 1383.700866445306,
      "variance": 4.899399317736443,
      "rating_q975": 1388.321278681463,
      "rating_q025": 1380.2055283678949,
      "Votes": 17365,
      "Rank (UB)": 14,
      "Model": "deepseek-v3-0324",
      "95% CI": {
        "rating_q025": 1380.2055283678949,
        "rating_q975": 1388.321278681463
      }
    },
    {
      "Score": 1377.154683072285,
      "variance": 21.72561004242505,
      "rating_q975": 1386.7796813804887,
      "rating_q025": 1368.7694987668629,
      "Votes": 3905,
      "Rank (UB)": 14,
      "Model": "gemini-2.5-flash-lite-preview-06-17-thinking",
      "95% CI": {
        "rating_q025": 1368.7694987668629,
        "rating_q975": 1386.7796813804887
      }
    },
    {
      "Score": 1373.428719714109,
      "variance": 12.306662314020093,
      "rating_q975": 1380.795502821834,
      "rating_q025": 1366.6233506590972,
      "Votes": 6747,
      "Rank (UB)": 16,
      "Model": "hunyuan-turbos-20250416",
      "95% CI": {
        "rating_q025": 1366.6233506590972,
        "rating_q975": 1380.795502821834
      }
    },
    {
      "Score": 1373.2522555847645,
      "variance": 5.257913360080786,
      "rating_q975": 1377.3190880924133,
      "rating_q025": 1369.6756583858848,
      "Votes": 15254,
      "Rank (UB)": 19,
      "Model": "claude-opus-4-20250514",
      "95% CI": {
        "rating_q025": 1369.6756583858848,
        "rating_q975": 1377.3190880924133
      }
    },
    {
      "Score": 1373.1423442813139,
      "variance": 5.088119084604408,
      "rating_q975": 1376.723829000309,
      "rating_q025": 1368.4334985835676,
      "Votes": 19430,
      "Rank (UB)": 20,
      "Model": "deepseek-r1",
      "95% CI": {
        "rating_q025": 1368.4334985835676,
        "rating_q975": 1376.723829000309
      }
    },
    {
      "Score": 1365.3355130993448,
      "variance": 6.98288357950095,
      "rating_q975": 1369.4200211850289,
      "rating_q025": 1360.3557831215653,
      "Votes": 13385,
      "Rank (UB)": 23,
      "Model": "mistral-medium-2505",
      "95% CI": {
        "rating_q025": 1360.3557831215653,
        "rating_q975": 1369.4200211850289
      }
    },
    {
      "Score": 1364.9974138999398,
      "variance": 2.965106746402728,
      "rating_q975": 1368.081493169764,
      "rating_q025": 1362.108721212098,
      "Votes": 29038,
      "Rank (UB)": 25,
      "Model": "o1-2024-12-17",
      "95% CI": {
        "rating_q025": 1362.108721212098,
        "rating_q975": 1368.081493169764
      }
    },
    {
      "Score": 1363.0382922559115,
      "variance": 3.139050407114271,
      "rating_q975": 1366.514774355834,
      "rating_q025": 1359.7793206884915,
      "Votes": 34803,
      "Rank (UB)": 27,
      "Model": "gemini-2.0-flash-001",
      "95% CI": {
        "rating_q025": 1359.7793206884915,
        "rating_q975": 1366.514774355834
      }
    },
    {
      "Score": 1362.8264751288784,
      "variance": 7.2690484658847225,
      "rating_q975": 1367.7144533873009,
      "rating_q025": 1356.9046419289793,
      "Votes": 11429,
      "Rank (UB)": 25,
      "Model": "qwen3-235b-a22b",
      "95% CI": {
        "rating_q025": 1356.9046419289793,
        "rating_q975": 1367.7144533873009
      }
    },
    {
      "Score": 1361.5631056496768,
      "variance": 4.197012499380749,
      "rating_q975": 1365.835366538056,
      "rating_q025": 1357.9299004036575,
      "Votes": 14392,
      "Rank (UB)": 27,
      "Model": "o4-mini-2025-04-16",
      "95% CI": {
        "rating_q025": 1357.9299004036575,
        "rating_q975": 1365.835366538056
      }
    },
    {
      "Score": 1361.2771789684286,
      "variance": 9.69364036929507,
      "rating_q975": 1368.3185635453294,
      "rating_q025": 1354.820912951086,
      "Votes": 6984,
      "Rank (UB)": 25,
      "Model": "grok-3-mini-beta",
      "95% CI": {
        "rating_q025": 1354.820912951086,
        "rating_q975": 1368.3185635453294
      }
    },
    {
      "Score": 1360.3531382297797,
      "variance": 2.3522428908852144,
      "rating_q975": 1363.099900930992,
      "rating_q025": 1357.4893747233975,
      "Votes": 30065,
      "Rank (UB)": 27,
      "Model": "qwen2.5-max",
      "95% CI": {
        "rating_q025": 1357.4893747233975,
        "rating_q975": 1363.099900930992
      }
    },
    {
      "Score": 1356.174439268687,
      "variance": 4.3684935499895055,
      "rating_q975": 1360.2091650875557,
      "rating_q025": 1353.2391231277932,
      "Votes": 22300,
      "Rank (UB)": 29,
      "Model": "gemma-3-27b-it",
      "95% CI": {
        "rating_q025": 1353.2391231277932,
        "rating_q975": 1360.2091650875557
      }
    },
    {
      "Score": 1350.152908783235,
      "variance": 3.09583251706987,
      "rating_q975": 1353.1946708259627,
      "rating_q025": 1346.4342005281626,
      "Votes": 33177,
      "Rank (UB)": 36,
      "Model": "o1-preview",
      "95% CI": {
        "rating_q025": 1346.4342005281626,
        "rating_q975": 1353.1946708259627
      }
    },
    {
      "Score": 1346.2918381172178,
      "variance": 5.9245859894708355,
      "rating_q975": 1350.989177438189,
      "rating_q025": 1341.1270208552687,
      "Votes": 12143,
      "Rank (UB)": 37,
      "Model": "claude-sonnet-4-20250514",
      "95% CI": {
        "rating_q025": 1341.1270208552687,
        "rating_q975": 1350.989177438189
      }
    },
    {
      "Score": 1339.711746891935,
      "variance": 3.639541098129234,
      "rating_q975": 1343.8962426372111,
      "rating_q025": 1336.4956349930649,
      "Votes": 19404,
      "Rank (UB)": 38,
      "Model": "o3-mini-high",
      "95% CI": {
        "rating_q025": 1336.4956349930649,
        "rating_q975": 1343.8962426372111
      }
    },
    {
      "Score": 1337.8449586272204,
      "variance": 4.570394131228972,
      "rating_q975": 1341.7042942300695,
      "rating_q025": 1333.0759847909405,
      "Votes": 13519,
      "Rank (UB)": 38,
      "Model": "gpt-4.1-mini-2025-04-14",
      "95% CI": {
        "rating_q025": 1333.0759847909405,
        "rating_q975": 1341.7042942300695
      }
    },
    {
      "Score": 1335.6404373334663,
      "variance": 19.816722565363012,
      "rating_q975": 1343.6364733993835,
      "rating_q025": 1325.5333671573953,
      "Votes": 3976,
      "Rank (UB)": 38,
      "Model": "gemma-3-12b-it",
      "95% CI": {
        "rating_q025": 1325.5333671573953,
        "rating_q975": 1343.6364733993835
      }
    },
    {
      "Score": 1333.6758354446843,
      "variance": 3.5770087858677857,
      "rating_q975": 1337.2289881350998,
      "rating_q025": 1329.793268026643,
      "Votes": 22841,
      "Rank (UB)": 39,
      "Model": "deepseek-v3",
      "95% CI": {
        "rating_q025": 1329.793268026643,
        "rating_q975": 1337.2289881350998
      }
    },
    {
      "Score": 1332.1568852424596,
      "variance": 4.484534836007033,
      "rating_q975": 1335.8124931916332,
      "rating_q025": 1327.3889248684084,
      "Votes": 16438,
      "Rank (UB)": 40,
      "Model": "qwq-32b",
      "95% CI": {
        "rating_q025": 1327.3889248684084,
        "rating_q975": 1335.8124931916332
      }
    },
    {
      "Score": 1327.9716469888428,
      "variance": 3.015600556438721,
      "rating_q975": 1330.9293567026957,
      "rating_q025": 1324.651740624086,
      "Votes": 26104,
      "Rank (UB)": 41,
      "Model": "gemini-2.0-flash-lite-preview-02-05",
      "95% CI": {
        "rating_q025": 1324.651740624086,
        "rating_q975": 1330.9293567026957
      }
    },
    {
      "Score": 1326.1777347198847,
      "variance": 15.67334826141917,
      "rating_q975": 1332.250253760766,
      "rating_q025": 1317.126219004012,
      "Votes": 6028,
      "Rank (UB)": 41,
      "Model": "glm-4-plus-0111",
      "95% CI": {
        "rating_q025": 1317.126219004012,
        "rating_q975": 1332.250253760766
      }
    },
    {
      "Score": 1325.6063755039777,
      "variance": 11.97912239630589,
      "rating_q975": 1331.3592507842054,
      "rating_q025": 1318.2257267238715,
      "Votes": 6055,
      "Rank (UB)": 41,
      "Model": "qwen-plus-0125",
      "95% CI": {
        "rating_q025": 1318.2257267238715,
        "rating_q975": 1331.3592507842054
      }
    },
    {
      "Score": 1325.294645652571,
      "variance": 5.287889165915506,
      "rating_q975": 1330.905705344193,
      "rating_q025": 1322.4309551220565,
      "Votes": 21092,
      "Rank (UB)": 41,
      "Model": "command-a-03-2025",
      "95% CI": {
        "rating_q025": 1322.4309551220565,
        "rating_q975": 1330.905705344193
      }
    },
    {
      "Score": 1320.9470522873178,
      "variance": 2.3669818568742085,
      "rating_q975": 1324.0825748976583,
      "rating_q025": 1318.6484389841241,
      "Votes": 33256,
      "Rank (UB)": 46,
      "Model": "o3-mini",
      "95% CI": {
        "rating_q025": 1318.6484389841241,
        "rating_q975": 1324.0825748976583
      }
    },
    {
      "Score": 1319.9359060266768,
      "variance": 16.980402859450678,
      "rating_q975": 1329.0993521440034,
      "rating_q025": 1312.607101374388,
      "Votes": 5126,
      "Rank (UB)": 42,
      "Model": "step-2-16k-exp-202412",
      "95% CI": {
        "rating_q025": 1312.607101374388,
        "rating_q975": 1329.0993521440034
      }
    },
    {
      "Score": 1319.105385073927,
      "variance": 1.9286966173016995,
      "rating_q975": 1321.672380965302,
      "rating_q025": 1316.6014782587702,
      "Votes": 54951,
      "Rank (UB)": 47,
      "Model": "o1-mini",
      "95% CI": {
        "rating_q025": 1316.6014782587702,
        "rating_q975": 1321.672380965302
      }
    },
    {
      "Score": 1317.6165649833765,
      "variance": 30.203461976795897,
      "rating_q975": 1328.1491329800465,
      "rating_q025": 1307.6376030233444,
      "Votes": 2452,
      "Rank (UB)": 43,
      "Model": "hunyuan-turbos-20250226",
      "95% CI": {
        "rating_q025": 1307.6376030233444,
        "rating_q975": 1328.1491329800465
      }
    },
    {
      "Score": 1317.354541011182,
      "variance": 1.64815366331361,
      "rating_q975": 1319.7372910112615,
      "rating_q025": 1314.7353211314414,
      "Votes": 58645,
      "Rank (UB)": 47,
      "Model": "gemini-1.5-pro-002",
      "95% CI": {
        "rating_q025": 1314.7353211314414,
        "rating_q975": 1319.7372910112615
      }
    },
    {
      "Score": 1314.7062766336492,
      "variance": 4.1423697891040305,
      "rating_q975": 1319.41760779882,
      "rating_q025": 1311.6294492714285,
      "Votes": 22385,
      "Rank (UB)": 47,
      "Model": "claude-3-7-sonnet-20250219-thinking-32k",
      "95% CI": {
        "rating_q025": 1311.6294492714285,
        "rating_q975": 1319.41760779882
      }
    },
    {
      "Score": 1311.5775919646255,
      "variance": 50.408645764783614,
      "rating_q975": 1326.8579447842417,
      "rating_q025": 1299.0326984517874,
      "Votes": 2371,
      "Rank (UB)": 44,
      "Model": "llama-3.3-nemotron-49b-super-v1",
      "95% CI": {
        "rating_q025": 1299.0326984517874,
        "rating_q975": 1326.8579447842417
      }
    },
    {
      "Score": 1311.3479404311013,
      "variance": 30.326075186238054,
      "rating_q975": 1322.070509291019,
      "rating_q025": 1299.2654937753077,
      "Votes": 2510,
      "Rank (UB)": 47,
      "Model": "hunyuan-turbo-0110",
      "95% CI": {
        "rating_q025": 1299.2654937753077,
        "rating_q975": 1322.070509291019
      }
    },
    {
      "Score": 1307.3383817867584,
      "variance": 3.223758684270712,
      "rating_q975": 1310.8400242179682,
      "rating_q025": 1304.250063371744,
      "Votes": 26866,
      "Rank (UB)": 55,
      "Model": "claude-3-7-sonnet-20250219",
      "95% CI": {
        "rating_q025": 1304.250063371744,
        "rating_q975": 1310.8400242179682
      }
    },
    {
      "Score": 1303.0378092085577,
      "variance": 1.3305979580195457,
      "rating_q975": 1304.775731207459,
      "rating_q025": 1300.460976564564,
      "Votes": 67084,
      "Rank (UB)": 57,
      "Model": "grok-2-2024-08-13",
      "95% CI": {
        "rating_q025": 1300.460976564564,
        "rating_q975": 1304.775731207459
      }
    },
    {
      "Score": 1302.9931849794282,
      "variance": 21.179443110599646,
      "rating_q975": 1311.7713633308556,
      "rating_q025": 1294.2589517876681,
      "Votes": 3913,
      "Rank (UB)": 53,
      "Model": "gemma-3n-e4b-it",
      "95% CI": {
        "rating_q025": 1294.2589517876681,
        "rating_q975": 1311.7713633308556
      }
    },
    {
      "Score": 1302.2731324636093,
      "variance": 2.716844310382982,
      "rating_q975": 1304.8276271546233,
      "rating_q025": 1298.7774025634058,
      "Votes": 28968,
      "Rank (UB)": 57,
      "Model": "yi-lightning",
      "95% CI": {
        "rating_q025": 1298.7774025634058,
        "rating_q975": 1304.8276271546233
      }
    },
    {
      "Score": 1300.1988819590672,
      "variance": 1.0635849065231153,
      "rating_q975": 1301.8356539438735,
      "rating_q025": 1298.2834482401886,
      "Votes": 117747,
      "Rank (UB)": 58,
      "Model": "gpt-4o-2024-05-13",
      "95% CI": {
        "rating_q025": 1298.2834482401886,
        "rating_q975": 1301.8356539438735
      }
    },
    {
      "Score": 1298.8027872290986,
      "variance": 1.2430788688479424,
      "rating_q975": 1300.8586244405715,
      "rating_q025": 1296.6289914335566,
      "Votes": 74230,
      "Rank (UB)": 58,
      "Model": "claude-3-5-sonnet-20241022",
      "95% CI": {
        "rating_q025": 1296.6289914335566,
        "rating_q975": 1300.8586244405715
      }
    },
    {
      "Score": 1297.3390231915112,
      "variance": 8.38293006558767,
      "rating_q975": 1302.065367469381,
      "rating_q025": 1290.8104871513797,
      "Votes": 10715,
      "Rank (UB)": 58,
      "Model": "qwen2.5-plus-1127",
      "95% CI": {
        "rating_q025": 1290.8104871513797,
        "rating_q975": 1302.065367469381
      }
    },
    {
      "Score": 1294.264293829972,
      "variance": 8.18030483596057,
      "rating_q975": 1300.7309864385647,
      "rating_q025": 1290.0748151778098,
      "Votes": 7243,
      "Rank (UB)": 58,
      "Model": "deepseek-v2.5-1210",
      "95% CI": {
        "rating_q025": 1290.0748151778098,
        "rating_q975": 1300.7309864385647
      }
    },
    {
      "Score": 1290.6002590117953,
      "variance": 2.8358234299925873,
      "rating_q975": 1293.3954917463866,
      "rating_q025": 1286.9902871606205,
      "Votes": 26074,
      "Rank (UB)": 65,
      "Model": "athene-v2-chat",
      "95% CI": {
        "rating_q025": 1286.9902871606205,
        "rating_q975": 1293.3954917463866
      }
    },
    {
      "Score": 1290.1271278842055,
      "variance": 22.05050812378543,
      "rating_q975": 1299.2737472260399,
      "rating_q025": 1282.6938285043498,
      "Votes": 4321,
      "Rank (UB)": 59,
      "Model": "gemma-3-4b-it",
      "95% CI": {
        "rating_q025": 1282.6938285043498,
        "rating_q975": 1299.2737472260399
      }
    },
    {
      "Score": 1289.1772409115638,
      "variance": 2.6079653900795616,
      "rating_q975": 1292.444086192691,
      "rating_q025": 1286.2930594030372,
      "Votes": 27788,
      "Rank (UB)": 65,
      "Model": "glm-4-plus",
      "95% CI": {
        "rating_q025": 1286.2930594030372,
        "rating_q975": 1292.444086192691
      }
    },
    {
      "Score": 1288.1978683421073,
      "variance": 5.683155875397643,
      "rating_q975": 1292.9012323525362,
      "rating_q025": 1283.9320335950144,
      "Votes": 14164,
      "Rank (UB)": 65,
      "Model": "llama-4-maverick-17b-128e-instruct",
      "95% CI": {
        "rating_q025": 1283.9320335950144,
        "rating_q975": 1292.9012323525362
      }
    },
    {
      "Score": 1287.0754331756975,
      "variance": 21.40306478944709,
      "rating_q975": 1297.7663309955374,
      "rating_q025": 1279.102953656057,
      "Votes": 3856,
      "Rank (UB)": 63,
      "Model": "hunyuan-large-2025-02-10",
      "95% CI": {
        "rating_q025": 1279.102953656057,
        "rating_q975": 1297.7663309955374
      }
    },
    {
      "Score": 1286.8895101948583,
      "variance": 1.004676059005672,
      "rating_q975": 1289.1380200120634,
      "rating_q025": 1285.132397055811,
      "Votes": 72536,
      "Rank (UB)": 67,
      "Model": "gpt-4o-mini-2024-07-18",
      "95% CI": {
        "rating_q025": 1285.132397055811,
        "rating_q975": 1289.1380200120634
      }
    },
    {
      "Score": 1286.6803593281056,
      "variance": 2.55058823960972,
      "rating_q975": 1289.1978658576058,
      "rating_q025": 1283.6475993057707,
      "Votes": 37021,
      "Rank (UB)": 67,
      "Model": "gemini-1.5-flash-002",
      "95% CI": {
        "rating_q025": 1283.6475993057707,
        "rating_q975": 1289.1978658576058
      }
    },
    {
      "Score": 1285.9072569376217,
      "variance": 12.198460392697655,
      "rating_q975": 1293.0440092202427,
      "rating_q025": 1279.4100472508344,
      "Votes": 6302,
      "Rank (UB)": 65,
      "Model": "gpt-4.1-nano-2025-04-14",
      "95% CI": {
        "rating_q025": 1279.4100472508344,
        "rating_q975": 1293.0440092202427
      }
    },
    {
      "Score": 1284.1863904159295,
      "variance": 1.7993921317389974,
      "rating_q975": 1286.9946762299378,
      "rating_q025": 1281.7741051378157,
      "Votes": 43788,
      "Rank (UB)": 67,
      "Model": "llama-3.1-405b-instruct-bf16",
      "95% CI": {
        "rating_q025": 1281.7741051378157,
        "rating_q975": 1286.9946762299378
      }
    },
    {
      "Score": 1283.7703311118464,
      "variance": 9.960572556294682,
      "rating_q975": 1289.3966175009166,
      "rating_q025": 1277.9627437718593,
      "Votes": 7577,
      "Rank (UB)": 67,
      "Model": "llama-3.1-nemotron-70b-instruct",
      "95% CI": {
        "rating_q025": 1277.9627437718593,
        "rating_q975": 1289.3966175009166
      }
    },
    {
      "Score": 1283.4252850748671,
      "variance": 1.4171006443219107,
      "rating_q975": 1285.3658637594192,
      "rating_q025": 1280.8436967540802,
      "Votes": 86159,
      "Rank (UB)": 69,
      "Model": "claude-3-5-sonnet-20240620",
      "95% CI": {
        "rating_q025": 1280.8436967540802,
        "rating_q975": 1285.3658637594192
      }
    },
    {
      "Score": 1282.7039039363767,
      "variance": 1.4067980691815123,
      "rating_q975": 1284.6546656911144,
      "rating_q025": 1280.228056793565,
      "Votes": 63038,
      "Rank (UB)": 70,
      "Model": "llama-3.1-405b-instruct-fp8",
      "95% CI": {
        "rating_q025": 1280.228056793565,
        "rating_q975": 1284.6546656911144
      }
    },
    {
      "Score": 1281.7555994573252,
      "variance": 1.7783781402794792,
      "rating_q975": 1284.2940222662037,
      "rating_q025": 1279.3329286480084,
      "Votes": 52144,
      "Rank (UB)": 70,
      "Model": "gemini-advanced-0514",
      "95% CI": {
        "rating_q025": 1279.3329286480084,
        "rating_q975": 1284.2940222662037
      }
    },
    {
      "Score": 1281.3873052094468,
      "variance": 1.2332115387057283,
      "rating_q975": 1283.384705921422,
      "rating_q025": 1279.5186106892859,
      "Votes": 55442,
      "Rank (UB)": 72,
      "Model": "grok-2-mini-2024-08-13",
      "95% CI": {
        "rating_q025": 1279.5186106892859,
        "rating_q975": 1283.384705921422
      }
    },
    {
      "Score": 1280.4512107493317,
      "variance": 1.9298623769918868,
      "rating_q975": 1282.7689719804573,
      "rating_q025": 1277.8460088207269,
      "Votes": 47973,
      "Rank (UB)": 72,
      "Model": "gpt-4o-2024-08-06",
      "95% CI": {
        "rating_q025": 1277.8460088207269,
        "rating_q975": 1282.7689719804573
      }
    },
    {
      "Score": 1278.5435116279498,
      "variance": 4.714799279237263,
      "rating_q975": 1282.1494878776057,
      "rating_q025": 1274.0272708664238,
      "Votes": 17432,
      "Rank (UB)": 73,
      "Model": "qwen-max-0919",
      "95% CI": {
        "rating_q025": 1274.0272708664238,
        "rating_q975": 1282.1494878776057
      }
    },
    {
      "Score": 1275.9448665848345,
      "variance": 24.12190184750729,
      "rating_q975": 1284.5441617545885,
      "rating_q025": 1266.9775820247148,
      "Votes": 4014,
      "Rank (UB)": 70,
      "Model": "hunyuan-standard-2025-02-10",
      "95% CI": {
        "rating_q025": 1266.9775820247148,
        "rating_q975": 1284.5441617545885
      }
    },
    {
      "Score": 1275.0850532516224,
      "variance": 1.2301159323963213,
      "rating_q975": 1277.101943170774,
      "rating_q025": 1272.6863502490644,
      "Votes": 82435,
      "Rank (UB)": 83,
      "Model": "gemini-1.5-pro-001",
      "95% CI": {
        "rating_q025": 1272.6863502490644,
        "rating_q975": 1277.101943170774
      }
    },
    {
      "Score": 1273.296871735034,
      "variance": 3.689673740374058,
      "rating_q975": 1276.329742425126,
      "rating_q025": 1269.544778605187,
      "Votes": 26344,
      "Rank (UB)": 83,
      "Model": "deepseek-v2.5",
      "95% CI": {
        "rating_q025": 1269.544778605187,
        "rating_q975": 1276.329742425126
      }
    },
    {
      "Score": 1272.302136605938,
      "variance": 1.8807213653130115,
      "rating_q975": 1274.9248349959987,
      "rating_q025": 1269.8952693177455,
      "Votes": 45404,
      "Rank (UB)": 84,
      "Model": "llama-3.3-70b-instruct",
      "95% CI": {
        "rating_q025": 1269.8952693177455,
        "rating_q975": 1274.9248349959987
      }
    },
    {
      "Score": 1272.287944773455,
      "variance": 1.9718285538576272,
      "rating_q975": 1274.3477096908339,
      "rating_q025": 1269.4871468547738,
      "Votes": 41519,
      "Rank (UB)": 84,
      "Model": "qwen2.5-72b-instruct",
      "95% CI": {
        "rating_q025": 1269.4871468547738,
        "rating_q975": 1274.3477096908339
      }
    },
    {
      "Score": 1271.6363420999805,
      "variance": 1.2344472471705326,
      "rating_q975": 1273.8178871172765,
      "rating_q025": 1269.4906957372225,
      "Votes": 102133,
      "Rank (UB)": 85,
      "Model": "gpt-4-turbo-2024-04-09",
      "95% CI": {
        "rating_q025": 1269.4906957372225,
        "rating_q975": 1273.8178871172765
      }
    },
    {
      "Score": 1266.7698973424945,
      "variance": 1.8987785770250478,
      "rating_q975": 1269.3734722431934,
      "rating_q025": 1263.951564352405,
      "Votes": 48217,
      "Rank (UB)": 91,
      "Model": "mistral-large-2407",
      "95% CI": {
        "rating_q025": 1263.951564352405,
        "rating_q975": 1269.3734722431934
      }
    },
    {
      "Score": 1265.9333799271465,
      "variance": 24.654632032329083,
      "rating_q975": 1274.633430254178,
      "rating_q025": 1255.8030203165608,
      "Votes": 3258,
      "Rank (UB)": 84,
      "Model": "mistral-small-3.1-24b-instruct-2503",
      "95% CI": {
        "rating_q025": 1255.8030203165608,
        "rating_q975": 1274.633430254178
      }
    },
    {
      "Score": 1265.5485059633347,
      "variance": 3.67476771594574,
      "rating_q975": 1269.0084186192025,
      "rating_q025": 1261.5467405519964,
      "Votes": 20580,
      "Rank (UB)": 91,
      "Model": "athene-70b-0725",
      "95% CI": {
        "rating_q025": 1261.5467405519964,
        "rating_q975": 1269.0084186192025
      }
    },
    {
      "Score": 1265.225982167393,
      "variance": 1.122860672444956,
      "rating_q975": 1266.967377447467,
      "rating_q025": 1262.8416996838819,
      "Votes": 103748,
      "Rank (UB)": 92,
      "Model": "gpt-4-1106-preview",
      "95% CI": {
        "rating_q025": 1262.8416996838819,
        "rating_q975": 1266.967377447467
      }
    },
    {
      "Score": 1264.1319454483876,
      "variance": 2.6662658622889004,
      "rating_q975": 1267.0488510729194,
      "rating_q025": 1260.7078150819293,
      "Votes": 29633,
      "Rank (UB)": 91,
      "Model": "mistral-large-2411",
      "95% CI": {
        "rating_q025": 1260.7078150819293,
        "rating_q975": 1267.0488510729194
      }
    },
    {
      "Score": 1262.92825306758,
      "variance": 1.4981579517598984,
      "rating_q975": 1265.0685030298694,
      "rating_q025": 1260.614428515716,
      "Votes": 58637,
      "Rank (UB)": 92,
      "Model": "llama-3.1-70b-instruct",
      "95% CI": {
        "rating_q025": 1260.614428515716,
        "rating_q975": 1265.0685030298694
      }
    },
    {
      "Score": 1262.4881455766458,
      "variance": 0.74591329255731,
      "rating_q975": 1264.157976896515,
      "rating_q025": 1261.0480802418072,
      "Votes": 202641,
      "Rank (UB)": 92,
      "Model": "claude-3-opus-20240229",
      "95% CI": {
        "rating_q025": 1261.0480802418072,
        "rating_q975": 1264.157976896515
      }
    },
    {
      "Score": 1260.1738329763248,
      "variance": 2.933237341060129,
      "rating_q975": 1263.5301235472375,
      "rating_q025": 1257.03907174055,
      "Votes": 26371,
      "Rank (UB)": 93,
      "Model": "amazon-nova-pro-v1.0",
      "95% CI": {
        "rating_q025": 1257.03907174055,
        "rating_q975": 1263.5301235472375
      }
    },
    {
      "Score": 1260.0859316452108,
      "variance": 1.0730301950713934,
      "rating_q975": 1261.9394173847845,
      "rating_q025": 1258.184074982596,
      "Votes": 97079,
      "Rank (UB)": 94,
      "Model": "gpt-4-0125-preview",
      "95% CI": {
        "rating_q025": 1258.184074982596,
        "rating_q975": 1261.9394173847845
      }
    },
    {
      "Score": 1259.442369299832,
      "variance": 15.889412195475225,
      "rating_q975": 1267.100276966872,
      "rating_q025": 1252.725706768904,
      "Votes": 3010,
      "Rank (UB)": 91,
      "Model": "llama-3.1-tulu-3-70b",
      "95% CI": {
        "rating_q025": 1252.725706768904,
        "rating_q975": 1267.100276966872
      }
    },
    {
      "Score": 1253.043613189574,
      "variance": 1.8728964609532055,
      "rating_q975": 1255.6117100111937,
      "rating_q025": 1250.7159822056842,
      "Votes": 45698,
      "Rank (UB)": 101,
      "Model": "claude-3-5-haiku-20241022",
      "95% CI": {
        "rating_q025": 1250.7159822056842,
        "rating_q975": 1255.6117100111937
      }
    },
    {
      "Score": 1250.331632095888,
      "variance": 10.58005131473746,
      "rating_q975": 1256.1186956633142,
      "rating_q025": 1243.9707497060012,
      "Votes": 7948,
      "Rank (UB)": 100,
      "Model": "reka-core-20240904",
      "95% CI": {
        "rating_q025": 1243.9707497060012,
        "rating_q975": 1256.1186956633142
      }
    },
    {
      "Score": 1242.0831018113556,
      "variance": 1.5750605264494346,
      "rating_q975": 1244.1014820317941,
      "rating_q025": 1239.457374741518,
      "Votes": 65661,
      "Rank (UB)": 104,
      "Model": "gemini-1.5-flash-001",
      "95% CI": {
        "rating_q025": 1239.457374741518,
        "rating_q975": 1244.1014820317941
      }
    },
    {
      "Score": 1236.8519616066774,
      "variance": 7.816878277393212,
      "rating_q975": 1241.5293765609538,
      "rating_q025": 1231.0837427156664,
      "Votes": 9125,
      "Rank (UB)": 106,
      "Model": "jamba-1.5-large",
      "95% CI": {
        "rating_q025": 1231.0837427156664,
        "rating_q975": 1241.5293765609538
      }
    },
    {
      "Score": 1235.0371178734322,
      "variance": 0.873766909188518,
      "rating_q975": 1236.6391350299546,
      "rating_q025": 1233.184147417264,
      "Votes": 79538,
      "Rank (UB)": 108,
      "Model": "gemma-2-27b-it",
      "95% CI": {
        "rating_q025": 1233.184147417264,
        "rating_q975": 1236.6391350299546
      }
    },
    {
      "Score": 1232.5824351423971,
      "variance": 6.154353858066021,
      "rating_q975": 1238.2397904033785,
      "rating_q025": 1227.8431222181387,
      "Votes": 15321,
      "Rank (UB)": 107,
      "Model": "mistral-small-24b-instruct-2501",
      "95% CI": {
        "rating_q025": 1227.8431222181387,
        "rating_q975": 1238.2397904033785
      }
    },
    {
      "Score": 1232.4644763461536,
      "variance": 11.444286516352191,
      "rating_q975": 1238.2981333401237,
      "rating_q025": 1225.8366210720094,
      "Votes": 5730,
      "Rank (UB)": 107,
      "Model": "qwen2.5-coder-32b-instruct",
      "95% CI": {
        "rating_q025": 1225.8366210720094,
        "rating_q975": 1238.2981333401237
      }
    },
    {
      "Score": 1232.1035439844231,
      "variance": 3.514005254221974,
      "rating_q975": 1235.175008179227,
      "rating_q025": 1227.7021478155932,
      "Votes": 20646,
      "Rank (UB)": 108,
      "Model": "amazon-nova-lite-v1.0",
      "95% CI": {
        "rating_q025": 1227.7021478155932,
        "rating_q975": 1235.175008179227
      }
    },
    {
      "Score": 1231.3125754044886,
      "variance": 5.685979537988071,
      "rating_q975": 1235.4289352638468,
      "rating_q025": 1226.4622865437873,
      "Votes": 10548,
      "Rank (UB)": 108,
      "Model": "gemma-2-9b-it-simpo",
      "95% CI": {
        "rating_q025": 1226.4622865437873,
        "rating_q975": 1235.4289352638468
      }
    },
    {
      "Score": 1230.5029763946045,
      "variance": 5.52270804415178,
      "rating_q975": 1234.3663320326398,
      "rating_q025": 1225.7008736827277,
      "Votes": 10535,
      "Rank (UB)": 108,
      "Model": "command-r-plus-08-2024",
      "95% CI": {
        "rating_q025": 1225.7008736827277,
        "rating_q975": 1234.3663320326398
      }
    },
    {
      "Score": 1227.8021866026977,
      "variance": 1.8505089534903543,
      "rating_q975": 1230.0991409369085,
      "rating_q025": 1225.2506003524907,
      "Votes": 37697,
      "Rank (UB)": 111,
      "Model": "gemini-1.5-flash-8b-001",
      "95% CI": {
        "rating_q025": 1225.2506003524907,
        "rating_q975": 1230.0991409369085
      }
    },
    {
      "Score": 1226.9998495803964,
      "variance": 16.58184768162264,
      "rating_q975": 1234.2794783575546,
      "rating_q025": 1217.490779967325,
      "Votes": 3889,
      "Rank (UB)": 108,
      "Model": "llama-3.1-nemotron-51b-instruct",
      "95% CI": {
        "rating_q025": 1217.490779967325,
        "rating_q975": 1234.2794783575546
      }
    },
    {
      "Score": 1224.4081023144495,
      "variance": 3.0957971671036146,
      "rating_q975": 1228.2676602606953,
      "rating_q025": 1221.532953247808,
      "Votes": 28768,
      "Rank (UB)": 111,
      "Model": "c4ai-aya-expanse-32b",
      "95% CI": {
        "rating_q025": 1221.532953247808,
        "rating_q975": 1228.2676602606953
      }
    },
    {
      "Score": 1224.308467636503,
      "variance": 4.302777822643269,
      "rating_q975": 1228.4831315740619,
      "rating_q025": 1220.566709803216,
      "Votes": 20608,
      "Rank (UB)": 111,
      "Model": "nemotron-4-340b-instruct",
      "95% CI": {
        "rating_q025": 1220.566709803216,
        "rating_q975": 1228.4831315740619
      }
    },
    {
      "Score": 1221.8425430288612,
      "variance": 6.579473531423039,
      "rating_q975": 1227.323716776508,
      "rating_q025": 1216.8888702575612,
      "Votes": 10221,
      "Rank (UB)": 113,
      "Model": "glm-4-0520",
      "95% CI": {
        "rating_q025": 1216.8888702575612,
        "rating_q975": 1227.323716776508
      }
    },
    {
      "Score": 1221.7275162978644,
      "variance": 0.6761673929084774,
      "rating_q975": 1223.0369383689451,
      "rating_q025": 1219.870666156094,
      "Votes": 163629,
      "Rank (UB)": 119,
      "Model": "llama-3-70b-instruct",
      "95% CI": {
        "rating_q025": 1219.870666156094,
        "rating_q975": 1223.0369383689451
      }
    },
    {
      "Score": 1220.851391724953,
      "variance": 23.82797181878122,
      "rating_q975": 1230.647937277649,
      "rating_q025": 1212.6759011956503,
      "Votes": 3460,
      "Rank (UB)": 111,
      "Model": "olmo-2-0325-32b-instruct",
      "95% CI": {
        "rating_q025": 1212.6759011956503,
        "rating_q975": 1230.647937277649
      }
    },
    {
      "Score": 1220.7403998074383,
      "variance": 10.461721696653951,
      "rating_q975": 1226.4373100073165,
      "rating_q025": 1214.2275683603082,
      "Votes": 8132,
      "Rank (UB)": 114,
      "Model": "reka-flash-20240904",
      "95% CI": {
        "rating_q025": 1214.2275683603082,
        "rating_q975": 1226.4373100073165
      }
    },
    {
      "Score": 1220.567295747241,
      "variance": 2.9500122922564276,
      "rating_q975": 1224.585457828005,
      "rating_q025": 1217.4027244503181,
      "Votes": 25213,
      "Rank (UB)": 117,
      "Model": "phi-4",
      "95% CI": {
        "rating_q025": 1217.4027244503181,
        "rating_q975": 1224.585457828005
      }
    },
    {
      "Score": 1216.1939987760825,
      "variance": 1.0236098617615481,
      "rating_q975": 1217.9230205601227,
      "rating_q025": 1214.287314701395,
      "Votes": 113067,
      "Rank (UB)": 122,
      "Model": "claude-3-sonnet-20240229",
      "95% CI": {
        "rating_q025": 1214.287314701395,
        "rating_q975": 1217.9230205601227
      }
    },
    {
      "Score": 1213.2219440897982,
      "variance": 4.45094023833747,
      "rating_q975": 1217.2230836603808,
      "rating_q025": 1208.4647164894154,
      "Votes": 20654,
      "Rank (UB)": 125,
      "Model": "amazon-nova-micro-v1.0",
      "95% CI": {
        "rating_q025": 1208.4647164894154,
        "rating_q975": 1217.2230836603808
      }
    },
    {
      "Score": 1207.232238071343,
      "variance": 1.5908243967735662,
      "rating_q975": 1209.4336719516655,
      "rating_q025": 1204.9354879834887,
      "Votes": 57197,
      "Rank (UB)": 132,
      "Model": "gemma-2-9b-it",
      "95% CI": {
        "rating_q025": 1204.9354879834887,
        "rating_q975": 1209.4336719516655
      }
    },
    {
      "Score": 1205.1376897811447,
      "variance": 1.3070775231145901,
      "rating_q975": 1206.8851542861612,
      "rating_q025": 1202.4268394268158,
      "Votes": 80846,
      "Rank (UB)": 133,
      "Model": "command-r-plus",
      "95% CI": {
        "rating_q025": 1202.4268394268158,
        "rating_q975": 1206.8851542861612
      }
    },
    {
      "Score": 1204.1185136413587,
      "variance": 37.70264421986878,
      "rating_q975": 1215.252543459977,
      "rating_q025": 1191.6461023178224,
      "Votes": 2901,
      "Rank (UB)": 127,
      "Model": "hunyuan-standard-256k",
      "95% CI": {
        "rating_q025": 1191.6461023178224,
        "rating_q975": 1215.252543459977
      }
    },
    {
      "Score": 1202.2816244818007,
      "variance": 1.9004424252575118,
      "rating_q975": 1204.7640628667978,
      "rating_q025": 1199.053606153484,
      "Votes": 38872,
      "Rank (UB)": 134,
      "Model": "qwen2-72b-instruct",
      "95% CI": {
        "rating_q025": 1199.053606153484,
        "rating_q975": 1204.7640628667978
      }
    },
    {
      "Score": 1201.3683044961035,
      "variance": 1.4154663679124118,
      "rating_q975": 1203.074475424451,
      "rating_q025": 1198.7474646138921,
      "Votes": 55962,
      "Rank (UB)": 134,
      "Model": "gpt-4-0314",
      "95% CI": {
        "rating_q025": 1198.7474646138921,
        "rating_q975": 1203.074475424451
      }
    },
    {
      "Score": 1200.5245169690584,
      "variance": 20.043858939083602,
      "rating_q975": 1208.6257267145184,
      "rating_q025": 1191.3749612332333,
      "Votes": 3074,
      "Rank (UB)": 132,
      "Model": "llama-3.1-tulu-3-8b",
      "95% CI": {
        "rating_q025": 1191.3749612332333,
        "rating_q975": 1208.6257267145184
      }
    },
    {
      "Score": 1197.437156664742,
      "variance": 15.217807167753197,
      "rating_q975": 1205.2034858520408,
      "rating_q025": 1189.7901450498412,
      "Votes": 5111,
      "Rank (UB)": 133,
      "Model": "ministral-8b-2410",
      "95% CI": {
        "rating_q025": 1189.7901450498412,
        "rating_q975": 1205.2034858520408
      }
    },
    {
      "Score": 1195.2037971187283,
      "variance": 7.106866453039263,
      "rating_q975": 1199.4211725316202,
      "rating_q025": 1189.9351076536311,
      "Votes": 10391,
      "Rank (UB)": 135,
      "Model": "c4ai-aya-expanse-8b",
      "95% CI": {
        "rating_q025": 1189.9351076536311,
        "rating_q975": 1199.4211725316202
      }
    },
    {
      "Score": 1194.8406522145217,
      "variance": 6.286885162199461,
      "rating_q975": 1199.0505982281302,
      "rating_q025": 1189.4637047541894,
      "Votes": 10851,
      "Rank (UB)": 136,
      "Model": "command-r-08-2024",
      "95% CI": {
        "rating_q025": 1189.4637047541894,
        "rating_q975": 1199.0505982281302
      }
    },
    {
      "Score": 1194.51539821409,
      "variance": 1.1069897452803337,
      "rating_q975": 1196.4301599358598,
      "rating_q025": 1192.3746925383755,
      "Votes": 122309,
      "Rank (UB)": 137,
      "Model": "claude-3-haiku-20240307",
      "95% CI": {
        "rating_q025": 1192.3746925383755,
        "rating_q975": 1196.4301599358598
      }
    },
    {
      "Score": 1193.590970035479,
      "variance": 5.836073856864164,
      "rating_q975": 1198.0055683389912,
      "rating_q025": 1189.1118824590676,
      "Votes": 15753,
      "Rank (UB)": 137,
      "Model": "deepseek-coder-v2",
      "95% CI": {
        "rating_q025": 1189.1118824590676,
        "rating_q975": 1198.0055683389912
      }
    },
    {
      "Score": 1191.1460681419485,
      "variance": 9.411172155223525,
      "rating_q975": 1197.7901447297866,
      "rating_q025": 1184.6957925971963,
      "Votes": 9274,
      "Rank (UB)": 137,
      "Model": "jamba-1.5-mini",
      "95% CI": {
        "rating_q025": 1184.6957925971963,
        "rating_q975": 1197.7901447297866
      }
    },
    {
      "Score": 1190.869717898004,
      "variance": 1.5936111820042291,
      "rating_q975": 1193.1645096657576,
      "rating_q025": 1188.2034747320085,
      "Votes": 52578,
      "Rank (UB)": 138,
      "Model": "llama-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1188.2034747320085,
        "rating_q975": 1193.1645096657576
      }
    },
    {
      "Score": 1178.26626164884,
      "variance": 1.0588885059220385,
      "rating_q975": 1180.058177601131,
      "rating_q025": 1176.4355766450199,
      "Votes": 91614,
      "Rank (UB)": 148,
      "Model": "gpt-4-0613",
      "95% CI": {
        "rating_q025": 1176.4355766450199,
        "rating_q975": 1180.058177601131
      }
    },
    {
      "Score": 1176.4845374885804,
      "variance": 3.004769003413438,
      "rating_q975": 1179.5325376041603,
      "rating_q025": 1173.0678230650233,
      "Votes": 27430,
      "Rank (UB)": 148,
      "Model": "qwen1.5-110b-chat",
      "95% CI": {
        "rating_q025": 1173.0678230650233,
        "rating_q975": 1179.5325376041603
      }
    },
    {
      "Score": 1172.5897027524918,
      "variance": 4.217404254952813,
      "rating_q975": 1176.2420369004096,
      "rating_q025": 1168.0609529914607,
      "Votes": 25135,
      "Rank (UB)": 150,
      "Model": "yi-1.5-34b-chat",
      "95% CI": {
        "rating_q025": 1168.0609529914607,
        "rating_q975": 1176.2420369004096
      }
    },
    {
      "Score": 1172.5331260375644,
      "variance": 1.2639085517698998,
      "rating_q975": 1174.2972252363222,
      "rating_q025": 1170.016929622747,
      "Votes": 64926,
      "Rank (UB)": 150,
      "Model": "mistral-large-2402",
      "95% CI": {
        "rating_q025": 1170.016929622747,
        "rating_q975": 1174.2972252363222
      }
    },
    {
      "Score": 1171.0124803773288,
      "variance": 5.084546521402832,
      "rating_q975": 1175.5619733769859,
      "rating_q025": 1166.9496859362587,
      "Votes": 16027,
      "Rank (UB)": 150,
      "Model": "reka-flash-21b-20240226-online",
      "95% CI": {
        "rating_q025": 1166.9496859362587,
        "rating_q975": 1175.5619733769859
      }
    },
    {
      "Score": 1168.063942778751,
      "variance": 32.701959893924695,
      "rating_q975": 1179.6772771527367,
      "rating_q025": 1157.923236582302,
      "Votes": 3410,
      "Rank (UB)": 148,
      "Model": "qwq-32b-preview",
      "95% CI": {
        "rating_q025": 1157.923236582302,
        "rating_q975": 1179.6772771527367
      }
    },
    {
      "Score": 1167.180117959023,
      "variance": 0.834331286059717,
      "rating_q975": 1168.931096763657,
      "rating_q025": 1165.290854047417,
      "Votes": 109056,
      "Rank (UB)": 152,
      "Model": "llama-3-8b-instruct",
      "95% CI": {
        "rating_q025": 1165.290854047417,
        "rating_q975": 1168.931096763657
      }
    },
    {
      "Score": 1164.2040089454104,
      "variance": 7.806554260337338,
      "rating_q975": 1169.3101746320813,
      "rating_q025": 1159.1749977582408,
      "Votes": 10599,
      "Rank (UB)": 152,
      "Model": "internlm2_5-20b-chat",
      "95% CI": {
        "rating_q025": 1159.1749977582408,
        "rating_q975": 1169.3101746320813
      }
    },
    {
      "Score": 1163.8586817767914,
      "variance": 1.8658202539816349,
      "rating_q975": 1166.1380831604533,
      "rating_q025": 1161.149923996895,
      "Votes": 56398,
      "Rank (UB)": 154,
      "Model": "command-r",
      "95% CI": {
        "rating_q025": 1161.149923996895,
        "rating_q975": 1166.1380831604533
      }
    },
    {
      "Score": 1163.060610032911,
      "variance": 3.000251248982332,
      "rating_q975": 1166.1910725274442,
      "rating_q025": 1159.6803705295542,
      "Votes": 35556,
      "Rank (UB)": 154,
      "Model": "mistral-medium",
      "95% CI": {
        "rating_q025": 1159.6803705295542,
        "rating_q975": 1166.1910725274442
      }
    },
    {
      "Score": 1162.7797549060301,
      "variance": 1.905124283684391,
      "rating_q975": 1165.1342940836385,
      "rating_q025": 1160.27869161006,
      "Votes": 53751,
      "Rank (UB)": 155,
      "Model": "mixtral-8x22b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1160.27869161006,
        "rating_q975": 1165.1342940836385
      }
    },
    {
      "Score": 1162.622229452794,
      "variance": 4.531701354285157,
      "rating_q975": 1166.5203090627292,
      "rating_q025": 1158.7817502830226,
      "Votes": 25803,
      "Rank (UB)": 154,
      "Model": "reka-flash-21b-20240226",
      "95% CI": {
        "rating_q025": 1158.7817502830226,
        "rating_q975": 1166.5203090627292
      }
    },
    {
      "Score": 1162.6210837184544,
      "variance": 1.9919383943717506,
      "rating_q975": 1165.2711363785402,
      "rating_q025": 1160.3276813377333,
      "Votes": 40658,
      "Rank (UB)": 155,
      "Model": "qwen1.5-72b-chat",
      "95% CI": {
        "rating_q025": 1160.3276813377333,
        "rating_q975": 1165.2711363785402
      }
    },
    {
      "Score": 1159.0194825030349,
      "variance": 1.8095742445975194,
      "rating_q975": 1161.1634365783573,
      "rating_q025": 1156.3638434246964,
      "Votes": 48892,
      "Rank (UB)": 155,
      "Model": "gemma-2-2b-it",
      "95% CI": {
        "rating_q025": 1156.3638434246964,
        "rating_q975": 1161.1634365783573
      }
    },
    {
      "Score": 1157.9865980964878,
      "variance": 24.05368511311592,
      "rating_q975": 1166.7504278768918,
      "rating_q025": 1148.294601541792,
      "Votes": 3289,
      "Rank (UB)": 154,
      "Model": "granite-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1148.294601541792,
        "rating_q975": 1166.7504278768918
      }
    },
    {
      "Score": 1146.5361120590342,
      "variance": 5.19527347885059,
      "rating_q975": 1150.5333064436688,
      "rating_q025": 1141.819332297733,
      "Votes": 18800,
      "Rank (UB)": 164,
      "Model": "gemini-pro-dev-api",
      "95% CI": {
        "rating_q025": 1141.819332297733,
        "rating_q975": 1150.5333064436688
      }
    },
    {
      "Score": 1142.3406176897804,
      "variance": 19.97644190665659,
      "rating_q975": 1149.9574742618956,
      "rating_q025": 1133.7151167485797,
      "Votes": 4854,
      "Rank (UB)": 164,
      "Model": "zephyr-orpo-141b-A35b-v0.1",
      "95% CI": {
        "rating_q025": 1133.7151167485797,
        "rating_q975": 1149.9574742618956
      }
    },
    {
      "Score": 1140.5605542410299,
      "variance": 3.582346263236489,
      "rating_q975": 1143.938639189686,
      "rating_q025": 1136.7607022878697,
      "Votes": 22765,
      "Rank (UB)": 165,
      "Model": "qwen1.5-32b-chat",
      "95% CI": {
        "rating_q025": 1136.7607022878697,
        "rating_q975": 1143.938639189686
      }
    },
    {
      "Score": 1138.146880959005,
      "variance": 2.975085636878116,
      "rating_q975": 1141.450196712886,
      "rating_q025": 1135.0468206077535,
      "Votes": 26105,
      "Rank (UB)": 166,
      "Model": "phi-3-medium-4k-instruct",
      "95% CI": {
        "rating_q025": 1135.0468206077535,
        "rating_q975": 1141.450196712886
      }
    },
    {
      "Score": 1134.5504412963148,
      "variance": 25.006537233917875,
      "rating_q975": 1143.7786112707338,
      "rating_q025": 1125.4492855140884,
      "Votes": 3380,
      "Rank (UB)": 165,
      "Model": "granite-3.1-2b-instruct",
      "95% CI": {
        "rating_q025": 1125.4492855140884,
        "rating_q975": 1143.7786112707338
      }
    },
    {
      "Score": 1134.1207079639375,
      "variance": 4.401780918526635,
      "rating_q975": 1138.1350123871105,
      "rating_q025": 1130.1800500925879,
      "Votes": 16676,
      "Rank (UB)": 167,
      "Model": "starling-lm-7b-beta",
      "95% CI": {
        "rating_q025": 1130.1800500925879,
        "rating_q975": 1138.1350123871105
      }
    },
    {
      "Score": 1129.2392472267748,
      "variance": 1.2903073202923647,
      "rating_q975": 1131.1712120672946,
      "rating_q025": 1127.2576223731724,
      "Votes": 76126,
      "Rank (UB)": 171,
      "Model": "mixtral-8x7b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1127.2576223731724,
        "rating_q975": 1131.1712120672946
      }
    },
    {
      "Score": 1126.4386616622353,
      "variance": 4.978034699770373,
      "rating_q975": 1130.4306596235085,
      "rating_q025": 1122.3517363153055,
      "Votes": 15917,
      "Rank (UB)": 171,
      "Model": "yi-34b-chat",
      "95% CI": {
        "rating_q025": 1122.3517363153055,
        "rating_q975": 1130.4306596235085
      }
    },
    {
      "Score": 1125.882531356775,
      "variance": 12.73760615664362,
      "rating_q975": 1131.9114213596267,
      "rating_q025": 1117.7642024965376,
      "Votes": 6557,
      "Rank (UB)": 171,
      "Model": "gemini-pro",
      "95% CI": {
        "rating_q025": 1117.7642024965376,
        "rating_q975": 1131.9114213596267
      }
    },
    {
      "Score": 1124.1521797654573,
      "variance": 3.4715813112599467,
      "rating_q975": 1127.4081364561814,
      "rating_q025": 1120.4454093061756,
      "Votes": 18687,
      "Rank (UB)": 174,
      "Model": "qwen1.5-14b-chat",
      "95% CI": {
        "rating_q025": 1120.4454093061756,
        "rating_q975": 1127.4081364561814
      }
    },
    {
      "Score": 1121.6802607145667,
      "variance": 9.885235815893221,
      "rating_q975": 1127.961093331457,
      "rating_q025": 1115.8936497076436,
      "Votes": 8383,
      "Rank (UB)": 174,
      "Model": "wizardlm-70b",
      "95% CI": {
        "rating_q025": 1115.8936497076436,
        "rating_q975": 1127.961093331457
      }
    },
    {
      "Score": 1121.079123122949,
      "variance": 1.6784752340302997,
      "rating_q975": 1123.2039577997177,
      "rating_q025": 1118.5695407842227,
      "Votes": 68867,
      "Rank (UB)": 176,
      "Model": "gpt-3.5-turbo-0125",
      "95% CI": {
        "rating_q025": 1118.5695407842227,
        "rating_q975": 1123.2039577997177
      }
    },
    {
      "Score": 1118.3478102798133,
      "variance": 3.01541711237945,
      "rating_q975": 1121.3047272976157,
      "rating_q025": 1115.0709346406804,
      "Votes": 33743,
      "Rank (UB)": 178,
      "Model": "dbrx-instruct-preview",
      "95% CI": {
        "rating_q025": 1115.0709346406804,
        "rating_q975": 1121.3047272976157
      }
    },
    {
      "Score": 1117.9423373329148,
      "variance": 13.353524326165909,
      "rating_q975": 1124.844480945931,
      "rating_q025": 1111.7264257531679,
      "Votes": 8390,
      "Rank (UB)": 176,
      "Model": "llama-3.2-3b-instruct",
      "95% CI": {
        "rating_q025": 1111.7264257531679,
        "rating_q975": 1124.844480945931
      }
    },
    {
      "Score": 1117.1888575285684,
      "variance": 5.553817693087298,
      "rating_q975": 1120.858075182515,
      "rating_q025": 1112.4901972336124,
      "Votes": 18476,
      "Rank (UB)": 178,
      "Model": "phi-3-small-8k-instruct",
      "95% CI": {
        "rating_q025": 1112.4901972336124,
        "rating_q975": 1120.858075182515
      }
    },
    {
      "Score": 1114.261410464055,
      "variance": 13.029977235029133,
      "rating_q975": 1121.7420389260785,
      "rating_q025": 1107.1956887163458,
      "Votes": 6658,
      "Rank (UB)": 177,
      "Model": "tulu-2-dpo-70b",
      "95% CI": {
        "rating_q025": 1107.1956887163458,
        "rating_q975": 1121.7420389260785
      }
    },
    {
      "Score": 1108.3797944606208,
      "variance": 11.523228765395563,
      "rating_q975": 1113.7603398937022,
      "rating_q025": 1101.0569234209736,
      "Votes": 7002,
      "Rank (UB)": 184,
      "Model": "granite-3.0-8b-instruct",
      "95% CI": {
        "rating_q025": 1101.0569234209736,
        "rating_q975": 1113.7603398937022
      }
    },
    {
      "Score": 1108.1507032592433,
      "variance": 2.2348305401762207,
      "rating_q975": 1110.291258519583,
      "rating_q025": 1105.1586107961198,
      "Votes": 39595,
      "Rank (UB)": 186,
      "Model": "llama-2-70b-chat",
      "95% CI": {
        "rating_q025": 1105.1586107961198,
        "rating_q975": 1110.291258519583
      }
    },
    {
      "Score": 1106.6217932657273,
      "variance": 4.908551863156655,
      "rating_q975": 1110.393376728587,
      "rating_q025": 1102.3441246862196,
      "Votes": 12990,
      "Rank (UB)": 186,
      "Model": "openchat-3.5-0106",
      "95% CI": {
        "rating_q025": 1102.3441246862196,
        "rating_q975": 1110.393376728587
      }
    },
    {
      "Score": 1105.9215148710837,
      "variance": 4.0659761877253064,
      "rating_q975": 1109.8981181588506,
      "rating_q025": 1102.8113759152602,
      "Votes": 22936,
      "Rank (UB)": 186,
      "Model": "vicuna-33b",
      "95% CI": {
        "rating_q025": 1102.8113759152602,
        "rating_q975": 1109.8981181588506
      }
    },
    {
      "Score": 1105.1642382799791,
      "variance": 2.8855852707875758,
      "rating_q975": 1107.6105464950356,
      "rating_q025": 1101.5615479311368,
      "Votes": 34173,
      "Rank (UB)": 186,
      "Model": "snowflake-arctic-instruct",
      "95% CI": {
        "rating_q025": 1101.5615479311368,
        "rating_q975": 1107.6105464950356
      }
    },
    {
      "Score": 1103.606272251771,
      "variance": 7.480255150783541,
      "rating_q975": 1109.2188588058214,
      "rating_q025": 1098.2575834761442,
      "Votes": 10415,
      "Rank (UB)": 186,
      "Model": "starling-lm-7b-alpha",
      "95% CI": {
        "rating_q025": 1098.2575834761442,
        "rating_q975": 1109.2188588058214
      }
    },
    {
      "Score": 1099.3048742182796,
      "variance": 24.17414890774434,
      "rating_q975": 1109.2320695276076,
      "rating_q025": 1089.457506993413,
      "Votes": 3836,
      "Rank (UB)": 186,
      "Model": "nous-hermes-2-mixtral-8x7b-dpo",
      "95% CI": {
        "rating_q025": 1089.457506993413,
        "rating_q975": 1109.2320695276076
      }
    },
    {
      "Score": 1098.921908681879,
      "variance": 3.2465660975295862,
      "rating_q975": 1103.201671840093,
      "rating_q025": 1095.9418566870768,
      "Votes": 25070,
      "Rank (UB)": 188,
      "Model": "gemma-1.1-7b-it",
      "95% CI": {
        "rating_q025": 1095.9418566870768,
        "rating_q975": 1103.201671840093
      }
    },
    {
      "Score": 1095.8739286349187,
      "variance": 25.988265174911785,
      "rating_q975": 1106.0712360802936,
      "rating_q025": 1086.6150692100553,
      "Votes": 3636,
      "Rank (UB)": 187,
      "Model": "llama2-70b-steerlm-chat",
      "95% CI": {
        "rating_q025": 1086.6150692100553,
        "rating_q975": 1106.0712360802936
      }
    },
    {
      "Score": 1092.1717737571353,
      "variance": 15.87197346113632,
      "rating_q975": 1098.4346802967686,
      "rating_q025": 1085.2315992969752,
      "Votes": 4988,
      "Rank (UB)": 192,
      "Model": "deepseek-llm-67b-chat",
      "95% CI": {
        "rating_q025": 1085.2315992969752,
        "rating_q975": 1098.4346802967686
      }
    },
    {
      "Score": 1091.7123490081694,
      "variance": 11.200327736532744,
      "rating_q975": 1097.8421063292888,
      "rating_q025": 1085.5405945095902,
      "Votes": 8106,
      "Rank (UB)": 193,
      "Model": "openchat-3.5",
      "95% CI": {
        "rating_q025": 1085.5405945095902,
        "rating_q975": 1097.8421063292888
      }
    },
    {
      "Score": 1089.4824694130323,
      "variance": 16.18235518707998,
      "rating_q975": 1097.7477280031703,
      "rating_q025": 1081.8626992544055,
      "Votes": 5088,
      "Rank (UB)": 193,
      "Model": "openhermes-2.5-mistral-7b",
      "95% CI": {
        "rating_q025": 1081.8626992544055,
        "rating_q975": 1097.7477280031703
      }
    },
    {
      "Score": 1088.9451954570263,
      "variance": 13.050187099931657,
      "rating_q975": 1095.106314710715,
      "rating_q025": 1081.5900677739628,
      "Votes": 7191,
      "Rank (UB)": 194,
      "Model": "granite-3.0-2b-instruct",
      "95% CI": {
        "rating_q025": 1081.5900677739628,
        "rating_q975": 1095.106314710715
      }
    },
    {
      "Score": 1087.501022747661,
      "variance": 4.627076528221722,
      "rating_q975": 1091.7779698758993,
      "rating_q025": 1083.4346524192547,
      "Votes": 20067,
      "Rank (UB)": 194,
      "Model": "mistral-7b-instruct-v0.2",
      "95% CI": {
        "rating_q025": 1083.4346524192547,
        "rating_q975": 1091.7779698758993
      }
    },
    {
      "Score": 1086.0023616877224,
      "variance": 5.914178115030839,
      "rating_q975": 1090.4208970211196,
      "rating_q025": 1081.770662308593,
      "Votes": 12808,
      "Rank (UB)": 194,
      "Model": "phi-3-mini-4k-instruct-june-2024",
      "95% CI": {
        "rating_q025": 1081.770662308593,
        "rating_q975": 1090.4208970211196
      }
    },
    {
      "Score": 1084.9438581696809,
      "variance": 22.235845305113152,
      "rating_q975": 1093.376121856932,
      "rating_q025": 1077.1379429673452,
      "Votes": 4872,
      "Rank (UB)": 194,
      "Model": "qwen1.5-7b-chat",
      "95% CI": {
        "rating_q025": 1077.1379429673452,
        "rating_q975": 1093.376121856932
      }
    },
    {
      "Score": 1082.8636663132354,
      "variance": 4.477323215965785,
      "rating_q975": 1086.2369165917025,
      "rating_q025": 1079.1315389977312,
      "Votes": 17036,
      "Rank (UB)": 196,
      "Model": "gpt-3.5-turbo-1106",
      "95% CI": {
        "rating_q025": 1079.1315389977312,
        "rating_q975": 1086.2369165917025
      }
    },
    {
      "Score": 1081.6445296378101,
      "variance": 4.195915533925712,
      "rating_q975": 1085.929921311501,
      "rating_q025": 1078.0456260609785,
      "Votes": 21097,
      "Rank (UB)": 197,
      "Model": "phi-3-mini-4k-instruct",
      "95% CI": {
        "rating_q025": 1078.0456260609785,
        "rating_q975": 1085.929921311501
      }
    },
    {
      "Score": 1078.4090561721732,
      "variance": 6.006482440422865,
      "rating_q975": 1082.8415826976773,
      "rating_q025": 1072.9790528616877,
      "Votes": 19722,
      "Rank (UB)": 200,
      "Model": "llama-2-13b-chat",
      "95% CI": {
        "rating_q025": 1072.9790528616877,
        "rating_q975": 1082.8415826976773
      }
    },
    {
      "Score": 1077.6424680848402,
      "variance": 54.99558756553711,
      "rating_q975": 1091.2224669011023,
      "rating_q025": 1064.676343586269,
      "Votes": 1714,
      "Rank (UB)": 194,
      "Model": "dolphin-2.2.1-mistral-7b",
      "95% CI": {
        "rating_q025": 1064.676343586269,
        "rating_q975": 1091.2224669011023
      }
    },
    {
      "Score": 1077.31136304575,
      "variance": 18.47744115101804,
      "rating_q975": 1086.3834511211821,
      "rating_q025": 1069.8038632374585,
      "Votes": 4286,
      "Rank (UB)": 196,
      "Model": "solar-10.7b-instruct-v1.0",
      "95% CI": {
        "rating_q025": 1069.8038632374585,
        "rating_q975": 1086.3834511211821
      }
    },
    {
      "Score": 1073.9584091691067,
      "variance": 10.952580594894826,
      "rating_q975": 1079.50740368215,
      "rating_q025": 1066.8529219574034,
      "Votes": 7176,
      "Rank (UB)": 203,
      "Model": "wizardlm-13b",
      "95% CI": {
        "rating_q025": 1066.8529219574034,
        "rating_q975": 1079.50740368215
      }
    },
    {
      "Score": 1068.9050921405396,
      "variance": 9.784612095038852,
      "rating_q975": 1074.731202420374,
      "rating_q025": 1064.0348825183812,
      "Votes": 8523,
      "Rank (UB)": 206,
      "Model": "llama-3.2-1b-instruct",
      "95% CI": {
        "rating_q025": 1064.0348825183812,
        "rating_q975": 1074.731202420374
      }
    },
    {
      "Score": 1068.45961645641,
      "variance": 10.357087293019479,
      "rating_q975": 1074.4456825607258,
      "rating_q025": 1062.5792459116828,
      "Votes": 11321,
      "Rank (UB)": 206,
      "Model": "zephyr-7b-beta",
      "95% CI": {
        "rating_q025": 1062.5792459116828,
        "rating_q975": 1074.4456825607258
      }
    },
    {
      "Score": 1061.5801947513366,
      "variance": 45.27234165323566,
      "rating_q975": 1074.8249840129022,
      "rating_q025": 1049.2502403764163,
      "Votes": 2375,
      "Rank (UB)": 206,
      "Model": "smollm2-1.7b-instruct",
      "95% CI": {
        "rating_q025": 1049.2502403764163,
        "rating_q975": 1074.8249840129022
      }
    },
    {
      "Score": 1060.7656089321351,
      "variance": 32.359359232135205,
      "rating_q975": 1069.1179376264265,
      "rating_q025": 1049.6696785452398,
      "Votes": 2644,
      "Rank (UB)": 208,
      "Model": "mpt-30b-chat",
      "95% CI": {
        "rating_q025": 1049.6696785452398,
        "rating_q975": 1069.1179376264265
      }
    },
    {
      "Score": 1057.9935130751505,
      "variance": 12.38851940598694,
      "rating_q975": 1063.4566932336422,
      "rating_q025": 1050.6491908856615,
      "Votes": 7509,
      "Rank (UB)": 211,
      "Model": "codellama-34b-instruct",
      "95% CI": {
        "rating_q025": 1050.6491908856615,
        "rating_q975": 1063.4566932336422
      }
    },
    {
      "Score": 1057.2409891693646,
      "variance": 6.0058774175134415,
      "rating_q975": 1061.7952354988327,
      "rating_q025": 1052.5343027840745,
      "Votes": 19775,
      "Rank (UB)": 212,
      "Model": "vicuna-13b",
      "95% CI": {
        "rating_q025": 1052.5343027840745,
        "rating_q975": 1061.7952354988327
      }
    },
    {
      "Score": 1056.7806304095066,
      "variance": 62.94736149443712,
      "rating_q975": 1074.285990582512,
      "rating_q025": 1042.8382827641726,
      "Votes": 1192,
      "Rank (UB)": 206,
      "Model": "codellama-70b-instruct",
      "95% CI": {
        "rating_q025": 1042.8382827641726,
        "rating_q975": 1074.285990582512
      }
    },
    {
      "Score": 1055.7581809706294,
      "variance": 52.582953299607425,
      "rating_q975": 1069.0723873056052,
      "rating_q025": 1043.815501795411,
      "Votes": 1811,
      "Rank (UB)": 208,
      "Model": "zephyr-7b-alpha",
      "95% CI": {
        "rating_q025": 1043.815501795411,
        "rating_q975": 1069.0723873056052
      }
    },
    {
      "Score": 1052.627403953383,
      "variance": 11.503765196633388,
      "rating_q975": 1060.4380569238776,
      "rating_q025": 1046.6109489859311,
      "Votes": 9176,
      "Rank (UB)": 212,
      "Model": "gemma-7b-it",
      "95% CI": {
        "rating_q025": 1046.6109489859311,
        "rating_q975": 1060.4380569238776
      }
    },
    {
      "Score": 1052.166803754639,
      "variance": 3.288832641696345,
      "rating_q975": 1054.998635842115,
      "rating_q025": 1048.7003285672565,
      "Votes": 21622,
      "Rank (UB)": 212,
      "Model": "phi-3-mini-128k-instruct",
      "95% CI": {
        "rating_q025": 1048.7003285672565,
        "rating_q975": 1054.998635842115
      }
    },
    {
      "Score": 1052.1284701809625,
      "variance": 7.510610367361011,
      "rating_q975": 1057.0303590587016,
      "rating_q025": 1046.7320250706402,
      "Votes": 14532,
      "Rank (UB)": 212,
      "Model": "llama-2-7b-chat",
      "95% CI": {
        "rating_q025": 1046.7320250706402,
        "rating_q975": 1057.0303590587016
      }
    },
    {
      "Score": 1050.158529103501,
      "variance": 13.498184604033638,
      "rating_q975": 1056.900730677152,
      "rating_q025": 1042.0487461606274,
      "Votes": 5065,
      "Rank (UB)": 212,
      "Model": "qwen-14b-chat",
      "95% CI": {
        "rating_q025": 1042.0487461606274,
        "rating_q975": 1056.900730677152
      }
    },
    {
      "Score": 1049.3030911985688,
      "variance": 55.8051162144181,
      "rating_q975": 1064.6475950732345,
      "rating_q025": 1036.0188141449469,
      "Votes": 1327,
      "Rank (UB)": 210,
      "Model": "falcon-180b-chat",
      "95% CI": {
        "rating_q025": 1036.0188141449469,
        "rating_q975": 1064.6475950732345
      }
    },
    {
      "Score": 1048.0966386778719,
      "variance": 30.829600416501137,
      "rating_q975": 1057.765666955153,
      "rating_q025": 1036.5784089765978,
      "Votes": 2996,
      "Rank (UB)": 212,
      "Model": "guanaco-33b",
      "95% CI": {
        "rating_q025": 1036.5784089765978,
        "rating_q975": 1057.765666955153
      }
    },
    {
      "Score": 1035.9095563112635,
      "variance": 6.80601359527204,
      "rating_q975": 1040.7615508689626,
      "rating_q025": 1031.0941153128392,
      "Votes": 11351,
      "Rank (UB)": 223,
      "Model": "gemma-1.1-2b-it",
      "95% CI": {
        "rating_q025": 1031.0941153128392,
        "rating_q975": 1040.7615508689626
      }
    },
    {
      "Score": 1032.5231265802677,
      "variance": 18.50101287386191,
      "rating_q975": 1040.535616476357,
      "rating_q025": 1024.1839552947265,
      "Votes": 5276,
      "Rank (UB)": 223,
      "Model": "stripedhyena-nous-7b",
      "95% CI": {
        "rating_q025": 1024.1839552947265,
        "rating_q975": 1040.535616476357
      }
    },
    {
      "Score": 1030.3963379574457,
      "variance": 14.80011963740863,
      "rating_q975": 1036.45417299561,
      "rating_q025": 1022.4677530077247,
      "Votes": 6503,
      "Rank (UB)": 224,
      "Model": "olmo-7b-instruct",
      "95% CI": {
        "rating_q025": 1022.4677530077247,
        "rating_q975": 1036.45417299561
      }
    },
    {
      "Score": 1022.8712426143254,
      "variance": 10.995013432182095,
      "rating_q975": 1029.8131030429872,
      "rating_q025": 1016.9065373215556,
      "Votes": 9142,
      "Rank (UB)": 226,
      "Model": "mistral-7b-instruct",
      "95% CI": {
        "rating_q025": 1016.9065373215556,
        "rating_q975": 1029.8131030429872
      }
    },
    {
      "Score": 1020.0299468628052,
      "variance": 13.085796135224406,
      "rating_q975": 1025.6598481842516,
      "rating_q025": 1012.9053047206124,
      "Votes": 7017,
      "Rank (UB)": 226,
      "Model": "vicuna-7b",
      "95% CI": {
        "rating_q025": 1012.9053047206124,
        "rating_q975": 1025.6598481842516
      }
    },
    {
      "Score": 1018.6493361474098,
      "variance": 14.279919720486161,
      "rating_q975": 1024.5826565936293,
      "rating_q025": 1011.280253343256,
      "Votes": 8713,
      "Rank (UB)": 226,
      "Model": "palm-2",
      "95% CI": {
        "rating_q025": 1011.280253343256,
        "rating_q975": 1024.5826565936293
      }
    },
    {
      "Score": 1004.7149435999094,
      "variance": 18.51411364850236,
      "rating_q975": 1012.6248558380756,
      "rating_q025": 996.7496038268898,
      "Votes": 4918,
      "Rank (UB)": 230,
      "Model": "gemma-2b-it",
      "95% CI": {
        "rating_q025": 996.7496038268898,
        "rating_q975": 1012.6248558380756
      }
    },
    {
      "Score": 1003.5151156133348,
      "variance": 11.592309485134804,
      "rating_q975": 1008.9203578047877,
      "rating_q025": 996.4324096907694,
      "Votes": 7816,
      "Rank (UB)": 231,
      "Model": "qwen1.5-4b-chat",
      "95% CI": {
        "rating_q025": 996.4324096907694,
        "rating_q975": 1008.9203578047877
      }
    },
    {
      "Score": 979.8400564118542,
      "variance": 19.185577936234985,
      "rating_q975": 987.5469211796681,
      "rating_q025": 971.4957616799276,
      "Votes": 7020,
      "Rank (UB)": 233,
      "Model": "koala-13b",
      "95% CI": {
        "rating_q025": 971.4957616799276,
        "rating_q975": 987.5469211796681
      }
    },
    {
      "Score": 970.0787026732849,
      "variance": 15.762820892686381,
      "rating_q975": 975.872601064503,
      "rating_q025": 961.2213953706205,
      "Votes": 4763,
      "Rank (UB)": 233,
      "Model": "chatglm3-6b",
      "95% CI": {
        "rating_q025": 961.2213953706205,
        "rating_q975": 975.872601064503
      }
    },
    {
      "Score": 947.555015044787,
      "variance": 50.495709418082455,
      "rating_q975": 962.0879422919406,
      "rating_q025": 936.2868783832957,
      "Votes": 1788,
      "Rank (UB)": 234,
      "Model": "gpt4all-13b-snoozy",
      "95% CI": {
        "rating_q025": 936.2868783832957,
        "rating_q975": 962.0879422919406
      }
    },
    {
      "Score": 943.4175851158776,
      "variance": 27.026520981309396,
      "rating_q975": 952.2375549513007,
      "rating_q025": 932.6264532807664,
      "Votes": 3997,
      "Rank (UB)": 235,
      "Model": "mpt-7b-chat",
      "95% CI": {
        "rating_q025": 932.6264532807664,
        "rating_q975": 952.2375549513007
      }
    },
    {
      "Score": 939.6528233469469,
      "variance": 36.047166764980474,
      "rating_q975": 952.3265485981715,
      "rating_q025": 929.122742347863,
      "Votes": 2713,
      "Rank (UB)": 235,
      "Model": "chatglm2-6b",
      "95% CI": {
        "rating_q025": 929.122742347863,
        "rating_q975": 952.3265485981715
      }
    },
    {
      "Score": 936.9021786346786,
      "variance": 18.715094471159087,
      "rating_q975": 945.2068169123743,
      "rating_q025": 929.8801365249083,
      "Votes": 4920,
      "Rank (UB)": 235,
      "Model": "RWKV-4-Raven-14B",
      "95% CI": {
        "rating_q025": 929.8801365249083,
        "rating_q975": 945.2068169123743
      }
    },
    {
      "Score": 916.7283168023907,
      "variance": 16.31422471303007,
      "rating_q975": 924.6342177958975,
      "rating_q025": 909.7354730833158,
      "Votes": 5864,
      "Rank (UB)": 239,
      "Model": "alpaca-13b",
      "95% CI": {
        "rating_q025": 909.7354730833158,
        "rating_q975": 924.6342177958975
      }
    },
    {
      "Score": 908.3406543413778,
      "variance": 18.36323948729911,
      "rating_q975": 915.9321815828625,
      "rating_q025": 900.7735900515171,
      "Votes": 6368,
      "Rank (UB)": 239,
      "Model": "oasst-pythia-12b",
      "95% CI": {
        "rating_q025": 900.7735900515171,
        "rating_q975": 915.9321815828625
      }
    },
    {
      "Score": 894.3036181309022,
      "variance": 19.21923538539161,
      "rating_q975": 903.0723842217085,
      "rating_q025": 886.2900043742327,
      "Votes": 4983,
      "Rank (UB)": 240,
      "Model": "chatglm-6b",
      "95% CI": {
        "rating_q025": 886.2900043742327,
        "rating_q975": 903.0723842217085
      }
    },
    {
      "Score": 883.0588732825056,
      "variance": 25.485961211882504,
      "rating_q975": 891.0140363160587,
      "rating_q025": 871.2170558561097,
      "Votes": 4288,
      "Rank (UB)": 241,
      "Model": "fastchat-t5-3b",
      "95% CI": {
        "rating_q025": 871.2170558561097,
        "rating_q975": 891.0140363160587
      }
    },
    {
      "Score": 855.2819951094691,
      "variance": 37.411776430389125,
      "rating_q975": 866.6575029306824,
      "rating_q025": 841.6532657260632,
      "Votes": 3336,
      "Rank (UB)": 243,
      "Model": "stablelm-tuned-alpha-7b",
      "95% CI": {
        "rating_q025": 841.6532657260632,
        "rating_q975": 866.6575029306824
      }
    },
    {
      "Score": 837.5119194674609,
      "variance": 29.077704488008028,
      "rating_q975": 848.2788812099496,
      "rating_q025": 825.8497094342399,
      "Votes": 3480,
      "Rank (UB)": 243,
      "Model": "dolly-v2-12b",
      "95% CI": {
        "rating_q025": 825.8497094342399,
        "rating_q975": 848.2788812099496
      }
    },
    {
      "Score": 814.8399052842424,
      "variance": 40.929496803279086,
      "rating_q975": 827.0270228148473,
      "rating_q025": 804.7399738381346,
      "Votes": 2446,
      "Rank (UB)": 244,
      "Model": "llama-13b",
      "95% CI": {
        "rating_q025": 804.7399738381346,
        "rating_q975": 827.0270228148473
      }
    }
  ]
}