{
  "last_updated": "2025-06-11",
  "models": [
    {
      "Score": 1478.1345191324692,
      "variance": 14.065652848182042,
      "rating_q975": 1484.5968002623595,
      "rating_q025": 1470.8763003667675,
      "Votes": 7343,
      "Rank (UB)": 1,
      "Model": "gemini-2.5-pro-preview-06-05",
      "95% CI": {
        "rating_q025": 1470.8763003667675,
        "rating_q975": 1484.5968002623595
      }
    },
    {
      "Score": 1446.0,
      "variance": 8.99509296754853,
      "rating_q975": 1451.7033309185497,
      "rating_q025": 1439.475678659957,
      "Votes": 12351,
      "Rank (UB)": 2,
      "Model": "gemini-2.5-pro-preview-05-06",
      "95% CI": {
        "rating_q025": 1439.475678659957,
        "rating_q975": 1451.7033309185497
      }
    },
    {
      "Score": 1425.0653188607723,
      "variance": 6.691421357077914,
      "rating_q975": 1429.5356947740036,
      "rating_q025": 1420.532483404634,
      "Votes": 15210,
      "Rank (UB)": 3,
      "Model": "o3-2025-04-16",
      "95% CI": {
        "rating_q025": 1420.532483404634,
        "rating_q975": 1429.5356947740036
      }
    },
    {
      "Score": 1422.6609456870476,
      "variance": 4.5461105805300415,
      "rating_q975": 1426.9298324957292,
      "rating_q025": 1418.5764338644049,
      "Votes": 19762,
      "Rank (UB)": 3,
      "Model": "chatgpt-4o-latest-20250326",
      "95% CI": {
        "rating_q025": 1418.5764338644049,
        "rating_q975": 1426.9298324957292
      }
    },
    {
      "Score": 1419.6088062364313,
      "variance": 6.781470420659903,
      "rating_q975": 1424.2918540470062,
      "rating_q025": 1414.4575402136165,
      "Votes": 12614,
      "Rank (UB)": 3,
      "Model": "gemini-2.5-flash-preview-05-20",
      "95% CI": {
        "rating_q025": 1414.4575402136165,
        "rating_q975": 1424.2918540470062
      }
    },
    {
      "Score": 1417.410466545748,
      "variance": 4.037751695993676,
      "rating_q975": 1421.3711286914668,
      "rating_q025": 1413.7334186785372,
      "Votes": 21879,
      "Rank (UB)": 3,
      "Model": "grok-3-preview-02-24",
      "95% CI": {
        "rating_q025": 1413.7334186785372,
        "rating_q975": 1421.3711286914668
      }
    },
    {
      "Score": 1411.3106120027412,
      "variance": 5.416070196565883,
      "rating_q975": 1415.1975504013967,
      "rating_q025": 1406.100152440322,
      "Votes": 15271,
      "Rank (UB)": 5,
      "Model": "gpt-4.5-preview-2025-02-27",
      "95% CI": {
        "rating_q025": 1406.100152440322,
        "rating_q975": 1415.1975504013967
      }
    },
    {
      "Score": 1395.8646953353164,
      "variance": 8.03489895749739,
      "rating_q975": 1400.441123629907,
      "rating_q025": 1390.003715641302,
      "Votes": 14148,
      "Rank (UB)": 10,
      "Model": "gemini-2.5-flash-preview-04-17",
      "95% CI": {
        "rating_q025": 1390.003715641302,
        "rating_q975": 1400.441123629907
      }
    },
    {
      "Score": 1383.5187041252607,
      "variance": 5.559093571505392,
      "rating_q975": 1387.6432621581255,
      "rating_q025": 1378.5882462243007,
      "Votes": 13830,
      "Rank (UB)": 13,
      "Model": "gpt-4.1-2025-04-14",
      "95% CI": {
        "rating_q025": 1378.5882462243007,
        "rating_q975": 1387.6432621581255
      }
    },
    {
      "Score": 1382.2705532500108,
      "variance": 4.797320063738489,
      "rating_q975": 1385.8548297440027,
      "rating_q025": 1378.2879112040741,
      "Votes": 16550,
      "Rank (UB)": 13,
      "Model": "deepseek-v3-0324",
      "95% CI": {
        "rating_q025": 1378.2879112040741,
        "rating_q975": 1385.8548297440027
      }
    },
    {
      "Score": 1372.7350020531865,
      "variance": 5.673716372357435,
      "rating_q975": 1377.1615136510352,
      "rating_q025": 1368.6451916125397,
      "Votes": 13850,
      "Rank (UB)": 17,
      "Model": "claude-opus-4-20250514",
      "95% CI": {
        "rating_q025": 1368.6451916125397,
        "rating_q975": 1377.1615136510352
      }
    },
    {
      "Score": 1372.2209269563223,
      "variance": 13.115885249642155,
      "rating_q975": 1377.9497699977396,
      "rating_q025": 1364.9278888524113,
      "Votes": 5944,
      "Rank (UB)": 17,
      "Model": "hunyuan-turbos-20250416",
      "95% CI": {
        "rating_q025": 1364.9278888524113,
        "rating_q975": 1377.9497699977396
      }
    },
    {
      "Score": 1371.4636772668305,
      "variance": 3.9633696760122996,
      "rating_q975": 1374.8357491096685,
      "rating_q025": 1367.5101175599038,
      "Votes": 19430,
      "Rank (UB)": 18,
      "Model": "deepseek-r1",
      "95% CI": {
        "rating_q025": 1367.5101175599038,
        "rating_q975": 1374.8357491096685
      }
    },
    {
      "Score": 1363.2629886646737,
      "variance": 4.1434940089401495,
      "rating_q975": 1366.8543819072893,
      "rating_q025": 1359.7748501820943,
      "Votes": 29038,
      "Rank (UB)": 22,
      "Model": "o1-2024-12-17",
      "95% CI": {
        "rating_q025": 1359.7748501820943,
        "rating_q975": 1366.8543819072893
      }
    },
    {
      "Score": 1362.9328599325834,
      "variance": 8.532096786882656,
      "rating_q975": 1368.6469590749423,
      "rating_q025": 1358.149592570019,
      "Votes": 12003,
      "Rank (UB)": 20,
      "Model": "mistral-medium-2505",
      "95% CI": {
        "rating_q025": 1358.149592570019,
        "rating_q975": 1368.6469590749423
      }
    },
    {
      "Score": 1361.7839098062175,
      "variance": 2.9555993851357716,
      "rating_q975": 1365.244993125695,
      "rating_q025": 1358.5225802133655,
      "Votes": 34240,
      "Rank (UB)": 22,
      "Model": "gemini-2.0-flash-001",
      "95% CI": {
        "rating_q025": 1358.5225802133655,
        "rating_q975": 1365.244993125695
      }
    },
    {
      "Score": 1361.2149253582415,
      "variance": 8.047396435977092,
      "rating_q975": 1366.8489715748185,
      "rating_q025": 1356.03627028959,
      "Votes": 13554,
      "Rank (UB)": 22,
      "Model": "o4-mini-2025-04-16",
      "95% CI": {
        "rating_q025": 1356.03627028959,
        "rating_q975": 1366.8489715748185
      }
    },
    {
      "Score": 1360.9923105397315,
      "variance": 13.959783072320258,
      "rating_q975": 1368.5381177415961,
      "rating_q025": 1354.5760194172829,
      "Votes": 6636,
      "Rank (UB)": 21,
      "Model": "grok-3-mini-beta",
      "95% CI": {
        "rating_q025": 1354.5760194172829,
        "rating_q975": 1368.5381177415961
      }
    },
    {
      "Score": 1360.4855173167302,
      "variance": 7.245088566234177,
      "rating_q975": 1364.5853427608356,
      "rating_q025": 1355.465116366118,
      "Votes": 10677,
      "Rank (UB)": 24,
      "Model": "qwen3-235b-a22b",
      "95% CI": {
        "rating_q025": 1355.465116366118,
        "rating_q975": 1364.5853427608356
      }
    },
    {
      "Score": 1358.2222457176872,
      "variance": 2.552998505361272,
      "rating_q975": 1361.3132991416492,
      "rating_q025": 1354.875000476928,
      "Votes": 29484,
      "Rank (UB)": 24,
      "Model": "qwen2.5-max",
      "95% CI": {
        "rating_q025": 1354.875000476928,
        "rating_q975": 1361.3132991416492
      }
    },
    {
      "Score": 1355.117289410704,
      "variance": 5.139112294576065,
      "rating_q975": 1358.7269182022392,
      "rating_q025": 1349.883527186425,
      "Votes": 20295,
      "Rank (UB)": 25,
      "Model": "gemma-3-27b-it",
      "95% CI": {
        "rating_q025": 1349.883527186425,
        "rating_q975": 1358.7269182022392
      }
    },
    {
      "Score": 1348.4532188149044,
      "variance": 2.0881833724113266,
      "rating_q975": 1351.4500402821388,
      "rating_q025": 1346.0488836703146,
      "Votes": 33177,
      "Rank (UB)": 32,
      "Model": "o1-preview",
      "95% CI": {
        "rating_q025": 1346.0488836703146,
        "rating_q975": 1351.4500402821388
      }
    },
    {
      "Score": 1345.0654072692948,
      "variance": 8.297091921691058,
      "rating_q975": 1350.076568386603,
      "rating_q025": 1339.3918239561544,
      "Votes": 10740,
      "Rank (UB)": 32,
      "Model": "claude-sonnet-4-20250514",
      "95% CI": {
        "rating_q025": 1339.3918239561544,
        "rating_q975": 1350.076568386603
      }
    },
    {
      "Score": 1337.9898932031556,
      "variance": 5.473402373780345,
      "rating_q975": 1341.640811362432,
      "rating_q025": 1332.3814075851262,
      "Votes": 19404,
      "Rank (UB)": 35,
      "Model": "o3-mini-high",
      "95% CI": {
        "rating_q025": 1332.3814075851262,
        "rating_q975": 1341.640811362432
      }
    },
    {
      "Score": 1335.8455086057547,
      "variance": 6.4904528549535785,
      "rating_q975": 1340.5153622016144,
      "rating_q025": 1331.0168526314505,
      "Votes": 12702,
      "Rank (UB)": 35,
      "Model": "gpt-4.1-mini-2025-04-14",
      "95% CI": {
        "rating_q025": 1331.0168526314505,
        "rating_q975": 1340.5153622016144
      }
    },
    {
      "Score": 1334.0590225856595,
      "variance": 18.688185682574414,
      "rating_q975": 1341.1996620131374,
      "rating_q025": 1326.3204945828434,
      "Votes": 3976,
      "Rank (UB)": 35,
      "Model": "gemma-3-12b-it",
      "95% CI": {
        "rating_q025": 1326.3204945828434,
        "rating_q975": 1341.1996620131374
      }
    },
    {
      "Score": 1331.9928338981385,
      "variance": 3.570948366404868,
      "rating_q975": 1335.3057029478857,
      "rating_q025": 1328.3423311721826,
      "Votes": 22841,
      "Rank (UB)": 36,
      "Model": "deepseek-v3",
      "95% CI": {
        "rating_q025": 1328.3423311721826,
        "rating_q975": 1335.3057029478857
      }
    },
    {
      "Score": 1330.089700277889,
      "variance": 34.37255706610046,
      "rating_q975": 1340.8464241400989,
      "rating_q025": 1319.1979342850602,
      "Votes": 2595,
      "Rank (UB)": 35,
      "Model": "amazon-nova-experimental-chat-05-14",
      "95% CI": {
        "rating_q025": 1319.1979342850602,
        "rating_q975": 1340.8464241400989
      }
    },
    {
      "Score": 1329.6769520265732,
      "variance": 5.705402218359357,
      "rating_q975": 1334.248936287783,
      "rating_q025": 1324.9712738306275,
      "Votes": 15930,
      "Rank (UB)": 36,
      "Model": "qwq-32b",
      "95% CI": {
        "rating_q025": 1324.9712738306275,
        "rating_q975": 1334.248936287783
      }
    },
    {
      "Score": 1326.2671438774962,
      "variance": 3.193041583727734,
      "rating_q975": 1329.1889625833862,
      "rating_q025": 1322.5363708567313,
      "Votes": 26104,
      "Rank (UB)": 38,
      "Model": "gemini-2.0-flash-lite-preview-02-05",
      "95% CI": {
        "rating_q025": 1322.5363708567313,
        "rating_q975": 1329.1889625833862
      }
    },
    {
      "Score": 1324.478945378534,
      "variance": 14.306830252590942,
      "rating_q975": 1330.800550142067,
      "rating_q025": 1317.069439602655,
      "Votes": 6028,
      "Rank (UB)": 38,
      "Model": "glm-4-plus-0111",
      "95% CI": {
        "rating_q025": 1317.069439602655,
        "rating_q975": 1330.800550142067
      }
    },
    {
      "Score": 1323.9119929302544,
      "variance": 14.690196044049994,
      "rating_q975": 1332.19480090925,
      "rating_q025": 1317.100732155427,
      "Votes": 6055,
      "Rank (UB)": 37,
      "Model": "qwen-plus-0125",
      "95% CI": {
        "rating_q025": 1317.100732155427,
        "rating_q975": 1332.19480090925
      }
    },
    {
      "Score": 1323.237081907735,
      "variance": 4.342727176366378,
      "rating_q975": 1327.544076077156,
      "rating_q025": 1319.9639641127308,
      "Votes": 20084,
      "Rank (UB)": 39,
      "Model": "command-a-03-2025",
      "95% CI": {
        "rating_q025": 1319.9639641127308,
        "rating_q975": 1327.544076077156
      }
    },
    {
      "Score": 1319.2283292840345,
      "variance": 2.383945330944615,
      "rating_q975": 1322.0377849984625,
      "rating_q025": 1316.2546911977472,
      "Votes": 32421,
      "Rank (UB)": 43,
      "Model": "o3-mini",
      "95% CI": {
        "rating_q025": 1316.2546911977472,
        "rating_q975": 1322.0377849984625
      }
    },
    {
      "Score": 1318.2867461407786,
      "variance": 14.04278004415792,
      "rating_q975": 1325.010608339852,
      "rating_q025": 1311.7908762754516,
      "Votes": 5126,
      "Rank (UB)": 41,
      "Model": "step-2-16k-exp-202412",
      "95% CI": {
        "rating_q025": 1311.7908762754516,
        "rating_q975": 1325.010608339852
      }
    },
    {
      "Score": 1317.4152079332125,
      "variance": 1.5947607527337493,
      "rating_q975": 1319.8967175815221,
      "rating_q025": 1315.0752097829218,
      "Votes": 54951,
      "Rank (UB)": 44,
      "Model": "o1-mini",
      "95% CI": {
        "rating_q025": 1315.0752097829218,
        "rating_q975": 1319.8967175815221
      }
    },
    {
      "Score": 1315.939296566249,
      "variance": 22.66146078328801,
      "rating_q975": 1326.3945130316429,
      "rating_q025": 1308.1256480853685,
      "Votes": 2452,
      "Rank (UB)": 40,
      "Model": "hunyuan-turbos-20250226",
      "95% CI": {
        "rating_q025": 1308.1256480853685,
        "rating_q975": 1326.3945130316429
      }
    },
    {
      "Score": 1315.663465529717,
      "variance": 1.3814857625796937,
      "rating_q975": 1317.9609739721454,
      "rating_q025": 1313.5059878964125,
      "Votes": 58645,
      "Rank (UB)": 45,
      "Model": "gemini-1.5-pro-002",
      "95% CI": {
        "rating_q025": 1313.5059878964125,
        "rating_q975": 1317.9609739721454
      }
    },
    {
      "Score": 1313.2634084015228,
      "variance": 4.564501732142743,
      "rating_q975": 1317.196905061625,
      "rating_q025": 1309.1585684321658,
      "Votes": 21310,
      "Rank (UB)": 45,
      "Model": "claude-3-7-sonnet-20250219-thinking-32k",
      "95% CI": {
        "rating_q025": 1309.1585684321658,
        "rating_q975": 1317.196905061625
      }
    },
    {
      "Score": 1309.8895130126627,
      "variance": 27.76998817645259,
      "rating_q975": 1321.1911238072087,
      "rating_q025": 1301.6331470544535,
      "Votes": 2371,
      "Rank (UB)": 43,
      "Model": "llama-3.3-nemotron-49b-super-v1",
      "95% CI": {
        "rating_q025": 1301.6331470544535,
        "rating_q975": 1321.1911238072087
      }
    },
    {
      "Score": 1309.6590114494215,
      "variance": 31.061608203502555,
      "rating_q975": 1320.358035710821,
      "rating_q025": 1298.65724441903,
      "Votes": 2510,
      "Rank (UB)": 43,
      "Model": "hunyuan-turbo-0110",
      "95% CI": {
        "rating_q025": 1298.65724441903,
        "rating_q975": 1320.358035710821
      }
    },
    {
      "Score": 1305.572676592928,
      "variance": 3.172571308487028,
      "rating_q975": 1308.6417486796533,
      "rating_q025": 1302.3435737199434,
      "Votes": 25983,
      "Rank (UB)": 53,
      "Model": "claude-3-7-sonnet-20250219",
      "95% CI": {
        "rating_q025": 1302.3435737199434,
        "rating_q975": 1308.6417486796533
      }
    },
    {
      "Score": 1301.4113515181161,
      "variance": 21.147736687181094,
      "rating_q975": 1309.4134275970312,
      "rating_q025": 1293.1070187697203,
      "Votes": 3913,
      "Rank (UB)": 52,
      "Model": "gemma-3n-e4b-it",
      "95% CI": {
        "rating_q025": 1293.1070187697203,
        "rating_q975": 1309.4134275970312
      }
    },
    {
      "Score": 1301.3499638542241,
      "variance": 1.1535288592529238,
      "rating_q975": 1303.6850379460072,
      "rating_q025": 1299.697988624594,
      "Votes": 67084,
      "Rank (UB)": 55,
      "Model": "grok-2-2024-08-13",
      "95% CI": {
        "rating_q025": 1299.697988624594,
        "rating_q975": 1303.6850379460072
      }
    },
    {
      "Score": 1300.5645611418836,
      "variance": 2.6371635621290057,
      "rating_q975": 1304.1580907982886,
      "rating_q025": 1297.713774317729,
      "Votes": 28968,
      "Rank (UB)": 55,
      "Model": "yi-lightning",
      "95% CI": {
        "rating_q025": 1297.713774317729,
        "rating_q975": 1304.1580907982886
      }
    },
    {
      "Score": 1298.496268794927,
      "variance": 0.8243747536743061,
      "rating_q975": 1300.0100698267445,
      "rating_q025": 1296.5654054511522,
      "Votes": 117747,
      "Rank (UB)": 57,
      "Model": "gpt-4o-2024-05-13",
      "95% CI": {
        "rating_q025": 1296.5654054511522,
        "rating_q975": 1300.0100698267445
      }
    },
    {
      "Score": 1297.1628283058317,
      "variance": 1.4096023096341017,
      "rating_q975": 1299.3544580570858,
      "rating_q025": 1294.7993605633756,
      "Votes": 73327,
      "Rank (UB)": 58,
      "Model": "claude-3-5-sonnet-20241022",
      "95% CI": {
        "rating_q025": 1294.7993605633756,
        "rating_q975": 1299.3544580570858
      }
    },
    {
      "Score": 1295.6743314167027,
      "variance": 6.700430051958716,
      "rating_q975": 1300.2623581801272,
      "rating_q025": 1289.5950850938602,
      "Votes": 10715,
      "Rank (UB)": 57,
      "Model": "qwen2.5-plus-1127",
      "95% CI": {
        "rating_q025": 1289.5950850938602,
        "rating_q975": 1300.2623581801272
      }
    },
    {
      "Score": 1292.5390223962293,
      "variance": 10.22233077097146,
      "rating_q975": 1299.1682788113446,
      "rating_q025": 1287.6820371871047,
      "Votes": 7243,
      "Rank (UB)": 58,
      "Model": "deepseek-v2.5-1210",
      "95% CI": {
        "rating_q025": 1287.6820371871047,
        "rating_q975": 1299.1682788113446
      }
    },
    {
      "Score": 1288.9100377007906,
      "variance": 3.0139820850823806,
      "rating_q975": 1292.0521788936755,
      "rating_q025": 1285.8683805248868,
      "Votes": 26074,
      "Rank (UB)": 63,
      "Model": "athene-v2-chat",
      "95% CI": {
        "rating_q025": 1285.8683805248868,
        "rating_q975": 1292.0521788936755
      }
    },
    {
      "Score": 1288.543692387376,
      "variance": 25.73816153555565,
      "rating_q975": 1296.29469702029,
      "rating_q025": 1278.289476849355,
      "Votes": 4321,
      "Rank (UB)": 61,
      "Model": "gemma-3-4b-it",
      "95% CI": {
        "rating_q025": 1278.289476849355,
        "rating_q975": 1296.29469702029
      }
    },
    {
      "Score": 1287.4630292412442,
      "variance": 2.707724117717083,
      "rating_q975": 1290.994935305578,
      "rating_q025": 1284.349188145096,
      "Votes": 27788,
      "Rank (UB)": 63,
      "Model": "glm-4-plus",
      "95% CI": {
        "rating_q025": 1284.349188145096,
        "rating_q975": 1290.994935305578
      }
    },
    {
      "Score": 1286.7610438306544,
      "variance": 6.803294627508965,
      "rating_q975": 1291.207459236497,
      "rating_q025": 1281.6436660705572,
      "Votes": 13750,
      "Rank (UB)": 63,
      "Model": "llama-4-maverick-17b-128e-instruct",
      "95% CI": {
        "rating_q025": 1281.6436660705572,
        "rating_q975": 1291.207459236497
      }
    },
    {
      "Score": 1285.3934778416628,
      "variance": 18.1462802443717,
      "rating_q975": 1294.5622027283475,
      "rating_q025": 1278.2039582611026,
      "Votes": 3856,
      "Rank (UB)": 62,
      "Model": "hunyuan-large-2025-02-10",
      "95% CI": {
        "rating_q025": 1278.2039582611026,
        "rating_q975": 1294.5622027283475
      }
    },
    {
      "Score": 1285.1955856858922,
      "variance": 1.0741594511880004,
      "rating_q975": 1287.7011270758785,
      "rating_q025": 1283.348755721266,
      "Votes": 72536,
      "Rank (UB)": 64,
      "Model": "gpt-4o-mini-2024-07-18",
      "95% CI": {
        "rating_q025": 1283.348755721266,
        "rating_q975": 1287.7011270758785
      }
    },
    {
      "Score": 1284.9878122971566,
      "variance": 2.005851531445181,
      "rating_q975": 1287.9919265601472,
      "rating_q025": 1282.355358384149,
      "Votes": 37021,
      "Rank (UB)": 64,
      "Model": "gemini-1.5-flash-002",
      "95% CI": {
        "rating_q025": 1282.355358384149,
        "rating_q975": 1287.9919265601472
      }
    },
    {
      "Score": 1284.3167251772886,
      "variance": 16.22907900269128,
      "rating_q975": 1292.3045349486476,
      "rating_q025": 1277.4323077535175,
      "Votes": 6302,
      "Rank (UB)": 63,
      "Model": "gpt-4.1-nano-2025-04-14",
      "95% CI": {
        "rating_q025": 1277.4323077535175,
        "rating_q975": 1292.3045349486476
      }
    },
    {
      "Score": 1282.4736527074529,
      "variance": 1.9113111979952337,
      "rating_q975": 1284.8682755520797,
      "rating_q025": 1279.6773111406478,
      "Votes": 43788,
      "Rank (UB)": 66,
      "Model": "llama-3.1-405b-instruct-bf16",
      "95% CI": {
        "rating_q025": 1279.6773111406478,
        "rating_q975": 1284.8682755520797
      }
    },
    {
      "Score": 1282.1253892678208,
      "variance": 10.521945193510552,
      "rating_q975": 1287.743402869146,
      "rating_q025": 1276.071312463146,
      "Votes": 7577,
      "Rank (UB)": 64,
      "Model": "llama-3.1-nemotron-70b-instruct",
      "95% CI": {
        "rating_q025": 1276.071312463146,
        "rating_q975": 1287.743402869146
      }
    },
    {
      "Score": 1281.73639462323,
      "variance": 1.2515885225927663,
      "rating_q975": 1283.565510780763,
      "rating_q025": 1279.333828391839,
      "Votes": 86159,
      "Rank (UB)": 67,
      "Model": "claude-3-5-sonnet-20240620",
      "95% CI": {
        "rating_q025": 1279.333828391839,
        "rating_q975": 1283.565510780763
      }
    },
    {
      "Score": 1281.0149096812556,
      "variance": 1.1592372665453314,
      "rating_q975": 1282.9742742186254,
      "rating_q025": 1278.9861222954414,
      "Votes": 63038,
      "Rank (UB)": 68,
      "Model": "llama-3.1-405b-instruct-fp8",
      "95% CI": {
        "rating_q025": 1278.9861222954414,
        "rating_q975": 1282.9742742186254
      }
    },
    {
      "Score": 1280.0599257781923,
      "variance": 1.8238750815944564,
      "rating_q975": 1283.2107412679009,
      "rating_q025": 1278.3323338317473,
      "Votes": 52144,
      "Rank (UB)": 68,
      "Model": "gemini-advanced-0514",
      "95% CI": {
        "rating_q025": 1278.3323338317473,
        "rating_q975": 1283.2107412679009
      }
    },
    {
      "Score": 1279.6983153605415,
      "variance": 1.2918644264082915,
      "rating_q975": 1281.7067857305915,
      "rating_q025": 1276.7403781308826,
      "Votes": 55442,
      "Rank (UB)": 69,
      "Model": "grok-2-mini-2024-08-13",
      "95% CI": {
        "rating_q025": 1276.7403781308826,
        "rating_q975": 1281.7067857305915
      }
    },
    {
      "Score": 1278.7627464301008,
      "variance": 2.277464703167606,
      "rating_q975": 1281.5035698479774,
      "rating_q025": 1276.1555012688991,
      "Votes": 47973,
      "Rank (UB)": 70,
      "Model": "gpt-4o-2024-08-06",
      "95% CI": {
        "rating_q025": 1276.1555012688991,
        "rating_q975": 1281.5035698479774
      }
    },
    {
      "Score": 1276.866259303823,
      "variance": 4.310232011347979,
      "rating_q975": 1280.4222535340664,
      "rating_q025": 1272.4690573029725,
      "Votes": 17432,
      "Rank (UB)": 70,
      "Model": "qwen-max-0919",
      "95% CI": {
        "rating_q025": 1272.4690573029725,
        "rating_q975": 1280.4222535340664
      }
    },
    {
      "Score": 1274.2557171733156,
      "variance": 23.61240822645606,
      "rating_q975": 1284.2129408660205,
      "rating_q025": 1265.1159481396405,
      "Votes": 4014,
      "Rank (UB)": 67,
      "Model": "hunyuan-standard-2025-02-10",
      "95% CI": {
        "rating_q025": 1265.1159481396405,
        "rating_q975": 1284.2129408660205
      }
    },
    {
      "Score": 1273.3906588777181,
      "variance": 1.1067589471230959,
      "rating_q975": 1275.3753170333819,
      "rating_q025": 1271.562240216844,
      "Votes": 82435,
      "Rank (UB)": 81,
      "Model": "gemini-1.5-pro-001",
      "95% CI": {
        "rating_q025": 1271.562240216844,
        "rating_q975": 1275.3753170333819
      }
    },
    {
      "Score": 1271.6005121988082,
      "variance": 2.852631778711338,
      "rating_q975": 1273.7933671461426,
      "rating_q025": 1267.0850627414761,
      "Votes": 26344,
      "Rank (UB)": 82,
      "Model": "deepseek-v2.5",
      "95% CI": {
        "rating_q025": 1267.0850627414761,
        "rating_q975": 1273.7933671461426
      }
    },
    {
      "Score": 1270.596261252831,
      "variance": 1.6699529174046377,
      "rating_q975": 1272.5039052092238,
      "rating_q025": 1267.470183383796,
      "Votes": 41519,
      "Rank (UB)": 82,
      "Model": "qwen2.5-72b-instruct",
      "95% CI": {
        "rating_q025": 1267.470183383796,
        "rating_q975": 1272.5039052092238
      }
    },
    {
      "Score": 1270.4378930157525,
      "variance": 2.2003169924829136,
      "rating_q975": 1273.0677654422293,
      "rating_q025": 1267.533724298421,
      "Votes": 44800,
      "Rank (UB)": 82,
      "Model": "llama-3.3-70b-instruct",
      "95% CI": {
        "rating_q025": 1267.533724298421,
        "rating_q975": 1273.0677654422293
      }
    },
    {
      "Score": 1269.9544440807942,
      "variance": 1.141717511141938,
      "rating_q975": 1271.8465544160674,
      "rating_q025": 1267.8967146538716,
      "Votes": 102133,
      "Rank (UB)": 83,
      "Model": "gpt-4-turbo-2024-04-09",
      "95% CI": {
        "rating_q025": 1267.8967146538716,
        "rating_q975": 1271.8465544160674
      }
    },
    {
      "Score": 1265.081866346748,
      "variance": 1.6763369473109366,
      "rating_q975": 1267.4967921753175,
      "rating_q025": 1262.5022735621983,
      "Votes": 48217,
      "Rank (UB)": 87,
      "Model": "mistral-large-2407",
      "95% CI": {
        "rating_q025": 1262.5022735621983,
        "rating_q975": 1267.4967921753175
      }
    },
    {
      "Score": 1263.8860457096166,
      "variance": 2.909308761004214,
      "rating_q975": 1267.5139447528952,
      "rating_q025": 1261.3611602621581,
      "Votes": 20580,
      "Rank (UB)": 87,
      "Model": "athene-70b-0725",
      "95% CI": {
        "rating_q025": 1261.3611602621581,
        "rating_q975": 1267.5139447528952
      }
    },
    {
      "Score": 1263.4738246566712,
      "variance": 1.0057453860834118,
      "rating_q975": 1264.9374728302191,
      "rating_q025": 1261.113061102758,
      "Votes": 103748,
      "Rank (UB)": 90,
      "Model": "gpt-4-1106-preview",
      "95% CI": {
        "rating_q025": 1261.113061102758,
        "rating_q975": 1264.9374728302191
      }
    },
    {
      "Score": 1262.8604125277857,
      "variance": 29.91196779320816,
      "rating_q975": 1272.752355689078,
      "rating_q025": 1251.7741998756476,
      "Votes": 2484,
      "Rank (UB)": 82,
      "Model": "mistral-small-3.1-24b-instruct-2503",
      "95% CI": {
        "rating_q025": 1251.7741998756476,
        "rating_q975": 1272.752355689078
      }
    },
    {
      "Score": 1262.4512092987516,
      "variance": 1.8315505712261206,
      "rating_q975": 1264.530987192663,
      "rating_q025": 1260.0309903647717,
      "Votes": 29633,
      "Rank (UB)": 90,
      "Model": "mistral-large-2411",
      "95% CI": {
        "rating_q025": 1260.0309903647717,
        "rating_q975": 1264.530987192663
      }
    },
    {
      "Score": 1261.2389890584723,
      "variance": 1.6134586264490303,
      "rating_q975": 1263.8474156041702,
      "rating_q025": 1259.1080418380684,
      "Votes": 58637,
      "Rank (UB)": 90,
      "Model": "llama-3.1-70b-instruct",
      "95% CI": {
        "rating_q025": 1259.1080418380684,
        "rating_q975": 1263.8474156041702
      }
    },
    {
      "Score": 1260.795798650288,
      "variance": 0.6134833354009225,
      "rating_q975": 1262.1373652014843,
      "rating_q025": 1259.2214884117777,
      "Votes": 202641,
      "Rank (UB)": 91,
      "Model": "claude-3-opus-20240229",
      "95% CI": {
        "rating_q025": 1259.2214884117777,
        "rating_q975": 1262.1373652014843
      }
    },
    {
      "Score": 1258.4789857057767,
      "variance": 2.6922220111957595,
      "rating_q975": 1261.3505425808346,
      "rating_q025": 1254.955219588808,
      "Votes": 26371,
      "Rank (UB)": 92,
      "Model": "amazon-nova-pro-v1.0",
      "95% CI": {
        "rating_q025": 1254.955219588808,
        "rating_q975": 1261.3505425808346
      }
    },
    {
      "Score": 1258.4000380995312,
      "variance": 1.2541763100680423,
      "rating_q975": 1260.6846817555927,
      "rating_q025": 1256.4738241804532,
      "Votes": 97079,
      "Rank (UB)": 93,
      "Model": "gpt-4-0125-preview",
      "95% CI": {
        "rating_q025": 1256.4738241804532,
        "rating_q975": 1260.6846817555927
      }
    },
    {
      "Score": 1257.7708669608194,
      "variance": 19.911290348931992,
      "rating_q975": 1265.836098362675,
      "rating_q025": 1249.2889564310228,
      "Votes": 3010,
      "Rank (UB)": 89,
      "Model": "llama-3.1-tulu-3-70b",
      "95% CI": {
        "rating_q025": 1249.2889564310228,
        "rating_q975": 1265.836098362675
      }
    },
    {
      "Score": 1251.387247651871,
      "variance": 2.3405894284481614,
      "rating_q975": 1254.3054232392312,
      "rating_q025": 1248.6361749600649,
      "Votes": 44893,
      "Rank (UB)": 98,
      "Model": "claude-3-5-haiku-20241022",
      "95% CI": {
        "rating_q025": 1248.6361749600649,
        "rating_q975": 1254.3054232392312
      }
    },
    {
      "Score": 1248.6140543139068,
      "variance": 8.585725989524436,
      "rating_q975": 1254.5679854100629,
      "rating_q025": 1243.5940761030445,
      "Votes": 7948,
      "Rank (UB)": 98,
      "Model": "reka-core-20240904",
      "95% CI": {
        "rating_q025": 1243.5940761030445,
        "rating_q975": 1254.5679854100629
      }
    },
    {
      "Score": 1240.3893975045455,
      "variance": 1.7041584820509041,
      "rating_q975": 1242.7982513793054,
      "rating_q025": 1238.218127034455,
      "Votes": 65661,
      "Rank (UB)": 103,
      "Model": "gemini-1.5-flash-001",
      "95% CI": {
        "rating_q025": 1238.218127034455,
        "rating_q975": 1242.7982513793054
      }
    },
    {
      "Score": 1235.1699122234104,
      "variance": 8.411701514241368,
      "rating_q975": 1241.2062759839882,
      "rating_q025": 1230.591451529794,
      "Votes": 9125,
      "Rank (UB)": 103,
      "Model": "jamba-1.5-large",
      "95% CI": {
        "rating_q025": 1230.591451529794,
        "rating_q975": 1241.2062759839882
      }
    },
    {
      "Score": 1233.3469830564604,
      "variance": 1.075532718151504,
      "rating_q975": 1235.2796180953076,
      "rating_q025": 1231.5263200656368,
      "Votes": 79538,
      "Rank (UB)": 106,
      "Model": "gemma-2-27b-it",
      "95% CI": {
        "rating_q025": 1231.5263200656368,
        "rating_q975": 1235.2796180953076
      }
    },
    {
      "Score": 1230.8487463851275,
      "variance": 5.578165201909234,
      "rating_q975": 1235.4753892172048,
      "rating_q025": 1226.7485516418133,
      "Votes": 15321,
      "Rank (UB)": 106,
      "Model": "mistral-small-24b-instruct-2501",
      "95% CI": {
        "rating_q025": 1226.7485516418133,
        "rating_q975": 1235.4753892172048
      }
    },
    {
      "Score": 1230.8229409936873,
      "variance": 10.820509325927882,
      "rating_q975": 1238.2598651817154,
      "rating_q025": 1224.5066413773002,
      "Votes": 5730,
      "Rank (UB)": 104,
      "Model": "qwen2.5-coder-32b-instruct",
      "95% CI": {
        "rating_q025": 1224.5066413773002,
        "rating_q975": 1238.2598651817154
      }
    },
    {
      "Score": 1230.378608900985,
      "variance": 3.966803690058034,
      "rating_q975": 1233.7812831029908,
      "rating_q025": 1226.5801167444074,
      "Votes": 20646,
      "Rank (UB)": 106,
      "Model": "amazon-nova-lite-v1.0",
      "95% CI": {
        "rating_q025": 1226.5801167444074,
        "rating_q975": 1233.7812831029908
      }
    },
    {
      "Score": 1229.6259781864555,
      "variance": 7.809894908883998,
      "rating_q975": 1235.5048519903355,
      "rating_q025": 1224.6989247071792,
      "Votes": 10548,
      "Rank (UB)": 106,
      "Model": "gemma-2-9b-it-simpo",
      "95% CI": {
        "rating_q025": 1224.6989247071792,
        "rating_q975": 1235.5048519903355
      }
    },
    {
      "Score": 1228.8222377312568,
      "variance": 6.916202630616035,
      "rating_q975": 1233.1640541887784,
      "rating_q025": 1223.2281150579513,
      "Votes": 10535,
      "Rank (UB)": 106,
      "Model": "command-r-plus-08-2024",
      "95% CI": {
        "rating_q025": 1223.2281150579513,
        "rating_q975": 1233.1640541887784
      }
    },
    {
      "Score": 1226.1080633261126,
      "variance": 2.4107197354065755,
      "rating_q975": 1229.0748263939759,
      "rating_q025": 1223.6658815475828,
      "Votes": 37697,
      "Rank (UB)": 109,
      "Model": "gemini-1.5-flash-8b-001",
      "95% CI": {
        "rating_q025": 1223.6658815475828,
        "rating_q975": 1229.0748263939759
      }
    },
    {
      "Score": 1225.3404263596542,
      "variance": 18.95925941515817,
      "rating_q975": 1233.7526883085523,
      "rating_q025": 1218.2603640732943,
      "Votes": 3889,
      "Rank (UB)": 106,
      "Model": "llama-3.1-nemotron-51b-instruct",
      "95% CI": {
        "rating_q025": 1218.2603640732943,
        "rating_q975": 1233.7526883085523
      }
    },
    {
      "Score": 1222.7533707957773,
      "variance": 2.4164798648006833,
      "rating_q975": 1225.6510196208887,
      "rating_q025": 1219.9361171047137,
      "Votes": 28768,
      "Rank (UB)": 111,
      "Model": "c4ai-aya-expanse-32b",
      "95% CI": {
        "rating_q025": 1219.9361171047137,
        "rating_q975": 1225.6510196208887
      }
    },
    {
      "Score": 1222.6189266988745,
      "variance": 4.717652248285628,
      "rating_q975": 1226.4316191171442,
      "rating_q025": 1218.0728481634521,
      "Votes": 20608,
      "Rank (UB)": 111,
      "Model": "nemotron-4-340b-instruct",
      "95% CI": {
        "rating_q025": 1218.0728481634521,
        "rating_q975": 1226.4316191171442
      }
    },
    {
      "Score": 1220.1487845195852,
      "variance": 7.994404892920762,
      "rating_q975": 1224.9514922432975,
      "rating_q025": 1214.2958626395166,
      "Votes": 10221,
      "Rank (UB)": 111,
      "Model": "glm-4-0520",
      "95% CI": {
        "rating_q025": 1214.2958626395166,
        "rating_q975": 1224.9514922432975
      }
    },
    {
      "Score": 1220.057528010058,
      "variance": 0.933206834152461,
      "rating_q975": 1221.9345792112247,
      "rating_q025": 1218.1319620903105,
      "Votes": 163629,
      "Rank (UB)": 117,
      "Model": "llama-3-70b-instruct",
      "95% CI": {
        "rating_q025": 1218.1319620903105,
        "rating_q975": 1221.9345792112247
      }
    },
    {
      "Score": 1219.1199992825,
      "variance": 26.060181675830883,
      "rating_q975": 1228.435016605614,
      "rating_q025": 1209.103366081535,
      "Votes": 3460,
      "Rank (UB)": 109,
      "Model": "olmo-2-0325-32b-instruct",
      "95% CI": {
        "rating_q025": 1209.103366081535,
        "rating_q975": 1228.435016605614
      }
    },
    {
      "Score": 1219.0517387800785,
      "variance": 8.85031837579344,
      "rating_q975": 1224.9514740173786,
      "rating_q025": 1213.2982674615514,
      "Votes": 8132,
      "Rank (UB)": 111,
      "Model": "reka-flash-20240904",
      "95% CI": {
        "rating_q025": 1213.2982674615514,
        "rating_q975": 1224.9514740173786
      }
    },
    {
      "Score": 1218.8716267426155,
      "variance": 2.2290984975133523,
      "rating_q975": 1222.3365144550892,
      "rating_q025": 1216.2836648410835,
      "Votes": 25213,
      "Rank (UB)": 115,
      "Model": "phi-4",
      "95% CI": {
        "rating_q025": 1216.2836648410835,
        "rating_q975": 1222.3365144550892
      }
    },
    {
      "Score": 1214.4953147703536,
      "variance": 1.0605097205829763,
      "rating_q975": 1216.6597871548936,
      "rating_q025": 1212.7864181164905,
      "Votes": 113067,
      "Rank (UB)": 121,
      "Model": "claude-3-sonnet-20240229",
      "95% CI": {
        "rating_q025": 1212.7864181164905,
        "rating_q975": 1216.6597871548936
      }
    },
    {
      "Score": 1211.488594455956,
      "variance": 3.75318056089275,
      "rating_q975": 1215.4580416929223,
      "rating_q025": 1208.2682910511078,
      "Votes": 20654,
      "Rank (UB)": 123,
      "Model": "amazon-nova-micro-v1.0",
      "95% CI": {
        "rating_q025": 1208.2682910511078,
        "rating_q975": 1215.4580416929223
      }
    },
    {
      "Score": 1205.5414388148033,
      "variance": 1.4697094577456986,
      "rating_q975": 1208.2090277762584,
      "rating_q025": 1203.2391325418378,
      "Votes": 57197,
      "Rank (UB)": 131,
      "Model": "gemma-2-9b-it",
      "95% CI": {
        "rating_q025": 1203.2391325418378,
        "rating_q975": 1208.2090277762584
      }
    },
    {
      "Score": 1203.4388306205637,
      "variance": 1.3952699235590458,
      "rating_q975": 1205.8069937017026,
      "rating_q025": 1201.311315863114,
      "Votes": 80846,
      "Rank (UB)": 131,
      "Model": "command-r-plus",
      "95% CI": {
        "rating_q025": 1201.311315863114,
        "rating_q975": 1205.8069937017026
      }
    },
    {
      "Score": 1202.451726332479,
      "variance": 29.590998662877045,
      "rating_q975": 1214.036084066404,
      "rating_q025": 1191.5984031382743,
      "Votes": 2901,
      "Rank (UB)": 125,
      "Model": "hunyuan-standard-256k",
      "95% CI": {
        "rating_q025": 1191.5984031382743,
        "rating_q975": 1214.036084066404
      }
    },
    {
      "Score": 1200.586785807016,
      "variance": 1.8597356409233514,
      "rating_q975": 1203.0906012464222,
      "rating_q025": 1197.6800872413926,
      "Votes": 38872,
      "Rank (UB)": 132,
      "Model": "qwen2-72b-instruct",
      "95% CI": {
        "rating_q025": 1197.6800872413926,
        "rating_q975": 1203.0906012464222
      }
    },
    {
      "Score": 1199.657953791775,
      "variance": 2.0188812946168557,
      "rating_q975": 1202.4049281546943,
      "rating_q025": 1196.936378140123,
      "Votes": 55962,
      "Rank (UB)": 132,
      "Model": "gpt-4-0314",
      "95% CI": {
        "rating_q025": 1196.936378140123,
        "rating_q975": 1202.4049281546943
      }
    },
    {
      "Score": 1198.854160394954,
      "variance": 23.973665296180492,
      "rating_q975": 1207.892096148898,
      "rating_q025": 1189.8191504676893,
      "Votes": 3074,
      "Rank (UB)": 131,
      "Model": "llama-3.1-tulu-3-8b",
      "95% CI": {
        "rating_q025": 1189.8191504676893,
        "rating_q975": 1207.892096148898
      }
    },
    {
      "Score": 1195.719983633967,
      "variance": 13.92815064449691,
      "rating_q975": 1202.2391365198173,
      "rating_q025": 1188.359376232139,
      "Votes": 5111,
      "Rank (UB)": 132,
      "Model": "ministral-8b-2410",
      "95% CI": {
        "rating_q025": 1188.359376232139,
        "rating_q975": 1202.2391365198173
      }
    },
    {
      "Score": 1193.492739525937,
      "variance": 8.211887074889274,
      "rating_q975": 1198.699872845548,
      "rating_q025": 1188.9053085086784,
      "Votes": 10391,
      "Rank (UB)": 133,
      "Model": "c4ai-aya-expanse-8b",
      "95% CI": {
        "rating_q025": 1188.9053085086784,
        "rating_q975": 1198.699872845548
      }
    },
    {
      "Score": 1193.1604979076174,
      "variance": 8.585413175005675,
      "rating_q975": 1200.603356390595,
      "rating_q025": 1188.7086455800502,
      "Votes": 10851,
      "Rank (UB)": 133,
      "Model": "command-r-08-2024",
      "95% CI": {
        "rating_q025": 1188.7086455800502,
        "rating_q975": 1200.603356390595
      }
    },
    {
      "Score": 1192.8035240165602,
      "variance": 0.9876951535192385,
      "rating_q975": 1194.8548086659653,
      "rating_q025": 1190.9133940218396,
      "Votes": 122309,
      "Rank (UB)": 135,
      "Model": "claude-3-haiku-20240307",
      "95% CI": {
        "rating_q025": 1190.9133940218396,
        "rating_q975": 1194.8548086659653
      }
    },
    {
      "Score": 1191.8997092127556,
      "variance": 4.428088279569588,
      "rating_q975": 1195.4969314272357,
      "rating_q025": 1187.4654515578743,
      "Votes": 15753,
      "Rank (UB)": 135,
      "Model": "deepseek-coder-v2",
      "95% CI": {
        "rating_q025": 1187.4654515578743,
        "rating_q975": 1195.4969314272357
      }
    },
    {
      "Score": 1189.4813293948769,
      "variance": 8.969322496768482,
      "rating_q975": 1194.5489484628588,
      "rating_q025": 1183.7403087902585,
      "Votes": 9274,
      "Rank (UB)": 135,
      "Model": "jamba-1.5-mini",
      "95% CI": {
        "rating_q025": 1183.7403087902585,
        "rating_q975": 1194.5489484628588
      }
    },
    {
      "Score": 1189.180668369671,
      "variance": 1.7961059840889333,
      "rating_q975": 1191.2723682531764,
      "rating_q025": 1186.4246808490414,
      "Votes": 52578,
      "Rank (UB)": 138,
      "Model": "llama-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1186.4246808490414,
        "rating_q975": 1191.2723682531764
      }
    },
    {
      "Score": 1176.5701581562157,
      "variance": 1.3630066320753647,
      "rating_q975": 1178.7211296630323,
      "rating_q025": 1174.413712181166,
      "Votes": 91614,
      "Rank (UB)": 146,
      "Model": "gpt-4-0613",
      "95% CI": {
        "rating_q025": 1174.413712181166,
        "rating_q975": 1178.7211296630323
      }
    },
    {
      "Score": 1174.7749856252615,
      "variance": 2.898637844673541,
      "rating_q975": 1177.6056111218452,
      "rating_q025": 1171.4186413327225,
      "Votes": 27430,
      "Rank (UB)": 146,
      "Model": "qwen1.5-110b-chat",
      "95% CI": {
        "rating_q025": 1171.4186413327225,
        "rating_q975": 1177.6056111218452
      }
    },
    {
      "Score": 1170.8818656420706,
      "variance": 3.3358652012014476,
      "rating_q975": 1174.4068844115036,
      "rating_q025": 1167.4818787923566,
      "Votes": 25135,
      "Rank (UB)": 147,
      "Model": "yi-1.5-34b-chat",
      "95% CI": {
        "rating_q025": 1167.4818787923566,
        "rating_q975": 1174.4068844115036
      }
    },
    {
      "Score": 1170.8317310440143,
      "variance": 1.912121106473909,
      "rating_q975": 1173.1669109428194,
      "rating_q025": 1168.080918405182,
      "Votes": 64926,
      "Rank (UB)": 148,
      "Model": "mistral-large-2402",
      "95% CI": {
        "rating_q025": 1168.080918405182,
        "rating_q975": 1173.1669109428194
      }
    },
    {
      "Score": 1169.3150318815115,
      "variance": 5.86003842416678,
      "rating_q975": 1173.6232086603666,
      "rating_q025": 1164.8624033248736,
      "Votes": 16027,
      "Rank (UB)": 148,
      "Model": "reka-flash-21b-20240226-online",
      "95% CI": {
        "rating_q025": 1164.8624033248736,
        "rating_q975": 1173.6232086603666
      }
    },
    {
      "Score": 1166.395116577075,
      "variance": 30.66165023676536,
      "rating_q975": 1177.886479831662,
      "rating_q025": 1156.9923021795491,
      "Votes": 3410,
      "Rank (UB)": 146,
      "Model": "qwq-32b-preview",
      "95% CI": {
        "rating_q025": 1156.9923021795491,
        "rating_q975": 1177.886479831662
      }
    },
    {
      "Score": 1165.4930237678382,
      "variance": 1.044770689841068,
      "rating_q975": 1167.5381975146558,
      "rating_q025": 1163.3600970171472,
      "Votes": 109056,
      "Rank (UB)": 150,
      "Model": "llama-3-8b-instruct",
      "95% CI": {
        "rating_q025": 1163.3600970171472,
        "rating_q975": 1167.5381975146558
      }
    },
    {
      "Score": 1162.4816727771959,
      "variance": 9.277459443639057,
      "rating_q975": 1167.221912778032,
      "rating_q025": 1155.8435039194148,
      "Votes": 10599,
      "Rank (UB)": 151,
      "Model": "internlm2_5-20b-chat",
      "95% CI": {
        "rating_q025": 1155.8435039194148,
        "rating_q975": 1167.221912778032
      }
    },
    {
      "Score": 1162.1604060245331,
      "variance": 1.951122346153943,
      "rating_q975": 1164.6269785253196,
      "rating_q025": 1159.5010127871722,
      "Votes": 56398,
      "Rank (UB)": 152,
      "Model": "command-r",
      "95% CI": {
        "rating_q025": 1159.5010127871722,
        "rating_q975": 1164.6269785253196
      }
    },
    {
      "Score": 1161.3464514633736,
      "variance": 2.315054594396006,
      "rating_q975": 1164.5714347315854,
      "rating_q025": 1158.6113202609665,
      "Votes": 35556,
      "Rank (UB)": 152,
      "Model": "mistral-medium",
      "95% CI": {
        "rating_q025": 1158.6113202609665,
        "rating_q975": 1164.5714347315854
      }
    },
    {
      "Score": 1161.0808651838502,
      "variance": 1.5483255913596035,
      "rating_q975": 1163.4454736128953,
      "rating_q025": 1158.6059844938802,
      "Votes": 53751,
      "Rank (UB)": 152,
      "Model": "mixtral-8x22b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1158.6059844938802,
        "rating_q975": 1163.4454736128953
      }
    },
    {
      "Score": 1160.9238258292257,
      "variance": 3.453517633361557,
      "rating_q975": 1164.1928283062834,
      "rating_q025": 1157.4936368777746,
      "Votes": 25803,
      "Rank (UB)": 152,
      "Model": "reka-flash-21b-20240226",
      "95% CI": {
        "rating_q025": 1157.4936368777746,
        "rating_q975": 1164.1928283062834
      }
    },
    {
      "Score": 1160.9158079175259,
      "variance": 2.3862942822267024,
      "rating_q975": 1163.8446979425055,
      "rating_q025": 1157.7823422700067,
      "Votes": 40658,
      "Rank (UB)": 152,
      "Model": "qwen1.5-72b-chat",
      "95% CI": {
        "rating_q025": 1157.7823422700067,
        "rating_q975": 1163.8446979425055
      }
    },
    {
      "Score": 1157.3281603066378,
      "variance": 1.9878294814485653,
      "rating_q975": 1159.686793376131,
      "rating_q025": 1154.6002383566347,
      "Votes": 48892,
      "Rank (UB)": 153,
      "Model": "gemma-2-2b-it",
      "95% CI": {
        "rating_q025": 1154.6002383566347,
        "rating_q975": 1159.686793376131
      }
    },
    {
      "Score": 1156.3170745269458,
      "variance": 23.32953412814756,
      "rating_q975": 1163.988267136149,
      "rating_q025": 1147.1229096383395,
      "Votes": 3289,
      "Rank (UB)": 152,
      "Model": "granite-3.1-8b-instruct",
      "95% CI": {
        "rating_q025": 1147.1229096383395,
        "rating_q975": 1163.988267136149
      }
    },
    {
      "Score": 1144.8284589657812,
      "variance": 4.890647988568622,
      "rating_q975": 1149.0918866274242,
      "rating_q025": 1140.4279430973947,
      "Votes": 18800,
      "Rank (UB)": 162,
      "Model": "gemini-pro-dev-api",
      "95% CI": {
        "rating_q025": 1140.4279430973947,
        "rating_q975": 1149.0918866274242
      }
    },
    {
      "Score": 1140.6237230421495,
      "variance": 16.889424969227175,
      "rating_q975": 1148.171064533584,
      "rating_q025": 1132.9999678191473,
      "Votes": 4854,
      "Rank (UB)": 162,
      "Model": "zephyr-orpo-141b-A35b-v0.1",
      "95% CI": {
        "rating_q025": 1132.9999678191473,
        "rating_q975": 1148.171064533584
      }
    },
    {
      "Score": 1138.8473827414007,
      "variance": 3.905204143335956,
      "rating_q975": 1142.5623792973502,
      "rating_q025": 1134.6999284994383,
      "Votes": 22765,
      "Rank (UB)": 163,
      "Model": "qwen1.5-32b-chat",
      "95% CI": {
        "rating_q025": 1134.6999284994383,
        "rating_q975": 1142.5623792973502
      }
    },
    {
      "Score": 1136.4466765560287,
      "variance": 3.6748518054230495,
      "rating_q975": 1139.7527074341265,
      "rating_q025": 1132.5614339962387,
      "Votes": 26105,
      "Rank (UB)": 165,
      "Model": "phi-3-medium-4k-instruct",
      "95% CI": {
        "rating_q025": 1132.5614339962387,
        "rating_q975": 1139.7527074341265
      }
    },
    {
      "Score": 1132.8953851991882,
      "variance": 19.80822584196237,
      "rating_q975": 1141.1877370726345,
      "rating_q025": 1125.797423188981,
      "Votes": 3380,
      "Rank (UB)": 164,
      "Model": "granite-3.1-2b-instruct",
      "95% CI": {
        "rating_q025": 1125.797423188981,
        "rating_q975": 1141.1877370726345
      }
    },
    {
      "Score": 1132.4206487393208,
      "variance": 5.263383841049844,
      "rating_q975": 1136.9187632653634,
      "rating_q025": 1128.7075205072324,
      "Votes": 16676,
      "Rank (UB)": 165,
      "Model": "starling-lm-7b-beta",
      "95% CI": {
        "rating_q025": 1128.7075205072324,
        "rating_q975": 1136.9187632653634
      }
    },
    {
      "Score": 1127.5378089900605,
      "variance": 1.3521853033053175,
      "rating_q975": 1130.1400699269527,
      "rating_q025": 1125.8402679274475,
      "Votes": 76126,
      "Rank (UB)": 169,
      "Model": "mixtral-8x7b-instruct-v0.1",
      "95% CI": {
        "rating_q025": 1125.8402679274475,
        "rating_q975": 1130.1400699269527
      }
    },
    {
      "Score": 1124.731311624802,
      "variance": 5.030169315264954,
      "rating_q975": 1129.0680635643973,
      "rating_q025": 1120.5827228101143,
      "Votes": 15917,
      "Rank (UB)": 169,
      "Model": "yi-34b-chat",
      "95% CI": {
        "rating_q025": 1120.5827228101143,
        "rating_q975": 1129.0680635643973
      }
    },
    {
      "Score": 1124.174365922866,
      "variance": 13.542178687721359,
      "rating_q975": 1131.5498899186564,
      "rating_q025": 1116.985607157857,
      "Votes": 6557,
      "Rank (UB)": 169,
      "Model": "gemini-pro",
      "95% CI": {
        "rating_q025": 1116.985607157857,
        "rating_q975": 1131.5498899186564
      }
    },
    {
      "Score": 1122.470981967826,
      "variance": 3.54771637541453,
      "rating_q975": 1127.1442184471243,
      "rating_q025": 1119.3072549285425,
      "Votes": 18687,
      "Rank (UB)": 172,
      "Model": "qwen1.5-14b-chat",
      "95% CI": {
        "rating_q025": 1119.3072549285425,
        "rating_q975": 1127.1442184471243
      }
    },
    {
      "Score": 1119.8758261266196,
      "variance": 8.633983682215918,
      "rating_q975": 1125.8474814259932,
      "rating_q025": 1115.3807533616314,
      "Votes": 8383,
      "Rank (UB)": 172,
      "Model": "wizardlm-70b",
      "95% CI": {
        "rating_q025": 1115.3807533616314,
        "rating_q975": 1125.8474814259932
      }
    },
    {
      "Score": 1119.3777789350638,
      "variance": 1.566480268867407,
      "rating_q975": 1122.0023692456823,
      "rating_q025": 1117.321413658507,
      "Votes": 68867,
      "Rank (UB)": 174,
      "Model": "gpt-3.5-turbo-0125",
      "95% CI": {
        "rating_q025": 1117.321413658507,
        "rating_q975": 1122.0023692456823
      }
    },
    {
      "Score": 1116.6481662684566,
      "variance": 2.776043962325877,
      "rating_q975": 1119.540962906182,
      "rating_q025": 1113.4832159009582,
      "Votes": 33743,
      "Rank (UB)": 176,
      "Model": "dbrx-instruct-preview",
      "95% CI": {
        "rating_q025": 1113.4832159009582,
        "rating_q975": 1119.540962906182
      }
    },
    {
      "Score": 1116.1916184612796,
      "variance": 12.293216828307626,
      "rating_q975": 1122.945035768022,
      "rating_q025": 1109.6996216156774,
      "Votes": 8390,
      "Rank (UB)": 174,
      "Model": "llama-3.2-3b-instruct",
      "95% CI": {
        "rating_q025": 1109.6996216156774,
        "rating_q975": 1122.945035768022
      }
    },
    {
      "Score": 1115.5185295550698,
      "variance": 4.351571300543309,
      "rating_q975": 1119.7672923779105,
      "rating_q025": 1111.387653674709,
      "Votes": 18476,
      "Rank (UB)": 176,
      "Model": "phi-3-small-8k-instruct",
      "95% CI": {
        "rating_q025": 1111.387653674709,
        "rating_q975": 1119.7672923779105
      }
    },
    {
      "Score": 1112.5637526744997,
      "variance": 11.899970590722152,
      "rating_q975": 1119.4792222988297,
      "rating_q025": 1107.0684374796024,
      "Votes": 6658,
      "Rank (UB)": 176,
      "Model": "tulu-2-dpo-70b",
      "95% CI": {
        "rating_q025": 1107.0684374796024,
        "rating_q975": 1119.4792222988297
      }
    },
    {
      "Score": 1106.8009557496046,
      "variance": 13.571462757019532,
      "rating_q975": 1114.514211616113,
      "rating_q025": 1099.7776930939028,
      "Votes": 7002,
      "Rank (UB)": 180,
      "Model": "granite-3.0-8b-instruct",
      "95% CI": {
        "rating_q025": 1099.7776930939028,
        "rating_q975": 1114.514211616113
      }
    },
    {
      "Score": 1106.4339064241526,
      "variance": 2.4024745709266093,
      "rating_q975": 1109.2399649775114,
      "rating_q025": 1103.8590678680735,
      "Votes": 39595,
      "Rank (UB)": 184,
      "Model": "llama-2-70b-chat",
      "95% CI": {
        "rating_q025": 1103.8590678680735,
        "rating_q975": 1109.2399649775114
      }
    },
    {
      "Score": 1104.9272754293972,
      "variance": 5.6208398604406105,
      "rating_q975": 1109.8616569698477,
      "rating_q025": 1100.641275781743,
      "Votes": 12990,
      "Rank (UB)": 183,
      "Model": "openchat-3.5-0106",
      "95% CI": {
        "rating_q025": 1100.641275781743,
        "rating_q975": 1109.8616569698477
      }
    },
    {
      "Score": 1104.1939169520113,
      "variance": 4.124427699437685,
      "rating_q975": 1107.3918587145624,
      "rating_q025": 1100.319701517917,
      "Votes": 22936,
      "Rank (UB)": 184,
      "Model": "vicuna-33b",
      "95% CI": {
        "rating_q025": 1100.319701517917,
        "rating_q975": 1107.3918587145624
      }
    },
    {
      "Score": 1103.4657395402119,
      "variance": 2.38451113838603,
      "rating_q975": 1105.953506716271,
      "rating_q025": 1100.4543673629696,
      "Votes": 34173,
      "Rank (UB)": 185,
      "Model": "snowflake-arctic-instruct",
      "95% CI": {
        "rating_q025": 1100.4543673629696,
        "rating_q975": 1105.953506716271
      }
    },
    {
      "Score": 1101.9018187184756,
      "variance": 9.173010668192514,
      "rating_q975": 1107.7665257221306,
      "rating_q025": 1097.011288044392,
      "Votes": 10415,
      "Rank (UB)": 184,
      "Model": "starling-lm-7b-alpha",
      "95% CI": {
        "rating_q025": 1097.011288044392,
        "rating_q975": 1107.7665257221306
      }
    },
    {
      "Score": 1097.5874926416584,
      "variance": 18.122550898919155,
      "rating_q975": 1104.9768995830543,
      "rating_q025": 1089.730674698276,
      "Votes": 3836,
      "Rank (UB)": 185,
      "Model": "nous-hermes-2-mixtral-8x7b-dpo",
      "95% CI": {
        "rating_q025": 1089.730674698276,
        "rating_q975": 1104.9768995830543
      }
    },
    {
      "Score": 1097.2063719376326,
      "variance": 3.3777380906596255,
      "rating_q975": 1100.2103071086278,
      "rating_q025": 1093.3185754871201,
      "Votes": 25070,
      "Rank (UB)": 189,
      "Model": "gemma-1.1-7b-it",
      "95% CI": {
        "rating_q025": 1093.3185754871201,
        "rating_q975": 1100.2103071086278
      }
    },
    {
      "Score": 1094.1079064251328,
      "variance": 25.354778409130432,
      "rating_q975": 1104.056693868138,
      "rating_q025": 1084.971564242002,
      "Votes": 3636,
      "Rank (UB)": 185,
      "Model": "llama2-70b-steerlm-chat",
      "95% CI": {
        "rating_q025": 1084.971564242002,
        "rating_q975": 1104.056693868138
      }
    },
    {
      "Score": 1090.4704713443764,
      "variance": 19.020129360671994,
      "rating_q975": 1099.2378805829383,
      "rating_q025": 1082.5750182417828,
      "Votes": 4988,
      "Rank (UB)": 190,
      "Model": "deepseek-llm-67b-chat",
      "95% CI": {
        "rating_q025": 1082.5750182417828,
        "rating_q975": 1099.2378805829383
      }
    },
    {
      "Score": 1089.938087257156,
      "variance": 9.16347483152837,
      "rating_q975": 1095.7398990799893,
      "rating_q025": 1084.8973551543347,
      "Votes": 8106,
      "Rank (UB)": 191,
      "Model": "openchat-3.5",
      "95% CI": {
        "rating_q025": 1084.8973551543347,
        "rating_q975": 1095.7398990799893
      }
    },
    {
      "Score": 1087.7764016868477,
      "variance": 18.302477459306925,
      "rating_q975": 1095.1043811326886,
      "rating_q025": 1076.715787528188,
      "Votes": 5088,
      "Rank (UB)": 191,
      "Model": "openhermes-2.5-mistral-7b",
      "95% CI": {
        "rating_q025": 1076.715787528188,
        "rating_q975": 1095.1043811326886
      }
    },
    {
      "Score": 1087.3782171789976,
      "variance": 12.376879780136097,
      "rating_q975": 1093.5891471793066,
      "rating_q025": 1079.999320021979,
      "Votes": 7191,
      "Rank (UB)": 191,
      "Model": "granite-3.0-2b-instruct",
      "95% CI": {
        "rating_q025": 1079.999320021979,
        "rating_q975": 1093.5891471793066
      }
    },
    {
      "Score": 1085.8078431987046,
      "variance": 4.348404573206927,
      "rating_q975": 1089.6729550249556,
      "rating_q025": 1081.0012721118203,
      "Votes": 20067,
      "Rank (UB)": 193,
      "Model": "mistral-7b-instruct-v0.2",
      "95% CI": {
        "rating_q025": 1081.0012721118203,
        "rating_q975": 1089.6729550249556
      }
    },
    {
      "Score": 1084.341268304057,
      "variance": 7.079501493855547,
      "rating_q975": 1089.033756358259,
      "rating_q025": 1079.6140255726848,
      "Votes": 12808,
      "Rank (UB)": 193,
      "Model": "phi-3-mini-4k-instruct-june-2024",
      "95% CI": {
        "rating_q025": 1079.6140255726848,
        "rating_q975": 1089.033756358259
      }
    },
    {
      "Score": 1083.2133932760166,
      "variance": 19.52362535916762,
      "rating_q975": 1090.8103637189733,
      "rating_q025": 1073.4761349492715,
      "Votes": 4872,
      "Rank (UB)": 192,
      "Model": "qwen1.5-7b-chat",
      "95% CI": {
        "rating_q025": 1073.4761349492715,
        "rating_q975": 1090.8103637189733
      }
    },
    {
      "Score": 1081.1394519230234,
      "variance": 4.854684776060536,
      "rating_q975": 1085.9172747175157,
      "rating_q025": 1076.9029982624154,
      "Votes": 17036,
      "Rank (UB)": 193,
      "Model": "gpt-3.5-turbo-1106",
      "95% CI": {
        "rating_q025": 1076.9029982624154,
        "rating_q975": 1085.9172747175157
      }
    },
    {
      "Score": 1079.9682081208323,
      "variance": 3.260933627759034,
      "rating_q975": 1084.0792166197803,
      "rating_q025": 1077.0215262456709,
      "Votes": 21097,
      "Rank (UB)": 196,
      "Model": "phi-3-mini-4k-instruct",
      "95% CI": {
        "rating_q025": 1077.0215262456709,
        "rating_q975": 1084.0792166197803
      }
    },
    {
      "Score": 1076.6834158350318,
      "variance": 4.899460783372175,
      "rating_q975": 1079.982807018361,
      "rating_q025": 1072.4998714684227,
      "Votes": 19722,
      "Rank (UB)": 199,
      "Model": "llama-2-13b-chat",
      "95% CI": {
        "rating_q025": 1072.4998714684227,
        "rating_q975": 1079.982807018361
      }
    },
    {
      "Score": 1076.2327434731494,
      "variance": 39.87561960587942,
      "rating_q975": 1087.5500841841351,
      "rating_q025": 1063.6146658000878,
      "Votes": 1714,
      "Rank (UB)": 193,
      "Model": "dolphin-2.2.1-mistral-7b",
      "95% CI": {
        "rating_q025": 1063.6146658000878,
        "rating_q975": 1087.5500841841351
      }
    },
    {
      "Score": 1075.6107863748105,
      "variance": 22.964383077021022,
      "rating_q975": 1083.447818006864,
      "rating_q025": 1065.2222272211795,
      "Votes": 4286,
      "Rank (UB)": 196,
      "Model": "solar-10.7b-instruct-v1.0",
      "95% CI": {
        "rating_q025": 1065.2222272211795,
        "rating_q975": 1083.447818006864
      }
    },
    {
      "Score": 1072.2023540973078,
      "variance": 14.321826729082362,
      "rating_q975": 1079.2807239652675,
      "rating_q025": 1064.8153371225362,
      "Votes": 7176,
      "Rank (UB)": 200,
      "Model": "wizardlm-13b",
      "95% CI": {
        "rating_q025": 1064.8153371225362,
        "rating_q975": 1079.2807239652675
      }
    },
    {
      "Score": 1067.1656597666447,
      "variance": 9.551024533491058,
      "rating_q975": 1072.8279946551163,
      "rating_q025": 1060.9862525268954,
      "Votes": 8523,
      "Rank (UB)": 204,
      "Model": "llama-3.2-1b-instruct",
      "95% CI": {
        "rating_q025": 1060.9862525268954,
        "rating_q975": 1072.8279946551163
      }
    },
    {
      "Score": 1066.7450968093708,
      "variance": 6.5860862493404975,
      "rating_q975": 1071.0417324456575,
      "rating_q025": 1061.4889856244606,
      "Votes": 11321,
      "Rank (UB)": 205,
      "Model": "zephyr-7b-beta",
      "95% CI": {
        "rating_q025": 1061.4889856244606,
        "rating_q975": 1071.0417324456575
      }
    },
    {
      "Score": 1059.9586665468073,
      "variance": 34.313018608885905,
      "rating_q975": 1072.455166348128,
      "rating_q025": 1049.7875027450166,
      "Votes": 2375,
      "Rank (UB)": 205,
      "Model": "smollm2-1.7b-instruct",
      "95% CI": {
        "rating_q025": 1049.7875027450166,
        "rating_q975": 1072.455166348128
      }
    },
    {
      "Score": 1058.8882662245396,
      "variance": 31.05164581751602,
      "rating_q975": 1067.8005891839462,
      "rating_q025": 1046.9870653015414,
      "Votes": 2644,
      "Rank (UB)": 205,
      "Model": "mpt-30b-chat",
      "95% CI": {
        "rating_q025": 1046.9870653015414,
        "rating_q975": 1067.8005891839462
      }
    },
    {
      "Score": 1056.2660499923804,
      "variance": 16.5530754693042,
      "rating_q975": 1065.530519658072,
      "rating_q025": 1050.0055430440182,
      "Votes": 7509,
      "Rank (UB)": 205,
      "Model": "codellama-34b-instruct",
      "95% CI": {
        "rating_q025": 1050.0055430440182,
        "rating_q975": 1065.530519658072
      }
    },
    {
      "Score": 1055.4744408174147,
      "variance": 5.409498495159544,
      "rating_q975": 1059.4774608313937,
      "rating_q025": 1051.071254470804,
      "Votes": 19775,
      "Rank (UB)": 210,
      "Model": "vicuna-13b",
      "95% CI": {
        "rating_q025": 1051.071254470804,
        "rating_q975": 1059.4774608313937
      }
    },
    {
      "Score": 1055.264998971371,
      "variance": 69.22017210498403,
      "rating_q975": 1071.26276667236,
      "rating_q025": 1038.2533706339882,
      "Votes": 1192,
      "Rank (UB)": 205,
      "Model": "codellama-70b-instruct",
      "95% CI": {
        "rating_q025": 1038.2533706339882,
        "rating_q975": 1071.26276667236
      }
    },
    {
      "Score": 1054.2604688569454,
      "variance": 58.958782795606794,
      "rating_q975": 1069.758246978951,
      "rating_q025": 1040.5842476505438,
      "Votes": 1811,
      "Rank (UB)": 205,
      "Model": "zephyr-7b-alpha",
      "95% CI": {
        "rating_q025": 1040.5842476505438,
        "rating_q975": 1069.758246978951
      }
    },
    {
      "Score": 1050.9631156964003,
      "variance": 8.865537582484118,
      "rating_q975": 1056.6651793112128,
      "rating_q025": 1046.2454381038444,
      "Votes": 9176,
      "Rank (UB)": 210,
      "Model": "gemma-7b-it",
      "95% CI": {
        "rating_q025": 1046.2454381038444,
        "rating_q975": 1056.6651793112128
      }
    },
    {
      "Score": 1050.4777550494439,
      "variance": 4.0165322118292845,
      "rating_q975": 1053.615412998281,
      "rating_q025": 1046.749460450157,
      "Votes": 21622,
      "Rank (UB)": 210,
      "Model": "phi-3-mini-128k-instruct",
      "95% CI": {
        "rating_q025": 1046.749460450157,
        "rating_q975": 1053.615412998281
      }
    },
    {
      "Score": 1050.4174430733128,
      "variance": 6.192401062386532,
      "rating_q975": 1054.774450212317,
      "rating_q025": 1044.8929502061012,
      "Votes": 14532,
      "Rank (UB)": 210,
      "Model": "llama-2-7b-chat",
      "95% CI": {
        "rating_q025": 1044.8929502061012,
        "rating_q975": 1054.774450212317
      }
    },
    {
      "Score": 1048.403677183907,
      "variance": 17.6744877071345,
      "rating_q975": 1057.5643842944244,
      "rating_q025": 1041.0833948753527,
      "Votes": 5065,
      "Rank (UB)": 210,
      "Model": "qwen-14b-chat",
      "95% CI": {
        "rating_q025": 1041.0833948753527,
        "rating_q975": 1057.5643842944244
      }
    },
    {
      "Score": 1047.638901820516,
      "variance": 66.50633206884072,
      "rating_q975": 1063.4875657713005,
      "rating_q025": 1031.189430251246,
      "Votes": 1327,
      "Rank (UB)": 208,
      "Model": "falcon-180b-chat",
      "95% CI": {
        "rating_q025": 1031.189430251246,
        "rating_q975": 1063.4875657713005
      }
    },
    {
      "Score": 1046.2377399987547,
      "variance": 39.20725230918819,
      "rating_q975": 1057.4334327147487,
      "rating_q025": 1034.6872940753867,
      "Votes": 2996,
      "Rank (UB)": 210,
      "Model": "guanaco-33b",
      "95% CI": {
        "rating_q025": 1034.6872940753867,
        "rating_q975": 1057.4334327147487
      }
    },
    {
      "Score": 1034.2000742600806,
      "variance": 6.169699632957862,
      "rating_q975": 1039.2482038784815,
      "rating_q025": 1030.0305715481356,
      "Votes": 11351,
      "Rank (UB)": 220,
      "Model": "gemma-1.1-2b-it",
      "95% CI": {
        "rating_q025": 1030.0305715481356,
        "rating_q975": 1039.2482038784815
      }
    },
    {
      "Score": 1030.8261407948332,
      "variance": 19.232794115078384,
      "rating_q975": 1038.331565115969,
      "rating_q025": 1023.1939028521554,
      "Votes": 5276,
      "Rank (UB)": 220,
      "Model": "stripedhyena-nous-7b",
      "95% CI": {
        "rating_q025": 1023.1939028521554,
        "rating_q975": 1038.331565115969
      }
    },
    {
      "Score": 1028.7470089622018,
      "variance": 14.018565166804356,
      "rating_q975": 1035.1337000908115,
      "rating_q025": 1020.7632347017588,
      "Votes": 6503,
      "Rank (UB)": 221,
      "Model": "olmo-7b-instruct",
      "95% CI": {
        "rating_q025": 1020.7632347017588,
        "rating_q975": 1035.1337000908115
      }
    },
    {
      "Score": 1021.2093931651413,
      "variance": 12.819305888592481,
      "rating_q975": 1028.090621819619,
      "rating_q025": 1014.7976968545061,
      "Votes": 9142,
      "Rank (UB)": 224,
      "Model": "mistral-7b-instruct",
      "95% CI": {
        "rating_q025": 1014.7976968545061,
        "rating_q975": 1028.090621819619
      }
    },
    {
      "Score": 1018.231288541019,
      "variance": 10.583519763170969,
      "rating_q975": 1024.6542146417567,
      "rating_q025": 1012.4689188825182,
      "Votes": 7017,
      "Rank (UB)": 224,
      "Model": "vicuna-7b",
      "95% CI": {
        "rating_q025": 1012.4689188825182,
        "rating_q975": 1024.6542146417567
      }
    },
    {
      "Score": 1016.8789309508054,
      "variance": 13.214709022051881,
      "rating_q975": 1023.8970342437367,
      "rating_q025": 1010.4652645264415,
      "Votes": 8713,
      "Rank (UB)": 224,
      "Model": "palm-2",
      "95% CI": {
        "rating_q025": 1010.4652645264415,
        "rating_q975": 1023.8970342437367
      }
    },
    {
      "Score": 1003.005841830425,
      "variance": 21.89296157595602,
      "rating_q975": 1011.6667505038674,
      "rating_q025": 993.8934692287637,
      "Votes": 4918,
      "Rank (UB)": 228,
      "Model": "gemma-2b-it",
      "95% CI": {
        "rating_q025": 993.8934692287637,
        "rating_q975": 1011.6667505038674
      }
    },
    {
      "Score": 1001.6894524033428,
      "variance": 10.160716623810181,
      "rating_q975": 1006.9712621942149,
      "rating_q025": 995.7663027472182,
      "Votes": 7816,
      "Rank (UB)": 229,
      "Model": "qwen1.5-4b-chat",
      "95% CI": {
        "rating_q025": 995.7663027472182,
        "rating_q975": 1006.9712621942149
      }
    },
    {
      "Score": 978.0378580693247,
      "variance": 12.898890820954758,
      "rating_q975": 983.5510924249701,
      "rating_q025": 970.3791676432099,
      "Votes": 7020,
      "Rank (UB)": 231,
      "Model": "koala-13b",
      "95% CI": {
        "rating_q025": 970.3791676432099,
        "rating_q975": 983.5510924249701
      }
    },
    {
      "Score": 968.3294786556689,
      "variance": 17.328288752368792,
      "rating_q975": 976.253732201608,
      "rating_q025": 960.1312965980926,
      "Votes": 4763,
      "Rank (UB)": 231,
      "Model": "chatglm3-6b",
      "95% CI": {
        "rating_q025": 960.1312965980926,
        "rating_q975": 976.253732201608
      }
    },
    {
      "Score": 945.7145838846095,
      "variance": 54.848326108786594,
      "rating_q975": 959.3267782137501,
      "rating_q025": 931.0430866595127,
      "Votes": 1788,
      "Rank (UB)": 233,
      "Model": "gpt4all-13b-snoozy",
      "95% CI": {
        "rating_q025": 931.0430866595127,
        "rating_q975": 959.3267782137501
      }
    },
    {
      "Score": 941.5239136737032,
      "variance": 26.912079291965025,
      "rating_q975": 950.9321397197718,
      "rating_q025": 932.3087775547951,
      "Votes": 3997,
      "Rank (UB)": 233,
      "Model": "mpt-7b-chat",
      "95% CI": {
        "rating_q025": 932.3087775547951,
        "rating_q975": 950.9321397197718
      }
    },
    {
      "Score": 937.7851618143014,
      "variance": 46.5136150749716,
      "rating_q975": 950.3469626248566,
      "rating_q025": 923.9825231250999,
      "Votes": 2713,
      "Rank (UB)": 233,
      "Model": "chatglm2-6b",
      "95% CI": {
        "rating_q025": 923.9825231250999,
        "rating_q975": 950.3469626248566
      }
    },
    {
      "Score": 935.0172079031867,
      "variance": 22.34321427150955,
      "rating_q975": 944.2266848396266,
      "rating_q025": 926.5951497371877,
      "Votes": 4920,
      "Rank (UB)": 233,
      "Model": "RWKV-4-Raven-14B",
      "95% CI": {
        "rating_q025": 926.5951497371877,
        "rating_q975": 944.2266848396266
      }
    },
    {
      "Score": 914.886137013069,
      "variance": 16.588420644461156,
      "rating_q975": 920.8919884534474,
      "rating_q025": 906.2820373756919,
      "Votes": 5864,
      "Rank (UB)": 237,
      "Model": "alpaca-13b",
      "95% CI": {
        "rating_q025": 906.2820373756919,
        "rating_q975": 920.8919884534474
      }
    },
    {
      "Score": 906.4903079289531,
      "variance": 19.338111046967708,
      "rating_q975": 914.2181297330092,
      "rating_q025": 897.6797358867626,
      "Votes": 6368,
      "Rank (UB)": 237,
      "Model": "oasst-pythia-12b",
      "95% CI": {
        "rating_q025": 897.6797358867626,
        "rating_q975": 914.2181297330092
      }
    },
    {
      "Score": 892.372895122196,
      "variance": 22.307943851156555,
      "rating_q975": 901.2273779536145,
      "rating_q025": 882.2549160335961,
      "Votes": 4983,
      "Rank (UB)": 238,
      "Model": "chatglm-6b",
      "95% CI": {
        "rating_q025": 882.2549160335961,
        "rating_q975": 901.2273779536145
      }
    },
    {
      "Score": 881.157179624717,
      "variance": 20.253416712367688,
      "rating_q975": 888.8501914879985,
      "rating_q025": 872.6269403743927,
      "Votes": 4288,
      "Rank (UB)": 239,
      "Model": "fastchat-t5-3b",
      "95% CI": {
        "rating_q025": 872.6269403743927,
        "rating_q975": 888.8501914879985
      }
    },
    {
      "Score": 853.3706159341483,
      "variance": 28.531622098405922,
      "rating_q975": 863.616753107594,
      "rating_q025": 843.395651136357,
      "Votes": 3336,
      "Rank (UB)": 241,
      "Model": "stablelm-tuned-alpha-7b",
      "95% CI": {
        "rating_q025": 843.395651136357,
        "rating_q975": 863.616753107594
      }
    },
    {
      "Score": 835.5973398288966,
      "variance": 39.371268477807455,
      "rating_q975": 847.449939950953,
      "rating_q025": 823.9098777980524,
      "Votes": 3480,
      "Rank (UB)": 241,
      "Model": "dolly-v2-12b",
      "95% CI": {
        "rating_q025": 823.9098777980524,
        "rating_q975": 847.449939950953
      }
    },
    {
      "Score": 812.9076378941571,
      "variance": 45.09310286208724,
      "rating_q975": 826.5764057318277,
      "rating_q025": 801.3499374869191,
      "Votes": 2446,
      "Rank (UB)": 242,
      "Model": "llama-13b",
      "95% CI": {
        "rating_q025": 801.3499374869191,
        "rating_q975": 826.5764057318277
      }
    }
  ]
}